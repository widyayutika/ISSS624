---
title: "Take-home_Ex1: Public Bus Passengers in Singapore"
author: "Widya Tantiya Yutika"
date: "29 November 2023"
date-modified: "last-modified"
format: html
execute: 
  echo: true
  eval: true
  warning: false
editor: visual
---

## Overview

The increasing digitization of urban infrastructures, including buses, taxis, mass rapid transit, public utilities and roads, has generated vast datasets capturing movement patterns over space and time. This data, facilitated by technologies such as GPS and RFID, offers valuable insights into human mobility within cities. Smart cards and GPS devices on public buses, for instance, have enabled the collection of routes and ridership data, providing a rich source for understanding urban movement.

Despite the wealth of data collected, its utilization often remains limited to basic tracking and mapping using Geographic Information System (GIS) applications. This limitation is attributed to the inadequacy of conventional GIS functions in effectively analyzing and modeling spatial and spatio-temporal data.

The objectives of this study are centered around employing Exploratory Spatial Data Analysis (ESDA) techniques, specifically Local Indicators of Spatial Association (LISA) and Emerging Hot Spot Analysis (EHSA), to unveil the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore.

## **The Study Area and Data**

### Aspatial Data

The aspatial data used in this take-home exercise is extracted from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html) (*Passenger Volume by Origin Destination Bus Stops*) for the month of October 2023.

### Geospatial Data

The geospatial data used in this take-home exercise are as follows.

-   *Bus Stop Location* from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html), which provides information about all the bus stops currently being serviced by buses, including the bus stop code (identifier) and location coordinates.

-   *hexagon*, a [hexagon](https://desktop.arcgis.com/en/arcmap/latest/tools/spatial-statistics-toolbox/h-whyhexagons.htm) layer of 250m perpendicular distance between the centre of the hexagon and its edges is used to replace the relative coarse and irregular Master Plan 2019 Planning Sub-zone GIS data set of URA.

## **Setting the Analytical Tools**

Before I get started, I need to ensure that **sf**, **spdep**, **tmap**, **tidyverse,** and **knitr** packages of R are currently installed in my R.

-   *sf* : for importing and handling geospatial data in R,

-   *spdep* : for computing spatial weights, global and local spatial autocorrelation statistics, and

-   *tmap* : for preparing cartographic quality chropleth map

-   *tidyverse* : for wrangling attribute data in R ; [tidyverse](https://www.tidyverse.org/) has already included collection of packages such as readr, ggplot2, dplyr, tiblle, purr, etc.

-   knitr: for facilitating dynamic report generation in R Markdown documents.

The code chunk below is used to ensure that the necessary R packages have been installed , if its iyet to be installed, it will then be installed and ready to be used in the R environment.

```{r}
pacman::p_load(sf, spdep, tmap, tidyverse, knitr)
```

## **Getting the Data into R Environment**

### **Importing Shapefile into R Environment**

The code chunk below uses [*`st_read()`*](https://r-spatial.github.io/sf/reference/st_read.html) of **sf** package to import BusStop shapefile into R. The imported shapefile will be **simple features** Object of **sf**.

```{r}
busstop <- st_read(dsn = "data/geospatial", layer = "BusStop")
```

The code chunk below uses [*`st_geometry()`*](https://www.rdocumentation.org/packages/sf/versions/1.0-14/topics/st_geometry) of **sf** package to display basic information of feature class.

```{r}
st_geometry(busstop)
```

The code chunk below uses [*`glimpse()`*](https://www.rdocumentation.org/packages/dplyr/versions/1.0.10/topics/glimpse) of **dplyr** package to display the data type of each fields.

```{r}
glimpse(busstop)
```

Next, I will plot the geospatial data using the code chunk below.

```{r}
plot(busstop)
```

From the glimpse() check above, it is shown that the BUS_STOP_N is in character type. It needs to be converted to factor type to work with categorical variables so that I can use them to georeference with bus stop location data.

```{r}
busstop$BUS_STOP_N <- as.factor(busstop$BUS_STOP_N)
```

Next, I will confirm the data type for BUS_STOP_N has changed to data type of "factor" using glimpse().

```{r}
glimpse(busstop)
```

### **Importing Csv File into R Environment**

Next, I will import *origin_destination_bus_202310.csv* into R by using [*`st_read()`*](https://r-spatial.github.io/sf/reference/st_read.html) of **readr** package. The output is R dataframe class.

```{r}
odbus <- read_csv("data/aspatial/origin_destination_bus_202310.csv")
```

The code chunk below uses [*`glimpse()`*](https://www.rdocumentation.org/packages/dplyr/versions/1.0.10/topics/glimpse) of **dplyr** package to display the odbus tibble data tables.

```{r}
glimpse(odbus)
```

From the glimpse() check above, it is shown that the ORIGIN_PT_CODE and DESTINATION_PT_CODE are in character type. Both of them need to be converted to factor type to work with categorical variables so that I can use them to georeference with bus stop location data.

```{r}
odbus$ORIGIN_PT_CODE <- as.factor(odbus$ORIGIN_PT_CODE)
odbus$DESTINATION_PT_CODE <- as.factor(odbus$DESTINATION_PT_CODE) 
```

I will also change the data type of TIME_PER_HOUR and TOTAL_TRIPS from \<dbl\> to \<int\> because it both of these fields should be represented as whole numbers.

```{r}
odbus$TIME_PER_HOUR <- as.integer(odbus$TIME_PER_HOUR)
odbus$TOTAL_TRIPS <- as.integer(odbus$TOTAL_TRIPS) 
```

Next, I will confirm the data type for ORIGIN_PT_CODE and DESTINATION_PT_CODE have changed to factor using glimpse().

```{r}
glimpse(odbus)
```

## Data Wrangling

### Checking the Reference Coordinate System of Geospatial Data

Common issue in importing geospatial data into R is that the coordinate system of the source data was either missing (due to missing .proj for ESRI shapefile, etc.) or wrongly assigned.

The code chunk below uses [*`st_crs()`*](https://www.rdocumentation.org/packages/sf/versions/0.2-8/topics/st_crs) of **sf** package to retrieve the coordinate reference system of busstop.

```{r}
st_crs(busstop)
```

Both busstop is projected in svy21 as shown from the second line, but at the last line, it is mentioned that the EPSG is 9001. This is wrongly assigned because the correct EPSG code for svy21 is [3414](https://epsg.io/3414).

### Transforming the Projection

Next, I will transform busstop from geographic coordinate system to projected coordinated system as my analysis will measure distance or/and area.

The code chunk below uses [*`st_transform`*](https://r-spatial.github.io/sf/reference/st_transform.html) of **sp** package to convert coordinates to EPSG code of 3414.

```{r}
busstop3414 <- st_transform(busstop, 3414)
```

Next, I will check the coordinate system after transformation with the code chunk below.

```{r}
st_crs(busstop3414)
```

As noticed from the above, the Projected CRS is now SVY21 / Singapore TM and the last line has changed to EPSG 3414.

### Checking Duplicated Records

The code chunk below is used to check for duplicated records on odbus.

```{r}
duplicate <- odbus %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

The above code chunk shows that there is no duplicate record found.

### Remove Unnecessary Fields

First, I check the YEAR_MONTH and PT_TYPE unique values by using the *`table()`* to create a frequency table of each categorical representation.

```{r}
YEAR_MONTH_counts <- table(odbus$YEAR_MONTH)
print(YEAR_MONTH_counts)

DAY_TYPE_counts <- table(odbus$DAY_TYPE)
print(DAY_TYPE_counts)

PT_TYPE_counts <- table(odbus$PT_TYPE)
print(PT_TYPE_counts)
```

From the results above, I will exclude YEAR_MONTH and PT_TYPE as they only have single categorical representation using the code chunk below.

```{r}
odbus <- odbus[, !(names(odbus) %in% c("YEAR_MONTH", "PT_TYPE"))]
```

I will then use glimpse() to ensure the process is done correctly.

```{r}
glimpse(odbus)
```

## Creating honeycomb_grid

Honeycomb grid are preferred to replace coarse and irregular Master Plan 2019 Sub-zone GIS data set of URA because hexagon reduce sampling bias due to its grid shape of low perimeter to are ratio and its ability to form evenly spaced grid. Honeycomb grids are well-suited for approximating circular areas, making them suitable for mapping Singapore edges with is irregular shape.

The code chunk below uses [*`st_make_grid`*](https://r-spatial.github.io/sf/reference/st_make_grid.html) of **sf** package to create a hexagonal or honeycomb grid with a 250m (perpendicular distance between the center of hexagon and its edges). According the the R documentation, the cellsize is the distance between opposite edges, which is 2 times the perpendicular distance between the center of hexagon and its edges. Thus, for the purpose of this exercise, I will use the cellsize of 500m and indicate the square=FALSE for hexagonal grid. After doing do, I will create a grid_id for each hexagonal grid.

```{r}
area_honeycomb_grid = st_make_grid(busstop3414, c(500, 500), what = "polygons", square = FALSE)    
# To sf and add grid ID  
honeycomb_grid_sf = st_sf(area_honeycomb_grid) %>%    
  # add grid ID      
  mutate(grid_id = 1:length(lengths(area_honeycomb_grid)))
```

## Extracting the study data

In this exercise, I will extract the commuting flows during peak hours as follows.

|                              | Bus tap on time |
|------------------------------|-----------------|
| Weekday morning peak         | 6am to 9am      |
| Weekday afternoon peak       | 5pm to 8pm      |
| Weekend/holiday morning peak | 11am to 2pm     |
| Weekend/holiday evening peak | 4pm to 7pm      |

### Weekday Morning Peak

The code chunk below will be used to extract the weekday morning peak (Weekday: 6-9am) and calculate the passenger trips in each origin bus stop by using the *`group_by()`* from **dplyr** package and aggregate the values using *`summarise()`* and sum up the "Total_Trips". The *`mutate()`* in the code below is to ensure that after the group_by, the ORIGIN_PT_CODE remains in the factor data type.

```{r}
odbus_weekday_6_9 <- odbus %>%
  filter(DAY_TYPE == "WEEKDAY") %>%
  filter(TIME_PER_HOUR >= 6 &
           TIME_PER_HOUR <= 9) %>%
  group_by(ORIGIN_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))%>%
  mutate(ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE))
```

I will repeat the processes above for the other peak hours as shown below.

### Weekday Afternoon Peak

The code chunk below will be used to extract the weekday afternoon peak (Weekday: 5-8pm) and calculate the passenger trips in each origin bus stop.

```{r}
odbus_weekday_17_20 <- odbus %>%   
  filter(DAY_TYPE == "WEEKDAY") %>%   
  filter(TIME_PER_HOUR >= 17 &            
           TIME_PER_HOUR <= 20) %>%   
  group_by(ORIGIN_PT_CODE) %>%   
  summarise(TRIPS = sum(TOTAL_TRIPS)) %>%
  mutate(ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE))
```

### Weekends/Holiday Morning Peak

The code chunk below will be used to extract the weekend/holiday morning peak (Weekend/holiday: 11am-2pm) and calculate the passenger trips in each origin bus stop.

```{r}
odbus_weekend_11_14 <- odbus %>% 
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%   
  filter(TIME_PER_HOUR >= 11 &            
           TIME_PER_HOUR <= 14) %>%   
  group_by(ORIGIN_PT_CODE) %>%   
  summarise(TRIPS = sum(TOTAL_TRIPS)) %>%
  mutate(ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE))
```

### Weekends/Holiday Evening Peak

The code chunk below will be used to extract the weekend/holiday evening peak (Weekend/holiday: 4-7pm) and calculate the total trips in each origin and destination pair.

```{r}
odbus_weekend_16_19 <- odbus %>%   
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%   
  filter(TIME_PER_HOUR >= 16 &            
           TIME_PER_HOUR <= 19) %>%   
  group_by(ORIGIN_PT_CODE) %>%   
  summarise(TRIPS = sum(TOTAL_TRIPS)) %>%
  mutate(ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE))
```

## Further Data Wrangling for Weekday Morning Peak Hour: A Step-by-Step Guide

This section provides a comprehensive step-by step walkthrough to calculate the number of trips within each hexagonal grid during Weekday Morning Peak Hour with a subsequent plan to replicate the same process for Weekday Afternoon Peak Hour, Weekends/Holiday Morning Peak, and Weekends/Holiday Evening Peak in the subsequent section

### Performing Relational Join

The code chunk below will be used to join the busstop3414 SpatialPolygonsDataframe and odbus_weekday_6_9_data by BUS_STOP_N for busstop3414 and BUS_STOP_ID for original_destination_bus. This is performed by using `left_join()` of **dplyr** package. In this take-home exercise, I will focus on passenger trips generated by origin bus stop, I will remove the rows with bus stops solely serve as destinations which are indicated by NA values on the corresponding "Total_Trips" using the *`filter()`* from **dplyr** package.

```{r}
total_trips_per_busstop_wdmp <- left_join(busstop3414, odbus_weekday_6_9, by = c("BUS_STOP_N" = "ORIGIN_PT_CODE")) %>%
  filter(!is.na(TRIPS))
```

### Spatial Join with Hexagonal Honeycomb Grid and Calculating Total Trips in a Hexagonal Grid

The code chunk below will be used to join the total_trips_per_busstop and honeycomb grid spatially using *`st_join()`* from **sf** package and remove the hexagon grid without any bus stop which is indicated by NA value on the "BUS_STOP_N". Next, I will calculate the total trips in a hexagonal grid using the *`group_by()`* from **dplyr** package.

```{r}
total_trips_per_grid_wdmp <- st_join(honeycomb_grid_sf,total_trips_per_busstop_wdmp) %>%
  filter(!is.na(BUS_STOP_N))%>%
  group_by(grid_id) %>%
  summarise(total_trips = sum(TRIPS))
```

### Replicating the Steps for Other Peak Hours

#### Weekday Afternoon Peak

```{r}
total_trips_per_busstop_wdap <- left_join(busstop3414, odbus_weekday_17_20, by = c("BUS_STOP_N" = "ORIGIN_PT_CODE"))%>%
  filter(!is.na(TRIPS))

total_trips_per_grid_wdap <- st_join(honeycomb_grid_sf,total_trips_per_busstop_wdap) %>%
  filter(!is.na(BUS_STOP_N))%>%
  group_by(grid_id) %>%
  summarise(total_trips = sum(TRIPS))
```

#### Weekends/Holiday Morning Peak

```{r}

total_trips_per_busstop_wemp <- left_join(busstop3414, odbus_weekend_11_14, by = c("BUS_STOP_N" = "ORIGIN_PT_CODE"))%>%
  filter(!is.na(TRIPS))

total_trips_per_grid_wemp <- st_join(honeycomb_grid_sf,total_trips_per_busstop_wemp) %>%
  filter(!is.na(BUS_STOP_N))%>%
  group_by(grid_id) %>%
  summarise(total_trips = sum(TRIPS))
```

#### Weekends/Holiday Evening Peak

```{r}
total_trips_per_busstop_weep <- left_join(busstop3414, odbus_weekend_16_19, by = c("BUS_STOP_N" = "ORIGIN_PT_CODE"))%>%
  filter(!is.na(TRIPS))

total_trips_per_grid_weep <- st_join(honeycomb_grid_sf,total_trips_per_busstop_weep) %>%
  filter(!is.na(BUS_STOP_N))%>%
  group_by(grid_id) %>%
  summarise(total_trips = sum(TRIPS))
```

## Geovisualisation and Analysis

After the data preparation, I will first plot the distribution of passenger trips using *`ggplot()`* of **tidyverse** package. To consolidate these distributions into a single plot, it is necessary to introduce grouping variable to each dataframe. Within this unified plot, key summary statistics including Q1, median, Q3, and mean highligthed by a red circle will be presented. This approach aims to provide a comprehensive comparison of passenger trip characteristics across various peak hour groups.

```{r}
# Add a grouping variable to each dataframe
total_trips_per_grid_wdmp$Peak_Hour_Group <- "wdmp"
total_trips_per_grid_wdap$Peak_Hour_Group <- "wdap"
total_trips_per_grid_wemp$Peak_Hour_Group <- "wemp"
total_trips_per_grid_weep$Peak_Hour_Group <- "weep"

# Combine dataframes into a single dataframe
combined_peak_hour <- rbind(total_trips_per_grid_wdmp, total_trips_per_grid_wdap, total_trips_per_grid_wemp, total_trips_per_grid_weep) %>%
  filter(!is.na(total_trips))

# Create a box plot using ggplot
ggplot(combined_peak_hour, aes(x = Peak_Hour_Group, y = total_trips)) +
  geom_boxplot() +
  stat_summary(fun = "mean", geom = "point", shape = 18, size = 3, color = "red")+
  stat_boxplot(geom = 'errorbar', width = 0.5, color = 'blue', size = 1)+
  stat_summary(
    geom = 'text',
    fun.min = function(x) quantile(x, 0.25),
    fun = median,
    fun.max = function(x) quantile(x, 0.75),
    aes(label = sprintf("Q3: %.2f\nMedian: %.2f\nQ1: %.2f", ..ymax.., ..y.., ..ymin..)),
    vjust = -5,
    hjust= -0.08,
    position = position_dodge(width = 0.75),
    size = 3
  ) +
  stat_summary(geom = 'text', fun = mean, aes(label = sprintf("Mean: %.2f", ..y..)), vjust = -3, hjust=-0.08 ,  position = position_dodge(width = 0.9), color = 'red', size =3)+
  
  labs(title = "Boxplot for Total Trips in each Peak Hour Group", x="Peak Hour Group",y = "Passenger Trips") +
  theme_minimal()

```

The box plot analysis above reveal the patterns in Singapore's bus passenger trips. Notably, the mean passenger trips during weekdays significantly surpass those on weekends and holidays, suggesting higher demand for bus services during typical workdays. This aligns with a common observation that commuting is more crowded on workdays, reflecting the daily hustle and bustle of the workforce.

Furthermore, the observation that the mean of all peak hour groups exceeds their respective medians indicates a right-skewed distribution. This skewness implies that on average, there are more instances of relatively small number of passenger trips during peak hour with occasional instances of significantly higher demand. This distribution pattern underscores the challenges faced by commuters during peak hours, where a substantial portion of bus rides may experience higher congestion.

Some hexagonal grid has a total of more than 400,000 passenger trips highlighting that specific areas with exceptionally high demand during the weekdays. These areas are likely represent key commuting regions with concentrated commercial and residential activities and are possibly areas that are not easily accessible by MRT. To gain clearer insights on the concentrations of high passenger trips I will leverage **tmap** package.

### Weekday Morning Peak

```{r}
tmap_mode("view")

tm_shape(total_trips_per_grid_wdmp) +
  tm_fill(
    col = "total_trips",
    palette = c("#C5FFF8", "#FF4B91"),
    style = "cont",
    title = "Number of Trips",
    id = "grid_id",
    showNA = FALSE,
    alpha = 0.6,
    popup.vars = c("grid_id","total_trips"),
    popup.format = list(
      grid_id = list(format = "f", digits = 0),
      total_trips = list(format = "f", digits = 0))
  ) +
  tm_borders(col = "grey40", lwd = 0.7)
tmap_mode("plot")
```

It has been observed that certain bus stops are located outside the boundaries of Singapore, particularly in Johor.The influx of weekday morning peak passenger trips from Johor are qhigh surpassing 100,000 for October 2023. This significant amount shows the cross border commuting activity between Johor and Singapore suggesting a preference among some individuals to reside in Johor may be due to cost consideration while working in Singapore.

The Central Business District (CBD) area displays a comparatively lower passenger trips generated by origin bus stop. This is attributed to the absence of major bus interchanges within the CBD, suggesting that commuters in this central business hub may rely on alternative modes of transportation, such as the Mass Rapid Transit (MRT) system. In addition, it is worth noting that lower passenger trips may also be influences by the fact that bus routes typically do not commence within CBD area, even though the bus routes pass through CBD.

Areas that are exhibiting a high concentration during weekday morning peak are associated with prominent bus interchanges such as Woodlands and Boon Lay with more than 300,000 passenger trips within a single month on weekdays. Additionally, other bus interchanges including Bishan, Ang Mo Kio, Toa Payoh, Clementi, Punggol, Tampines, and Bedok also shows high passenger trips.

### Weekday Afternoon Peak

```{r}
tmap_mode("view")

tm_shape(total_trips_per_grid_wdap) +
  tm_fill(
    col = "total_trips",
    palette = c("#C5FFF8", "#FF4B91"),
    style = "cont",
    title = "Number of Trips",
    id = "grid_id",
    showNA = FALSE,
    alpha = 0.6,
    popup.vars = c("grid_id","total_trips"),
    popup.format = list(
      grid_id = list(format = "f", digits = 0),
      total_trips = list(format = "f", digits = 0))
  ) +
  tm_borders(col = "grey40", lwd = 0.7)
tmap_mode("plot")
```

During the weekday afternoon peak, Woodlands and Boon Lay continue to exhibit an impressive volume of passenger trips exceeding 400,000 as compared to the patterns observed during morning peak. In addition, there are additional areas such as Ang Mo Kio, Tampines, and Bedok which emerge as significant contributors to the passenger trips during the later peak period.

Typically the passengers volume during weekday afternoon are higher than weekday morning with Boon Lay with the highest contributor of more than 500,000 passenger trips. Boon Lay serves as a transportation hub for workers, residents and students from NTU. Moreover, the areas near Boon Lay are currently not accessible through MRT as the development of MRT is still in progress.

### Weekends/Holiday Morning Peak

```{r}
tmap_mode("view")
tm_shape(total_trips_per_grid_wemp) +
  tm_fill(
    col = "total_trips",
    palette = c("#C5FFF8", "#FF4B91"),
    style = "cont",
    title = "Number of Trips",
    id = "grid_id",
    showNA = FALSE,
    alpha = 0.6,
    popup.vars = c("grid_id","total_trips"),
    popup.format = list(
      grid_id = list(format = "f", digits = 0),
      total_trips = list(format = "f", digits = 0))
  ) +
  tm_borders(col = "grey40", lwd = 0.7)
tmap_mode("plot")
```

Given that weekends consist of only two days, the passenger trip numbers exhibit lower in total numbers but comparable to those recorded during five weekdays, following a similar pattern. Notably, major bus interchanges, such as Woodlands, Boon Lay, Bedok and Tampines continue to play a pivotal role contributing significantly to the overall passenger trips during both weekdays and weekends.

During weekend and holidays, the morning peak hour (11am to 2pm) is later as compared to weekday morning (6 to 9am). This temporal shift can be attributed to social dynamic where individuals engaging in lunchtime activity with family and friends during weekends. This distinctive pattern underscores the influence of social interactions on commuter behaviors during non-working days.

### Weekends/Holiday Evening Peak

```{r}
tmap_mode("view")
tm_shape(total_trips_per_grid_weep) +
  tm_fill(
    col = "total_trips",
    palette = c("#C5FFF8", "#FF4B91"),
    style = "cont",
    title = "Number of Trips",
    id = "grid_id",
    showNA = FALSE,
    alpha = 0.6,
    popup.vars = c("grid_id","total_trips"),
    popup.format = list(
      grid_id = list(format = "f", digits = 0),
      total_trips = list(format = "f", digits = 0))
  ) +
  tm_borders(col = "grey40", lwd = 0.7)

```

Similar with the other peak hours, the passenger trips are mostly contributed by major bus interchange as the origin.

Typically , the afternoon/evening peak hours witnesses a higher volume of passenger trips both on weekdays and weekend as compared to morning peak hours. This preference may be due to individuals opting for MRT during hours as it provide punctual mode of transportation as compared to bus which might be affected by congestion and could potentially lead to delays in reaching to work/schools/leisure activities.

During weekend and holidays, the evening peak hour (4 to 7pm) is slightly earlier as compared to weekday morning afternoon (5 to 8pm). This shift in timing could be attributed to the altered schedules and leisurely activities that individuals typically engage in during weekends and holidays. People might be more inclined to initiate their evening commutes earlier to accommodate social plans, family gatherings, or recreational activities that are common during non-working days.

## **Exploratory Spatial Data Analysis**

Exploratory Spatial Data Analysis or ESDA consists of descriptive techniques to discover spatial distribution of data and identify outliers.

In this section, I will cover global spatial autocorrelation which focuses on overall tredn and local spatial autocorrelation which focuses on identifying hot and cold spots in the data.

### **Global Spatial Autocorrelation**

In this section, I will include the computation of global spatial autocorrelation statistics and spatial complete randomness test for global spatial autocorrelation. The goal here is to understand whether the passenger trips generated by origin are evenly distributed across Singapore.

#### **Spatial Weights Matrix**

Before computing global spatial autocorrelation, we need to define spatial neighbourhood by using spatial weight. There are two common methods to compute spatial weight which are contiguity-based and distanced-based.

In contiguity-based method, neighbour share common boundary and there are 2 methods in defining the boundary, ROOK by common edge while QUEEN by common edge and vertices as shown below for square shape.

![Hook Neighbors](data/figure/Rook.png){width="205"}

![Queen Neighbors](data/figure/Queen.png){width="206"}

In hexagonal grid, finding neighbours are straighforward. Both ROOK and QUEEN yield the same results as shown below.

![Hexagon Neighbors](data/figure/Hexagonal_Neighbors.png){alt="Hexagon Neighbors" width="194"}

```         
![Hexagon Neighbors](data/figure/Hexagonal_Neighbors.png)
![Hook Neighbors](data/figure/Rook.png)
![Queen Neighbors](data/figure/Queen.png)
```

In distance-based method, there are 2 method fixed weighting where the grid are considered neighbours if they are within specified distance from one another and adaptive weighting where each grid has same specified number of neighbours.

If the hexagonal grid are isolated from each other, contiguity-based method may not be appropriate as it may yield many grids with no neighbours.

#### Contiguity Weight Matrix (QUEEN)

In the code chunk below, I will use [*`poly2nb()`*](https://r-spatial.github.io/spdep/reference/poly2nb.html) of **spdep** package to compute contiguity weight matrices for weekday morning peak hour. This function builds a list of neighbours based on grids with contiguous boundaries. By default, Queen contiguity is applied.

```{r}
wm_q <- poly2nb(total_trips_per_grid_wdmp, queen=TRUE)
summary(wm_q)
```

The summary report above shows that there are 1492 hexagonal grids. There are 12 hexagonal grid with no neighbour, 40 hexagonal grids with 1 neighbour and the most connected grids have 6 links. The average number of links is 4.5.

#### Contiguity Weight Matrix (ROOK)

In the code chunk below, I will use [*`poly2nb()`*](https://r-spatial.github.io/spdep/reference/poly2nb.html) of **spdep** package to compute contiguity weight matrices for weekday morning peak hour by specifying `queen = FALSE` to compute Rook contiguity.

```{r}
wm_r <- poly2nb(total_trips_per_grid_wdmp, queen=FALSE)
summary(wm_r)
```

The summary report above shows that there are 1492 hexagonal grids. There are 12 hexagonal grid with no neighbour, 40 hexagonal grids with 1 neighbour and the most connected grids have 6 links. The average number of links is 4.5.

Note: The results for both rook and queen method are the same as shown from the computation above.

#### Visualising contiguity weights

Before visualising the weights, I need to reproject coordingate to WGS84 for longitude-latitude projection using *`st_transform`* of **sf** package.

```{r}
total_trips_per_grid_wdmp$area_honeycomb_grid <- st_transform(total_trips_per_grid_wdmp$area_honeycomb_grid, "+proj=longlat +datum=WGS84")
```

Next, I will get the coordinates of the hexagonal grid centroid in longitude and latitude using the *`st_coordinates`* and *`st_centroid`* of **sf** package.

```{r}
coords <- st_coordinates(st_centroid(total_trips_per_grid_wdmp$area_honeycomb_grid))
head(coords)
```

I will only plot queen contiguity as rook contiguity yields the same results.

```{r}
plot(total_trips_per_grid_wdmp$area_honeycomb_grid, border="lightgrey", main="Queen and Rook Contiguity")
plot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= "red")
```

#### Fixed Distance Weight Matrix

*The [dnearneigh()](https://r-spatial.github.io/spdep/reference/dnearneigh.html)* of **spdep** package will be used to derive the distance-based weight matrices by . This function identifies neighbours of hexagonal grid centroid points by Euclidean distance with a lower and upper bounds distance controlled by the *bounds* argument or by Great Circle distance in kilometres if *longlat* argument is set to TRUE.

##### **Determine the cut-off distance**

-   Return a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using [*`knearneigh()`*](https://r-spatial.github.io/spdep/reference/knearneigh.html) of **spdep.**

-   Convert the knn object returned by *knearneigh()* into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using [*`knn2nb()`*](https://r-spatial.github.io/spdep/reference/knn2nb.html).

-   Return the length of neighbour relationship edges by using [*`nbdists()`*](https://r-spatial.github.io/spdep/reference/nbdists.html) of **spdep**. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.

-   Remove the list structure of the returned object by using [`unlist()`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/unlist).

```{r}
k1 <- knn2nb(knearneigh(coords))
k1dists <- unlist(nbdists(k1, coords, longlat = TRUE))
summary(k1dists)
```

The summary report shows that the largest first nearest neighbour distance is 4,582.5 metres, so using a number slightly larger than this (i.e. 4.6) as the upper threshold gives certainty that all regions will have at least one neighbour.

```{r}
total_trips_per_grid_wdmp$grid_id[match(max(k1dists), k1dists)]
```

Using the code chunk above, we discover that the grid_id with the maximum distance to its nearest neighbour is 1767 which is the grid in Johor.

##### **Computing fixed distance weight matrix**

Now, we will compute the distance weight matrix by using [*dnearneigh()*](https://r-spatial.github.io/spdep/reference/dnearneigh.html) as shown below.

```{r}
wm_d4.6 <- dnearneigh(coords, 0, 4.6, longlat = TRUE)
wm_d4.6
```

From the output, we see that the average number of links is 158.4651. The number is quite high and may skew the analysis.

Next, we will use *str()* to display the content of `wm_d4.6` weight matrix.

```{r}
str(wm_d4.6)
```

```{r}
par(mfrow = c(1,2))
plot(total_trips_per_grid_wdmp$area_honeycomb_grid, border = "lightgrey",main="1st nearest neighbours" )
plot(k1, coords, add = TRUE, col = "red", length = 0.88, )

plot(total_trips_per_grid_wdmp$area_honeycomb_grid, border = "lightgrey", main = "Distance Link")
plot(wm_d4.6, coords, add = TRUE, pch = 19, cex = 0.6)
```

Due to a high number of links, we have very dense graphs which make it difficult to interpret. However, we can still make some observations:

-   The above charts actually illustrates a characteristic of fixed distance weight matrix whereby the hexagonal grid of busttop origin in the centre of Singapore tend to have more neighbours and the edges of Singapore with lesser neighbours like Johor, Tanah Merah Coast.

-   Based on the above charts, we can tell that the geographical areas of the regions in Singapore are highly connected by the bus.

#### Adaptive Distance Weight Matrix

To overcome the issue of fixed distance weight matrix where there is uneven distribution of neighbours, we can use directly control the numbers of neighbours using k-nearest neighbours, as shown in the code chunk below.

I will set k = 6 i.e., all hexagonal grids will have 6 neighbours for hexagonal grids.

```{r}
knn6 <- knn2nb(knearneigh(coords, k=6))
knn6
```

#### Plotting distance based neighbours

```{r}
par(mfrow = c(1,2))
plot(total_trips_per_grid_wdmp$area_honeycomb_grid, border = "lightgrey",main="6 nearest neighbours" )
plot(knn6, coords, add = TRUE, col = "red", length = 0.88, )

plot(total_trips_per_grid_wdmp$area_honeycomb_grid, border = "lightgrey", main = "Distance Link w KNN")
plot(knn6, coords, add = TRUE, pch = 19, cex = 0.6)


```

#### Determining Which Weights Matrix to Use

Selecting a spatial weight matrix is use is dependent on the geographical area of interest and the focus of the study. Contiguity-based is preferred for hexagonal grid with uniform sizes because contiguity matrices are well-suited for regular grids where neighboring units share common boundaries. However, in my case, there are some hexagonal grid with no neighbour making contiguity-based not preferable. Therefore, I will use distance-based methods with adaptive distance spatial weight matrix because fixed distance has disadvantage where some regions only have 1 neighbour, while others may have 158 neighbours.

#### **Row-Standardised Weights Matrix**

After selecting the weight matrix to use, I will now assign weights to each neighboring polygon. Each neighboring polygon will be assigned equal weight (style=\"W\") by assigning the fraction 1/(#of neighbors) to each neighbouring area. This is also known as a row-standardised matrix where each row in the matrix sums to 1.

```{r}
rswm_knn6 <- nb2listw(knn6,
                   style = "W",
                   zero.policy = TRUE)
rswm_knn6
```

I will be using the row-standardised weight matrix for the next part of the analysis.

Notes:

The input of *nb2listw()* must be an object of class **nb**. The syntax of the function has two major arguments, namely style and zero.poly.

-   *style* can take values \"W\", \"B\", \"C\", \"U\", \"minmax\" and \"S\". B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).

-   If *zero policy* is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %\*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice.

#### **Computing Global Spatial Autocorrelation Statistics**

This in sub-section, I will use two methods: Moran\'s I and Geary\'s C to test the hypothesis the following hypothesis:

-   H0: Observed spatial patterns of values is equally likely as any other spatial pattern i.e. data is randomly disbursed, no spatial pattern

-   H1: Data is more spatially clustered than expected by chance alone.

**Moran\'s I**

I will perform Moran\'s I statistical testing by using [*moran.test()*](https://r-spatial.github.io/spdep/reference/moran.test.html) of **spdep**. Moran\'s I describe how features differ from the values in the study area as a whole. The Moran I statistic ranges from -1 to 1. If the Moran I is:

-   positive (I\>0): Clustered, observations tend to be similar

-   negative (I\<0): Disperse, observations tend to be dissimilar

-   approximately zero: observations arranged randomly over space

The below code chunk will perform the Moran\'s I test on the passenger trips generate by origin.

```{r}
moran.test(total_trips_per_grid_wdmp$total_trips,
           listw = rswm_knn6,
           zero.policy = TRUE,
           na.action = na.omit)
```

Since the p-value \< 0.05, I have sufficient statistical evidence to reject the null hypothesis at the 95% level of confidence. This means that data is more spatially clustered than expected by chance alone. Since Moran I statistics are larger than 0, the observation are clustered, observations tend to be similar.

**Computing Monte Carlo Moran\'s I**

If there are doubts that the assumptions of Moran\'s I are true (normality and randomisation), I will use a Monte Carlo simulation to perform a permutation test for Moran\'s I.

The permutation tests consists of randomly reassigning the attribute values to a cell under the assumption of no spatial pattern. This random assignment is conducted n times. Each time, I will compute the Moran\'s I to creating an empirical distribution of Moran\'s I under H0.

The code chunk below performs permutation test for Moran\'s I statistic by using [*moran.mc()*](https://r-spatial.github.io/spdep/reference/moran.mc.html) of **spdep**. A total of 1000 simulation will be performed.

```{r}
set.seed(1234)
bperm= moran.mc(total_trips_per_grid_wdmp$total_trips, 
                listw=rswm_knn6, 
                nsim=999, 
                zero.policy = TRUE, 
                na.action=na.omit)
bperm
```

Since the p-value is \< 0.05, we have sufficient statistical evidence to reject the null hypothesis at the 95% level of confidence. This means that data is more spatially clustered than expected by chance alone.

**Visualising Monte Carlo Moran\'s I**

The code chunk below will be used to plot the distribution of the simulated Moran\'s I test statistics as a histogram using *`ggplot2`* from **tidyverse** package**.**

```{}
```

## Task 2: **Local Indicators of Spatial Association (LISA) Analysis**

### **Deriving Continuity Spatial Weights: Queen's Method**

wm_q \<- hunan_GDPPC %\>% mutate(nb = st_contiguity(geometry), wt = st_weights(nb, style = "W"), .before=1)

Local Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable.

In this section, you will learn how to apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran'I to detect cluster and/or outlier from passenger trips generate by origin at hexagon level.

The code chunks below are used to compute local Moran's I of Total Trips at the hexagonal grid level.

fips \<- order(total_trips_per_grid_wdmp\$grid_id) localMI \<- localmoran(hunan\$GDPPC, rswm_q) head(localMI)

Reproducible:

just change the read_csv and the file
