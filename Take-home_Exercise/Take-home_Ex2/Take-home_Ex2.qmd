---
title: "Take-home-Ex2:Applied Spatial Interaction Models: A case study of Singapore public bus commuter flows"
author: "Widya Tantiya Yutika"
date: "08 December 2023"
date-modified: "last-modified"
format: html
execute: 
  echo: true
  eval: true
  warning: false
editor: visual
---

## Setting the Scene

Urban mobility challenges encompass understanding the driving forces behind early morning commutes for city dwellers and evaluating the impact of removing public bus services along specific routes, presenting complex issues for transport operators and urban managers. Traditional commuter surveys, though effective, are costly and time-consuming. With the digitization of urban infrastructures, particularly public transportation, massive geospatial data sets are generated through technologies like GPS and SMART cards. However, the inability to efficiently utilize this data can hinder effective decision-making. This exercise aims to address two key issues: the lack of research on integrating diverse open data sources for policymaking and the insufficient exploration of geospatial data science for decision support.

## Objective

The objective is to conduct a case study showcasing the potential of geospatial data science in integrating publicly available data to build spatial interaction models, explaining factors influencing urban mobility patterns in public bus.

## **The Study Area and Data**

### Aspatial Data

-   Open Government Data

    -   *Passenger Volume by Origin Destination Bus Stops* from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html).

    -   *School Directory and Information* from [Data.gov.sg](https://beta.data.gov.sg/).

-   Instructor-curated Datasets for Educational Purpose

    -   HDB: This data set is the geocoded version of *HDB Property Information* data from [Data.gov.sg](https://beta.data.gov.sg/). The data set is prepared using September 2021 data.

### Geospatial Data

-   Open Government Data

    -   *Bus Stop Location*, *Train Station* and *Train Station Exit Point from* [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html).

    -   *Master Plan 2019 Subzone Boundary* from [Data.gov.sg](https://beta.data.gov.sg/).

-   Instructor-curated Datasets for Educational Purpose

    -   *Business*, *entertn*, *F&B*, *FinServ*, *Leisure&Recreation* and *Retails* consisting locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets.

## The Task

The specific tasks of this take-home exercise are as follows:

### **Geospatial Data Science**

-   Derive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the [traffic analysis zone (TAZ)](https://tmg.utoronto.ca/files/Reports/Traffic-Zone-Guidance_March-2021_Final.pdf).

-   With reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating *Passenger Volume by Origin Destination Bus Stops* and *Bus Stop Location* from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html). The O-D matrix must be aggregated at the analytics hexagon level

    |       Peak hour period       | Bus tap on time |
    |:----------------------------:|:---------------:|
    |     Weekday morning peak     |   6am to 9am    |
    |    Weekday afternoon peak    |   5pm to 8pm    |
    | Weekend/holiday morning peak |   11am to 2pm   |
    | Weekend/holiday evening peak |   4pm to 7pm    |

-   Display the O-D flows of the passenger trips by using appropriate geovisualisation methods (not more than 5 maps).

-   Describe the spatial patterns revealed by the geovisualisation (not more than 100 words per visual).

-   Assemble at least three propulsive and three attractiveness variables by using aspatial and geospatial from publicly available sources.

-   Compute a distance matrix by using the analytical hexagon data derived earlier.

### **Spatial Interaction Modelling**

-   Calibrate spatial interactive models to determine factors affecting urban commuting flows at the selected time interval.

-   Present the modelling results by using appropriate geovisualisation and graphical visualisation methods. (Not more than 5 visuals)

-   With reference to the Spatial Interaction Model output tables, maps and data visualisation prepared, describe the modelling results. (not more than 100 words per visual).

## **Setting the Analytical Tools**

Before I get started, I need to ensure that **sf**, **spdep**, **tmap**, **tidyverse,** and **knitr** packages of R are currently installed in my R.

-   *sf* : for importing and handling geospatial data in R,

-   *spdep* : for computing spatial weights, global and local spatial autocorrelation statistics, and

-   *tmap* : for preparing cartographic quality chropleth map

-   *tidyverse* : for wrangling attribute data in R ; [tidyverse](https://www.tidyverse.org/) has already included collection of packages such as readr, ggplot2, dplyr, tiblle, purr, etc.

-   knitr: for facilitating dynamic report generation in R Markdown documents.

-   sp:???

-   reshape2

-   stplanr

-   httr

The code chunk below is used to ensure that the necessary R packages have been installed , if it is yet to be installed, it will then be installed and ready to be used in the R environment.

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
pacman::p_load(sf, spdep, tmap, tidyverse, knitr, sp, reshape2, stplanr, httr)
```

## **Working with Aspatial Data (Commuter Flow)**

### **Importing OD Csv File into R Environment**

Next, I will import *Passenger Volume by Origin Destination Bus Stops* data set: *origin_destination_bus_202310.csv downloaded from LTA Datamall for October 2023* into R by using [*`st_read()`*](#0){style="font-size: 11pt;"} of **readr** package. The output is R dataframe class.

```{r}
#| code-fold: true  
#| code-summary: "Show the code"  
odbus <- read_csv("data/aspatial/origin_destination_bus_202310.csv")
```

The code chunk below uses [*`glimpse()`*](https://www.rdocumentation.org/packages/dplyr/versions/1.0.10/topics/glimpse) of **dplyr** package to display the odbus tibble data tables.

```{r}
#| code-fold: true  
#| code-summary: "Show the code"
glimpse(odbus)
```

From the glimpse() check above, it is shown that the ORIGIN_PT_CODE and DESTINATION_PT_CODE are in character type. Both of them need to be converted to factor type to work with categorical variables so that I can use them to georeference with bus stop location data.

```{r}
#| code-fold: true  
#| #| #| code-summary: "Show the code"  
odbus$ORIGIN_PT_CODE <- as.factor(odbus$ORIGIN_PT_CODE)
odbus$DESTINATION_PT_CODE <- as.factor(odbus$DESTINATION_PT_CODE) 
```

Next, I will confirm the data type changes using glimpse().

```{r}
#| code-fold: true  
#| code-summary: "Show the code"  
glimpse(odbus)
```

### Checking for Duplicate Commuter Flow

```{r}
#| code-fold: true 
#| #| code-summary: "Show the code" 
duplicate_rec <- odbus %>%   
  group_by_all() %>%   
  filter(n()>1) %>%   
  ungroup()
```

There is no duplicate record found on odbus dataset.

### Extracting the Study Data

For the purpose of this take-home exercise, I will extract commuting flows on Weekend/holiday morning peak which is between 11am to 2pm.

```{r}
#| code-fold: true   
#| #| code-summary: "Show the code"
odbus11_14 <- odbus %>%      
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%      
  filter(TIME_PER_HOUR >= 11 & TIME_PER_HOUR <= 14) %>%
  group_by(ORIGIN_PT_CODE,DESTINATION_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))
```

```{r}
glimpse(odbus11_14)
```

## Working with Geospatial Data

### **Importing Shapefile into R Environment**

-   Bus Stop Location

The code chunk below uses [*`st_read()`*](https://r-spatial.github.io/sf/reference/st_read.html) of **sf** package to import BusStop shapefile into R. The imported shapefile will be **simple features** Object of **sf**. Then, I use [*`st_transform`*](https://r-spatial.github.io/sf/reference/st_transform.html) of **sp** package to convert coordinates to EPSG code of 3414 for SVY21 (projected coordinate system for Singapore).

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
busstop <- st_read(dsn = "data/geospatial", layer = "BusStop") %>%
  st_transform(crs = 3414)
```

#### Checking Duplicate Bus Stop

```{r}
#| code-fold: true 
#| #| code-summary: "Show the code" 
duplicate_bus_stop <- busstop %>%   
  group_by(BUS_STOP_N) %>%   
  filter(n()>1)
```

There are 16 duplicated records with same BUS_STOP_N but different geometry points. However, upon investigation, the coordinates are quite near to each other, this lead to the possibility of temporary bus stop. In view of this, I will remove the duplicated records and only keep the first occurrence using the code chunk below

```{r}
#| code-fold: true 
#| #| code-summary: "Show the code" 
busstop <- busstop %>%   
  distinct(BUS_STOP_N, .keep_all =TRUE)
```

-   *Master Plan 2019 Subzone Boundary*

The code chunk below uses [*`st_read()`*](https://r-spatial.github.io/sf/reference/st_read.html) of **sf** package to import MPSZ-19 shapefile into R. The imported shapefile will be **simple features** Object of **sf**. Then, I use [*`st_transform`*](https://r-spatial.github.io/sf/reference/st_transform.html) of **sp** package to convert coordinates to EPSG code of 3414 for SVY21 (projected coordinate system for Singapore).

```{r}
#| code-fold: true 
#| code-summary: "Show the code" 
mpsz <- st_read(dsn = "data/geospatial", layer = "MPSZ-2019") %>%
  st_transform(crs = 3414)
```

### Creating Honeycomb/Hexagon Layer

Honeycomb layer are preferred to replace coarse and irregular Master Plan 2019 Sub-zone GIS data set of URA because hexagon reduce sampling bias due to its grid shape of low perimeter to are ratio and its ability to form evenly spaced grid. Honeycomb grids are well-suited for approximating circular areas, making them suitable for mapping Singapore edges with irregular shape.

The code chunk below uses [*`st_make_grid`*](https://r-spatial.github.io/sf/reference/st_make_grid.html) of **sf** package to create a hexagonal or honeycomb grid with a 375m (perpendicular distance between the center of hexagon and its edges) to represent the [traffic analysis zone (TAZ)](https://tmg.utoronto.ca/files/Reports/Traffic-Zone-Guidance_March-2021_Final.pdf). According the the R documentation, the cellsize is the distance between opposite edges, which is 2 times the perpendicular distance between the center of hexagon and its edges. Thus, for the purpose of this exercise, I will use cellsize of 750m and indicate the square=FALSE for hexagonal grid. After doing do, I will create a grid_id for each hexagonal grid (2,541 hexagonal grid).

```{r}
#| code-fold: true  
#| code-summary: "Show the code"  
area_honeycomb_grid = st_make_grid(busstop, cellsize=750, what = "polygons", square = FALSE)      
# To sf and add grid ID    
honeycomb_grid_sf = st_sf(area_honeycomb_grid) %>%
  # add grid ID            
  mutate(grid_id = 1:length(lengths(area_honeycomb_grid)))
```

Next, I will only retain the hexagons with at least 1 bus stop in it.

```{r}
honeycomb_grid_sf$busstop_count = lengths(st_intersects(honeycomb_grid_sf, busstop))


honeycomb_with_busstop = filter(honeycomb_grid_sf, busstop_count > 0)
```

There are 834 hexagonal units with at least 1 busstop.

```{r}
plot(honeycomb_with_busstop['busstop_count'], 
     main = 'Bus Stop Count in Each Hexagonal Grid')
```

Some hexagonal grids are highly packed with a maximum of 19 bus stops shown in yellow color above.

### Data Wrangling with Geospatial Data

#### Combining Busstop and honeycomb_grid_sf

In this section, I uses [`st_intersection()`](https://r-spatial.github.io/sf/reference/geos_binary_ops.html#arguments) from **sp** package to overlap the bus stop points and hexagonal grids.

```{r}
busstop_honeycomb <- st_intersection(busstop, honeycomb_with_busstop) %>%
  select(BUS_STOP_N, LOC_DESC, grid_id) %>%
  st_drop_geometry()
```

## Build an OD Matrix of Commuter Flows for Weekends/Holiday Morning Peak Hour

In this section, I will construct a matrix for Origin and Destination with the Trips counts on each combinations.

First, I will append "ORIGIN_GRID_ID" and "ORIGIN_LOC_DESC" from busstop_honeycomb data frame onto odbus11_14 data frame using the code chunk below.

```{r}
od_data <- left_join(odbus11_14 , busstop_honeycomb,
            by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>%
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         ORIGIN_GRID_ID = grid_id,
         DESTIN_BS = DESTINATION_PT_CODE,
         ORIGIN_LOC_DESC = LOC_DESC)
```

Next, I will check for duplicated records on od_data using the chunk code below.

```{r}
#| code-fold: true 
#| #| code-summary: "Show the code" 
duplicate_od_data <- od_data %>%   
  group_by_all() %>%   
  filter(n()>1) %>%   
  ungroup()
```

The above code chunk shows that there are no duplicate record found.

Next, I will update od_data data frame by performing another left join with busstop_honeycomb to get the "DESTIN_GRID_ID" and "DESTIN_LOC_DESC".

```{r}
od_data <- left_join(od_data , busstop_honeycomb,
            by = c("DESTIN_BS" = "BUS_STOP_N")) %>%
  rename(DESTIN_GRID_ID = grid_id,
         DESTIN_LOC_DESC = LOC_DESC)
```

```{r}
missing_values <- colSums(is.na(od_data))

# Print the columns with missing values
print(missing_values)
```

The missing grid_id for origin and destination bus stop may be due to the outdated Bus Stop Location Data which is uploaded on July 2023 as compared to the Passenger Volume by Origin Destination Bus Stops data collected in October 2023.

So, I will remove the rows with NA and calculate the number of trips on weekends/holiday morning peak hour by using the group_by "ORIGIN_GRID_ID" and "DESTIN_GRID_ID" and sum all the trips between each combination of hexagonal grid ids using the code chunk below.

```{r}
od_data <- od_data %>%
  drop_na()
```

```{r}
od_data1 <- od_data %>%
  group_by(ORIGIN_GRID_ID, DESTIN_GRID_ID) %>%
  summarise(AFTERNOON_PEAK = sum(TRIPS),
            ORIGIN_DESC = paste(unique(ORIGIN_LOC_DESC), collapse = ', '),
            DESTIN_DESC = paste(unique(DESTIN_LOC_DESC), collapse = ', '))
```

I will check the distribution of the Afternoon Peak trips using summary as shown below.

```{r}
summary(od_data1$AFTERNOON_PEAK)
```

I will save the output into an rds file format.

```{r}
write_rds(od_data, "data/rds/od_data.rds")
```

```{r}
od_data <- read_rds("data/rds/od_data.rds")
```

## **Computing Distance Matrix by** Analytics Hexagon Level

First [`as.Spatial()`](https://r-spatial.github.io/sf/reference/coerce-methods.html) from **sp** package will be used to convert honeycomb_with_busstop from sf tibble data frame to SpatialPolygonsDataFrame of sp object as shown in the code chunk below. This method is used because it is faster to compute distance matrix using sp than sf package.

```{r}
honeycomb_with_busstop_sp <- as(honeycomb_with_busstop, "Spatial") 
honeycomb_with_busstop_sp
```

Next, [`spDists()`](https://www.rdocumentation.org/packages/sp/versions/2.1-1/topics/spDistsN1) of **sp** package will be used to compute the Euclidean distance between the centroids of the hexagons. When longlat is set to FALSE, spDists will return a full matrix of distances in the metric of points. While longlat is set to TRUE, it will return in kilometers. In my case, since there are 834 hexagon, amtrix of 834 x 834 will be created and I will print out the first 8 rows using head().

```{r}
dist <- spDists(honeycomb_with_busstop_sp, 
                longlat = FALSE)
head(dist, n=c(8, 8))
```

From the matrix above, it is noticed that both the column and row headers are not labelled by grid_id. So, I label the headers by first creating a list sorted according to the distance matrix by grid_id and followed by attaching the grid_id to row and column for distance matrix matching.

```{r}
grid_id_desc <- honeycomb_with_busstop$grid_id

colnames(dist) <- paste0(grid_id_desc)
rownames(dist) <- paste0(grid_id_desc)
head(dist, n=c(8,8))
```

Next, I will pivot the distance matrix into a long table by using the row and column grid_id using melt() of reshape2 package to convert wide-format data to long-format data.

For reference : <https://www.statology.org/long-vs-wide-data/>

![Wide Long Format](figure/wide_long_data.png)

```{r}
distPair <- melt(dist) %>%
  rename(dist = value)
head(distPair, 10)
```

There are 695556 rows of distPair as the number of possible pair of hexagon permutations.

```{r}
summary(distPair$dist)
```

From the summary of "dist" above, it is noticed that there are 0 distance, this refer to intra-zonal distance meaning that the trips are taken from same grid id. Since my minimum inter-zonal distance is 750m, it will set my intra-zonal distance to value below 750/2 = 375m. So, I choose a value of 350m for intra-zonal distance.

```{r}
distPair %>%
  filter(dist > 0) %>%
  summary()
```

```{r}
distPair$dist <- ifelse(distPair$dist == 0, 350, distPair$dist)
```

Next, I will rename var1 and var 2 into origin and destination grid_id.

```{r}
distPair <- distPair %>%
  rename(ORIGIN_GRID_ID = Var1,
         DESTIN_GRID_ID = Var2)
```

```{r}
summary(distPair$dist)
```

## **Preparing Flow Data**

In this section, I will rename my od_data1 to flow_data.

```{r}
flow_data <- od_data1

print(head(flow_data))
```

### **Separating Intra-Flow from flow_data Data Frame**

The code chunk below is used to add two new fields in `flow_data` dataframe.

-   'FlowNoIntra' : 0 for intra-zonal flow and AFTERNOON_PEAK for inter-zonal flow

-   'offset' : a very small number (i.e. 0.000001) for intra-zonal flow and 1 for inter-zonal flow

```{r}
flow_data$FlowNoIntra <- ifelse(
  flow_data$ORIGIN_GRID_ID == flow_data$DESTIN_GRID_ID, 
  0, flow_data$AFTERNOON_PEAK)
flow_data$offset <- ifelse(
  flow_data$ORIGIN_GRID_ID == flow_data$DESTIN_GRID_ID, 
  0.000001, 1)
```

I will create 2 separate data frames for intra and inter zonal flow.

```{r}
intra_zonal_flow <- flow_data %>% 
  filter(FlowNoIntra ==0)

inter_zonal_flow <- flow_data %>% 
  filter(FlowNoIntra >0)
```

From a total of 62176 flow data, there are 568 intra-zonal flow and 61,608 inter-zonal flow.

```{r}
intra_zonal_flow
```

```{r}
# Count the number of rows where "origin" contains "TUAS"
count_tuas <- sum(grepl("TUAS", intra_zonal_flow$ORIGIN_DESC, ignore.case = TRUE))

# Print the result
print(paste("Number of rows with 'TUAS' in 'origin':", count_tuas))
```

### **Combining Passenger Volume Data (Inter-Zonal Flow Data) with Distance Value**

Before we can join "*inter_zonal_flow*" and "*distPair"*, I will convert data value type of *ORIGIN_GRID_ID* and *DESTIN_GRID_ID* fields of both dataframe into factor data type.

```{r}
inter_zonal_flow$ORIGIN_GRID_ID <- as.factor(inter_zonal_flow$ORIGIN_GRID_ID) 
inter_zonal_flow$DESTIN_GRID_ID  <- as.factor(inter_zonal_flow$DESTIN_GRID_ID)
distPair$ORIGIN_GRID_ID  <- as.factor(distPair$ORIGIN_GRID_ID)
distPair$DESTIN_GRID_ID  <- as.factor(distPair$DESTIN_GRID_ID )
```

Now, `left_join()` of **dplyr** will be used to combine "*inter_zonal_flow"* dataframe and *distPair* dataframe. The output is called *flow_data1*.

```{r}
flow_data1 <- inter_zonal_flow %>%
  left_join (distPair,
             by = c("ORIGIN_GRID_ID" = "ORIGIN_GRID_ID",
                    "DESTIN_GRID_ID" = "DESTIN_GRID_ID"))
print(head(flow_data1))
```

## **Visualising O-D Flows of Weekends/Holiday Morning Peak Hour**

In this section, I will create a desire line of inter-zonal flow (flow_data1) by using `od2line` of **stplanr** package.

```{r}
flowLine <- od2line(flow = flow_data1, zones = honeycomb_with_busstop, zone_code = "grid_id") 
```

### **Visualising the desire lines**

To visualise the resulting desire lines, the code chunk below is used.

```{r}
tmap_options(check.and.fix = TRUE) 
tmap_mode("plot") 
tm_shape(mpsz)+   
  tm_polygons(alpha=0.5)+ 
tm_shape(honeycomb_with_busstop) +
  tm_polygons(col = "lightblue", alpha=0.5) +    
  flowLine %>%   filter(AFTERNOON_PEAK >= 5000) %>%   tm_shape() +
  tm_lines(lwd = "AFTERNOON_PEAK",                        
           style = "quantile", 
           scale = c(0.1, 1, 3, 5, 7, 10),                        
           n = 6,                        
           alpha = 0.3,
           col="red")+   
  tm_view(set.zoom.limits =c(11,14))+
  tm_layout(main.title = 'OD Flow On Weekends/Holiday Morning Peak Hour' ,
            main.title.position = "center",
            main.title.size = 1.0,
            main.title.fontface = 'bold',
            legend.width=1.2) +
tmap_mode("plot")
```

The blue hexagonal grid refers to the grid with busstop and the plot above only show the flowline with more or equal than 5000trips.

## School Dataset

### **Geocoding using SLA API**

```{r}
#|eval: false
url <- "https://www.onemap.gov.sg/api/common/elastic/search"

csv <- read_csv("data/aspatial/Generalinformationofschools.csv")
postcodes <- csv$'postal_code'

found <- data.frame()
not_found <- data.frame()

for (postcode in postcodes){
  query <- list('searchVal'=postcode,'returnGeom'='Y','getAddrDetails'='Y','pageNum'='1')
  res<-GET(url, query=query)
  
  if((content(res)$found)!=0){
    found <-rbind(found, data.frame(content(res))[4:13])
  } else{
    not_found = data.frame(postcode)
  }
}
```

```{r}
#merged = merge(csv, found, by.x='postal_code', by.y='results.POSTAL', all=TRUE)
#write.csv (merged, file='data/aspatial/schools.csv')
#write.csv (not_found, file ='data/aspatial/not_found.csv')
```

### **Importing and tidying schools data**

```{r}
schools <- read_csv("data/aspatial/schools.csv") %>%
  rename(latitude='results.LATITUDE', longitude='results.LONGITUDE')%>%
  select(postal_code, school_name, latitude,longitude)
```

### **Converting an aspatial data into sf tibble data.frame**

```{r}
schools_sf <- st_as_sf(schools, 
                       coords=c('longitude', 'latitude'),
                       crs=4326) %>%
  st_transform(crs=3414)
```

```{r}
honeycomb_grid_sf$'SCHOOL_COUNT' <- lengths(st_intersects(honeycomb_grid_sf, schools_sf))
```

```{r}
summary(honeycomb_grid_sf$SCHOOL_COUNT)
```

```{r}
tmap_mode("view")
tm_shape(honeycomb_grid_sf %>% filter(SCHOOL_COUNT>0)) +   
  tm_polygons(col = "lightblue") + 
  tm_text("SCHOOL_COUNT", size=0.8) +
  tm_view(set.zoom.limits =c(11,14))


tmap_mode("plot")
```

## HDB Dataset

```{r}
hdb <- read_csv("data/aspatial/hdb.csv")
```

### **Converting an aspatial data into sf tibble data.frame**

```{r}
hdb_sf <- st_as_sf(hdb,                         
                       coords=c('lng', 'lat'),                        
                       crs=4326) %>%   
  st_transform(crs=3414)
```

```{r}
hdb_count <- st_intersection(honeycomb_grid_sf, hdb_sf) %>%
  group_by(grid_id) %>%
  summarise(HDB_DWELLING_UNIT=sum(total_dwelling_units)) %>%
  st_drop_geometry()
```

```{r}
honeycomb_grid_sf = left_join(honeycomb_grid_sf, hdb_count)
```

```{r}
summary(honeycomb_grid_sf$HDB_DWELLING_UNIT)
```

```{r}
#tmap_mode("view")
tmap_mode("plot")
tmap_options(check.and.fix = TRUE) 
tm_shape(mpsz)+
  tm_polygons()+
tm_shape(honeycomb_grid_sf %>% filter(HDB_DWELLING_UNIT>0)) +   
  tm_polygons(col="HDB_DWELLING_UNIT", alpha=0.6, breaks=c(1,800,1600,2400,3200,4800,8000)) + 
  #tm_text("HDB_DWELLING_UNIT", size=0.8) +
  tm_view(set.zoom.limits =c(11,14))


tmap_mode("plot")
```

## Business Layer

```{r}
business_sf <- st_read(dsn="data/geospatial", layer="Business")
```

```{r}
honeycomb_grid_sf$'BUSINESS_COUNT' <- lengths(st_intersects(honeycomb_grid_sf, business_sf))
```

```{r}
summary(honeycomb_grid_sf$'BUSINESS_COUNT')
```

```{r}
#tmap_mode("view")
tmap_mode("plot")
tmap_options(check.and.fix = TRUE) 
tm_shape(mpsz)+
  tm_polygons()+
tm_shape(honeycomb_grid_sf %>% filter(BUSINESS_COUNT>0)) +   
  tm_polygons(col="BUSINESS_COUNT", alpha=0.6, breaks=c(1,5,10,20,30,97)) + 
  #tm_text("BUSINESS_COUNT", size=0.8) +
  #tm_view(set.zoom.limits =c(11,14))
tmap_mode("plot")
```

## Entertainment Layer

```{r}
entertainment_sf <- st_read(dsn="data/geospatial", layer="entertn")
```

```{r}
honeycomb_grid_sf$'ENTERTAINMENT_COUNT' <- lengths(st_intersects(honeycomb_grid_sf, entertainment_sf))
```

```{r}
summary(honeycomb_grid_sf$'ENTERTAINMENT_COUNT')
```

```{r}
tmap_mode("plot")
tmap_options(check.and.fix = TRUE) 
tm_shape(mpsz)+
  tm_polygons()+
tm_shape(honeycomb_grid_sf %>% 
           filter(ENTERTAINMENT_COUNT>0)) +      
  tm_polygons(col="ENTERTAINMENT_COUNT", alpha=0.6, breaks=c(1,4,7,9)) +    
  #tm_view(set.zoom.limits =c(11,14))  
tmap_mode("plot")
```

## F&B

## Financial Service

## Leisure and Recreation

## Retails

## Train Station

```{r}
train_station_sf <- st_read(dsn="data/geospatial", layer="RapidTransitSystemStation") %>%
  st_transform(crs = 3414)
```

```{r}
st_crs(train_station_sf)
```

### Exclude Depot and Facility Center from the dataset

```{r}
filtered_train_station_sf <- train_station_sf %>%
  filter(grepl("STATION", STN_NAM_DE, ignore.case = TRUE))
```

```{r}
honeycomb_grid_sf$'TRAIN_STATION_COUNT' <- lengths(st_intersects(honeycomb_grid_sf, filtered_train_station_sf))
```

```{r}
summary(honeycomb_grid_sf$'TRAIN_STATION_COUNT')
```

```{r}
tmap_mode("plot")
tmap_options(check.and.fix = TRUE) 
tm_shape(mpsz)+
  tm_polygons()+
tm_shape(honeycomb_grid_sf %>% 
           filter(TRAIN_STATION_COUNT>0)) +      
  tm_polygons(col="TRAIN_STATION_COUNT", alpha=0.6, breaks=c(1,3,5,6)) +    
  #tm_view(set.zoom.limits =c(11,14))  
tmap_mode("plot")
```

## MRT Station EXIT

```{r}
MRT_exit_sf <- st_read(dsn="data/geospatial", layer="Train_Station_Exit_Layer") %>%
  st_transform(crs = 3414)
```

```{r}
honeycomb_grid_sf$'MRT_EXIT_COUNT' <- lengths(st_intersects(honeycomb_grid_sf, MRT_exit_sf))
```

```{r}
summary(honeycomb_grid_sf$'MRT_EXIT_COUNT')
```

```{r}
tmap_mode("plot")
tmap_options(check.and.fix = TRUE) 
tm_shape(mpsz)+
  tm_polygons()+
tm_shape(honeycomb_grid_sf %>% 
           filter(MRT_EXIT_COUNT>0)) +      
  tm_polygons(col="MRT_EXIT_COUNT", alpha=0.6, breaks=c(1,4,7,10,13)) +    
  #tm_view(set.zoom.limits =c(11,14))  
tmap_mode("plot")
```

## Flow_Data_Tidy

## 

## Reference

1.  <https://www.statology.org/long-vs-wide-data/>
