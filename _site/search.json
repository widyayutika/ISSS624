[
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html",
    "href": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html",
    "title": "Take-home_Ex1: Public Bus Passengers in Singapore",
    "section": "",
    "text": "The increasing digitization of urban infrastructures, including buses, taxis, mass rapid transit, public utilities and roads, has generated vast datasets capturing movement patterns over space and time. This data, facilitated by technologies such as GPS and RFID, offers valuable insights into human mobility within cities. Smart cards and GPS devices on public buses, for instance, have enabled the collection of routes and ridership data, providing a rich source for understanding urban movement.\nDespite the wealth of data collected, its utilization often remains limited to basic tracking and mapping using Geographic Information System (GIS) applications. This limitation is attributed to the inadequacy of conventional GIS functions in effectively analyzing and modeling spatial and spatio-temporal data.\nThe objectives of this study are centered around employing Exploratory Spatial Data Analysis (ESDA) techniques, specifically Local Indicators of Spatial Association (LISA) and Emerging Hot Spot Analysis (EHSA), to unveil the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore."
  },
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#setting-up",
    "href": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#setting-up",
    "title": "Take-home_Ex1: Public Bus Passengers in Singapore",
    "section": "2 Setting up",
    "text": "2 Setting up\n\n2.1 The Study Area and Data\n\n2.1.0.1 Aspatial Data\nThe Aspatial data used in this take-home exercise is extracted from LTA DataMall (Passenger Volume by Origin Destination Bus Stops)\n\n\n2.1.0.2 Geospatial Data\nTwo geospatial data sets will be used in this take-home exercise, which are:\n\nBus Stop Location from LTA DataMall, which provides information about all the bus stops currently being serviced by buses, including the bus stop code (identifier) and location coordinates.\nhexagon, a hexagon layer of 250m (this distance is the perpendicular distance between the centre of the hexagon and its edges.) should be used to replace the relative coarse and irregular Master Plan 2019 Planning Sub-zone GIS data set of URA.\n\n\n\n\n2.2 Setting the Analytical Tools\nBefore I get started, I need to ensure that sf, spdep, tmap, tidyverse, and knitr packages of R are currently installed in my R.\n\nsf : for importing and handling geospatial data in R,\nspdep : for computing spatial weights, global and local spatial autocorrelation statistics, and\ntmap : for preparing cartographic quality chropleth map\ntidyverse : for wrangling attribute data in R\nknitr: for facilitating dynamic report generation in R Markdown documents.\n\nThe code chunk below is used to ensure that the necessary R packages have been installed in the R environment to be used.\n\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)"
  },
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#getting-the-data-into-r-environment",
    "href": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#getting-the-data-into-r-environment",
    "title": "Take-home_Ex1: Public Bus Passengers in Singapore",
    "section": "4 Getting the Data into R Environment",
    "text": "4 Getting the Data into R Environment\n\n4.1 Importing Shapefile into R Environment\nThe code chunk below uses st_read() of sf package to import BusStop shapefile into R. The imported shapefile will be simple features Object of sf.\n\n\nShow the code\nbusstop &lt;- st_read(dsn = \"data/geospatial\", layer = \"BusStop\")\n\n\nReading layer `BusStop' from data source \n  `W:\\widyayutika\\ISSS624\\Take-home_Exercise\\Take-home_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\nThe code chunk below uses st_geometry() of sf package to display basic information of feature class.\n\n\nShow the code\nst_geometry(busstop)\n\n\nGeometry set for 5161 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nThe code chunk below uses glimpse() of dplyr package to display the data type of each fields.\n\n\nShow the code\nglimpse(busstop)\n\n\nRows: 5,161\nColumns: 4\n$ BUS_STOP_N &lt;chr&gt; \"22069\", \"32071\", \"44331\", \"96081\", \"11561\", \"66191\", \"2338…\n$ BUS_ROOF_N &lt;chr&gt; \"B06\", \"B23\", \"B01\", \"B05\", \"B05\", \"B03\", \"B02A\", \"B02\", \"B…\n$ LOC_DESC   &lt;chr&gt; \"OPP CEVA LOGISTICS\", \"AFT TRACK 13\", \"BLK 239\", \"GRACE IND…\n$ geometry   &lt;POINT [m]&gt; POINT (13576.31 32883.65), POINT (13228.59 44206.38),…\n\n\nNext, I will plot the geospatial data using the code chunk below.\n\n\nShow the code\nplot(busstop)\n\n\n\n\n\nFrom the glimpse() check above, it is shown that the BUS_STOP_N is in character type. It needs to be converted to factor type to work with categorical variables so that I can use them to georeference with bus stop location data.\n\n\nShow the code\nbusstop$BUS_STOP_N &lt;- as.factor(busstop$BUS_STOP_N)\n\n\nNext, I will confirm the data type for BUS_STOP_N has changed to data type of “factor” using glimpse().\n\n\nShow the code\nglimpse(busstop)\n\n\nRows: 5,161\nColumns: 4\n$ BUS_STOP_N &lt;fct&gt; 22069, 32071, 44331, 96081, 11561, 66191, 23389, 54411, 285…\n$ BUS_ROOF_N &lt;chr&gt; \"B06\", \"B23\", \"B01\", \"B05\", \"B05\", \"B03\", \"B02A\", \"B02\", \"B…\n$ LOC_DESC   &lt;chr&gt; \"OPP CEVA LOGISTICS\", \"AFT TRACK 13\", \"BLK 239\", \"GRACE IND…\n$ geometry   &lt;POINT [m]&gt; POINT (13576.31 32883.65), POINT (13228.59 44206.38),…\n\n\n\n\n4.2 Importing Csv File into R Environment\nNext, I will import origin_destination_bus_202310.csv into R by using st_read() of readr package. The output is R dataframe class.\n\n\nShow the code\nodbus &lt;- read_csv(\"data/aspatial/origin_destination_bus_202310.csv\")\n\n\nThe code chunk below uses glimpse() of dplyr package to display the odbus tibble data tables.\n\n\nShow the code\nglimpse(odbus)\n\n\nRows: 5,694,297\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-10\", \"2023-10\", \"2023-10\", \"2023-10\", \"2023-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKENDS/HOLIDAY\", \"WEEKDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;dbl&gt; 16, 16, 14, 14, 17, 17, 17, 7, 14, 14, 10, 20, 20,…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;chr&gt; \"04168\", \"04168\", \"80119\", \"80119\", \"44069\", \"2028…\n$ DESTINATION_PT_CODE &lt;chr&gt; \"10051\", \"10051\", \"90079\", \"90079\", \"17229\", \"2014…\n$ TOTAL_TRIPS         &lt;dbl&gt; 3, 5, 3, 5, 4, 1, 24, 2, 1, 7, 3, 2, 5, 1, 1, 1, 1…\n\n\nFrom the glimpse() check above, it is shown that the ORIGIN_PT_CODE and DESTINATION_PT_CODE are in character type. Both of them need to be converted to factor type to work with categorical variables so that I can use them to georeference with bus stop location data.\n\n\nShow the code\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE) \n\n\nI will also change the data type of TIME_PER_HOUR and TOTAL_TRIPS from &lt;dbl&gt; to &lt;int&gt; because it both of these fields should be represented as whole numbers.\n\n\nShow the code\nodbus$TIME_PER_HOUR &lt;- as.integer(odbus$TIME_PER_HOUR)\nodbus$TOTAL_TRIPS &lt;- as.integer(odbus$TOTAL_TRIPS) \n\n\nNext, I will confirm the data type for ORIGIN_PT_CODE and DESTINATION_PT_CODE have changed to factor using glimpse().\n\n\nShow the code\nglimpse(odbus)\n\n\nRows: 5,694,297\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-10\", \"2023-10\", \"2023-10\", \"2023-10\", \"2023-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKENDS/HOLIDAY\", \"WEEKDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;int&gt; 16, 16, 14, 14, 17, 17, 17, 7, 14, 14, 10, 20, 20,…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;fct&gt; 04168, 04168, 80119, 80119, 44069, 20281, 20281, 1…\n$ DESTINATION_PT_CODE &lt;fct&gt; 10051, 10051, 90079, 90079, 17229, 20141, 20141, 1…\n$ TOTAL_TRIPS         &lt;int&gt; 3, 5, 3, 5, 4, 1, 24, 2, 1, 7, 3, 2, 5, 1, 1, 1, 1…"
  },
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#data-wrangling",
    "href": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#data-wrangling",
    "title": "Take-home_Ex1: Public Bus Passengers in Singapore",
    "section": "5 Data Wrangling",
    "text": "5 Data Wrangling\n\n5.1 Checking the Reference Coordinate System of Geospatial Data\nCommon issue in importing geospatial data into R is that the coordinate system of the source data was either missing (due to missing .proj for ESRI shapefile, etc.) or wrongly assigned.\nThe code chunk below uses st_crs() of sf package to retrieve the coordinate reference system of busstop.\n\n\nShow the code\nst_crs(busstop)\n\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nBoth busstop is projected in svy21 as shown from the second line, but at the last line, it is mentioned that the EPSG is 9001. This is wrongly assigned because the correct EPSG code for svy21 is 3414.\n\n\n5.2 Transforming the Projection\nNext, I will transform busstop from geographic coordinate system to projected coordinated system as my analysis will measure distance or/and area.\nThe code chunk below uses st_transform of sp package to convert coordinates to EPSG code of 3414.\n\n\nShow the code\nbusstop3414 &lt;- st_transform(busstop, 3414)\n\n\nNext, I will check the coordinate system after transformation with the code chunk below.\n\n\nShow the code\nst_crs(busstop3414)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nAs noticed from the above, the Projected CRS is now SVY21 / Singapore TM and the last line has changed to EPSG 3414.\n\n\n5.3 Checking Duplicated Records\nThe code chunk below is used to check for duplicated records on odbus.\n\n\nShow the code\nduplicate &lt;- odbus %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\n\nThe above code chunk shows that there is no duplicate record found.\n\n\n5.4 Remove Unnecessary Fields\nFirst, I check the YEAR_MONTH and PT_TYPE unique values by using the table() to create a frequency table of each categorical representation.\n\n\nShow the code\nYEAR_MONTH_counts &lt;- table(odbus$YEAR_MONTH)\nprint(YEAR_MONTH_counts)\n\n\n\n2023-10 \n5694297 \n\n\nShow the code\nDAY_TYPE_counts &lt;- table(odbus$DAY_TYPE)\nprint(DAY_TYPE_counts)\n\n\n\n         WEEKDAY WEEKENDS/HOLIDAY \n         3259419          2434878 \n\n\nShow the code\nPT_TYPE_counts &lt;- table(odbus$PT_TYPE)\nprint(PT_TYPE_counts)\n\n\n\n    BUS \n5694297 \n\n\nFrom the results above, I will exclude YEAR_MONTH and PT_TYPE as they only have single categorical representation using the code chunk below.\n\n\nShow the code\nodbus &lt;- odbus[, !(names(odbus) %in% c(\"YEAR_MONTH\", \"PT_TYPE\"))]\n\n\nI will then use glimpse() to ensure the process is done correctly.\n\n\nShow the code\nglimpse(odbus)\n\n\nRows: 5,694,297\nColumns: 5\n$ DAY_TYPE            &lt;chr&gt; \"WEEKENDS/HOLIDAY\", \"WEEKDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;int&gt; 16, 16, 14, 14, 17, 17, 17, 7, 14, 14, 10, 20, 20,…\n$ ORIGIN_PT_CODE      &lt;fct&gt; 04168, 04168, 80119, 80119, 44069, 20281, 20281, 1…\n$ DESTINATION_PT_CODE &lt;fct&gt; 10051, 10051, 90079, 90079, 17229, 20141, 20141, 1…\n$ TOTAL_TRIPS         &lt;int&gt; 3, 5, 3, 5, 4, 1, 24, 2, 1, 7, 3, 2, 5, 1, 1, 1, 1…"
  },
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#task-1-geovisualisation-and-analysis",
    "href": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#task-1-geovisualisation-and-analysis",
    "title": "Take-home_Ex1: Public Bus Passengers in Singapore",
    "section": "5 Task 1: Geovisualisation and Analysis",
    "text": "5 Task 1: Geovisualisation and Analysis\n\ntmap_mode(\"view\")\n\nmap_honeycomb = tm_shape(total_trips_per_grid) +\n  tm_fill(\n    col = \"total_trips\",\n    palette = \"Reds\",\n    style = \"cont\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.6,\n    popup.vars = c(\"total_trips\"),\n    popup.format = list(\n      total_trips = list(format = \"f\", digits = 0))\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)\n\nmap_honeycomb"
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex2/In-class_Ex2_ver1.html",
    "href": "In-class_Exercise/In-class_Ex2/In-class_Ex2_ver1.html",
    "title": "In-class_Ex2_ver1",
    "section": "",
    "text": "FiveR packages will be used for this in-class exercise, they are: sf, sfdep, tmap, tidyverse, and knitr."
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex2/In-class_Ex2_ver1.html#getting-started",
    "href": "In-class_Exercise/In-class_Ex2/In-class_Ex2_ver1.html#getting-started",
    "title": "In-class_Ex2_ver1",
    "section": "",
    "text": "FiveR packages will be used for this in-class exercise, they are: sf, sfdep, tmap, tidyverse, and knitr."
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex2/In-class_Ex2_ver1.html#the-data",
    "href": "In-class_Exercise/In-class_Ex2/In-class_Ex2_ver1.html#the-data",
    "title": "In-class_Ex2_ver1",
    "section": "2 The Data",
    "text": "2 The Data\nFor the purpose of this in-class exercise, the Hunan data sets will be used. There are two data sets in this use case, they are:\n\nHunan, a geospatial data set in ESRI shapefile format, and\nHunan_2012, an attribute data set in csv format."
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex2/In-class_Ex2_ver1.html#getting-data-in-r-environment",
    "href": "In-class_Exercise/In-class_Ex2/In-class_Ex2_ver1.html#getting-data-in-r-environment",
    "title": "In-class_Ex2_ver1",
    "section": "3 Getting Data in R Environment",
    "text": "3 Getting Data in R Environment\n\n3.1 Importing geospatial data\nptw hunan &lt;- st_read(dsn = \"data/geospatial\", layer = \"Hunan\")}\n\n\n3.2 Importing attribute table\nptw hunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")}\n\n\n3.3 Combining both data frame by using left join\n\nIn order to retain the geospatial properties, the left data frame must be the sf data.frame(i.e. hunan)\n\n\n3.4 Plotting a choropleth map"
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex2/In-class_Ex2_ver1.html#deriving-continuity-spatial-weights",
    "href": "In-class_Exercise/In-class_Ex2/In-class_Ex2_ver1.html#deriving-continuity-spatial-weights",
    "title": "In-class_Ex2_ver1",
    "section": "4 Deriving Continuity Spatial Weights",
    "text": "4 Deriving Continuity Spatial Weights"
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex2/In-class_Ex2_ver1.html#deriving-continuity-spatial-weights-queens-method",
    "href": "In-class_Exercise/In-class_Ex2/In-class_Ex2_ver1.html#deriving-continuity-spatial-weights-queens-method",
    "title": "In-class_Ex2_ver1",
    "section": "5 Deriving Continuity Spatial Weights: Queen’s Method",
    "text": "5 Deriving Continuity Spatial Weights: Queen’s Method\nIn the code below, queen method is used to derive the contiguity weights.\nptw wm_q &lt;- hunan_GDPPC %&gt;%   mutate(nb = st_contiguity(geometry),          wt = st_weights(nb,                          style='W'),          .before=1)}\nNotes: ,before1 -&gt; put nb and wt at the front of the tibble dataset"
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex1/In-class_Ex1.html",
    "href": "In-class_Exercise/In-class_Ex1/In-class_Ex1.html",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "",
    "text": "In this in-class exercise, I learn how to import and wrangling geospatial data using appropriate R packages."
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex1/In-class_Ex1.html#overview",
    "href": "In-class_Exercise/In-class_Ex1/In-class_Ex1.html#overview",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "",
    "text": "In this in-class exercise, I learn how to import and wrangling geospatial data using appropriate R packages."
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex1/In-class_Ex1.html#getting-started",
    "href": "In-class_Exercise/In-class_Ex1/In-class_Ex1.html#getting-started",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "2 Getting Started",
    "text": "2 Getting Started\nThe code chunk below load the following packages:\n\ntmap: for thematic mapping\nsf: for geospatial data handling\ntidyverse: non-spatial data handling\n\n\npacman::p_load(tmap, sf, tidyverse)"
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex1/In-class_Ex1.html#preparing-the-flow-data",
    "href": "In-class_Exercise/In-class_Ex1/In-class_Ex1.html#preparing-the-flow-data",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "3 Preparing the Flow Data",
    "text": "3 Preparing the Flow Data"
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex1/In-class_Ex1.html#importing-the-od-data",
    "href": "In-class_Exercise/In-class_Ex1/In-class_Ex1.html#importing-the-od-data",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "4 Importing the OD Data",
    "text": "4 Importing the OD Data\nFirstly, we will import the Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall by using read_csv() of readr package.\n\n# eval:false\nodbus &lt;- read_csv(\"data/aspatial/origin_destination_bus_202308.csv\")\n\n\n4.1 Extracting the study data\n\n#eval: false\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE)\n\n\n# eval: false\norigtrip_7_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE==\"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 7 &\n           TIME_PER_HOUR &lt;=9) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS=sum(TOTAL_TRIPS))\n\nTwo geospatial data will be used in this exercise, they are:\n\nbusstop &lt;- st_read(dsn=\"data/geospatial\", layer=\"BusStop\") %&gt;%\n  st_transform(crs=3414)\n\nReading layer `BusStop' from data source \n  `W:\\widyayutika\\ISSS624\\In-class_Exercise\\In-class_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\n\nmpsz &lt;- st_read(dsn=\"data/geospatial\", layer=\"MPSZ-2019\") %&gt;%\n  st_transform(crs=3414)\n\nReading layer `MPSZ-2019' from data source \n  `W:\\widyayutika\\ISSS624\\In-class_Exercise\\In-class_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\nmpsz\n\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                 SUBZONE_N SUBZONE_C       PLN_AREA_N PLN_AREA_C       REGION_N\n1              MARINA EAST    MESZ01      MARINA EAST         ME CENTRAL REGION\n2         INSTITUTION HILL    RVSZ05     RIVER VALLEY         RV CENTRAL REGION\n3           ROBERTSON QUAY    SRSZ01  SINGAPORE RIVER         SR CENTRAL REGION\n4  JURONG ISLAND AND BUKOM    WISZ01  WESTERN ISLANDS         WI    WEST REGION\n5             FORT CANNING    MUSZ02           MUSEUM         MU CENTRAL REGION\n6         MARINA EAST (MP)    MPSZ05    MARINE PARADE         MP CENTRAL REGION\n7                   SUDONG    WISZ03  WESTERN ISLANDS         WI    WEST REGION\n8                  SEMAKAU    WISZ02  WESTERN ISLANDS         WI    WEST REGION\n9           SOUTHERN GROUP    SISZ02 SOUTHERN ISLANDS         SI CENTRAL REGION\n10                 SENTOSA    SISZ01 SOUTHERN ISLANDS         SI CENTRAL REGION\n   REGION_C                       geometry\n1        CR MULTIPOLYGON (((33222.98 29...\n2        CR MULTIPOLYGON (((28481.45 30...\n3        CR MULTIPOLYGON (((28087.34 30...\n4        WR MULTIPOLYGON (((14557.7 304...\n5        CR MULTIPOLYGON (((29542.53 31...\n6        CR MULTIPOLYGON (((35279.55 30...\n7        WR MULTIPOLYGON (((15772.59 21...\n8        WR MULTIPOLYGON (((19843.41 21...\n9        CR MULTIPOLYGON (((30870.53 22...\n10       CR MULTIPOLYGON (((26879.04 26..."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex9/Hands-on_Ex9.html",
    "href": "Hands-on_Exercise/Hands-on_Ex9/Hands-on_Ex9.html",
    "title": "Hands-on_Ex9",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to compute Global and Local Measure of Spatial Autocorrelation (GLSA) by using spdep package. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\ncompute Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions spdep package;\ncompute Getis-Ord’s Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of spdep package; and\nto visualise the analysis output by using tmap package."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex9/Hands-on_Ex9.html#overview",
    "href": "Hands-on_Exercise/Hands-on_Ex9/Hands-on_Ex9.html#overview",
    "title": "Hands-on_Ex9",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to compute Global and Local Measure of Spatial Autocorrelation (GLSA) by using spdep package. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\ncompute Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions spdep package;\ncompute Getis-Ord’s Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of spdep package; and\nto visualise the analysis output by using tmap package."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex9/Hands-on_Ex9.html#getting-started",
    "href": "Hands-on_Exercise/Hands-on_Ex9/Hands-on_Ex9.html#getting-started",
    "title": "Hands-on_Ex9",
    "section": "2 Getting Started",
    "text": "2 Getting Started\n\n2.1 The analytical question\nIn spatial policy, one of the main development objective of the local government and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China.(https://en.wikipedia.org/wiki/Hunan)\n\n\n2.2 The Study Area and Data\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\n2.3 Setting the Analytical Tools\nBefore we get started, we need to ensure that spdep, sf, tmap and tidyverse packages of R are currently installed in your R.\n\nsf is use for importing and handling geospatial data in R,\ntidyverse is mainly use for wrangling attribute data in R,\nspdep will be used to compute spatial weights, global and local spatial autocorrelation statistics, and\ntmap will be used to prepare cartographic quality chropleth map.\n\nThe code chunk below is used to perform the following tasks:\n\ncreating a package list containing the necessary R packages,\nchecking if the R packages in the package list have been installed in R,\n\nif they have yet to be installed, RStudio will installed the missing packages,\n\nlaunching the packages into R environment.\n\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex9/Hands-on_Ex9.html#getting-the-data-into-r-environment",
    "href": "Hands-on_Exercise/Hands-on_Ex9/Hands-on_Ex9.html#getting-the-data-into-r-environment",
    "title": "Hands-on_Ex9",
    "section": "3 Getting the Data Into R Environment",
    "text": "3 Getting the Data Into R Environment\nIn this section, you will learn how to bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n3.1 Import shapefile into r environment\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `W:\\widyayutika\\ISSS624\\Hands-on_Exercise\\Hands-on_Ex9\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n3.2 Import csv file into r environment\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R data frame class.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n3.3 Performing relational join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\n3.4 Visualising Regional Development Indicator\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex9/Hands-on_Ex9.html#global-spatial-autocorrelation",
    "href": "Hands-on_Exercise/Hands-on_Ex9/Hands-on_Ex9.html#global-spatial-autocorrelation",
    "title": "Hands-on_Ex9",
    "section": "4 Global Spatial Autocorrelation",
    "text": "4 Global Spatial Autocorrelation\nIn this section, you will learn how to compute global spatial autocorrelation statistics and to perform spatial complete randomness test for global spatial autocorrelation.\n\n4.1 Computing Contiguity Spatial Weights\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\nMore specifically, the code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\n\n\n4.2 Row-standardised weights matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nThe input of nb2listw() must be an object of class nb. The syntax of the function has two major arguments, namely style and zero.poly.\n\nstyle can take values “W”, “B”, “C”, “U”, “minmax” and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nIf zero policy is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice.\n\n\n\n4.3 Global Spatial Autocorrelation: Moran’s I\nIn this section, you will learn how to perform Moran’s I statistics testing by using moran.test() of spdep.\n\n\n4.4 Maron’s I test\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep.\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nQuestion: What statistical conclusion can you draw from the output above?\nAns:\nH0: The GDPPC values are randomly distributed across Hunan following a completely random process\nThe small p-value and the positive Moran’s I statistic suggest that there is significant positive spatial autocorrelation in the hunan$GDPPC variable based on the provided spatial weights matrix. In other words, regions with similar GDPPC values tend to be spatially clustered in this dataset. The observed spatial pattern is unlikely to occur by random chance alone.\nNote: It implements a two-sided test as opposed to the one-sided test adopted in the above example (i.e. alternative = \"greater\"). A two-sided p-value is nothing more than twice the one-sided p-value. However, there is no important distinction in any of the documentation provided. Fortunately, the GDPPC value is so strongly clustered that both a one-sided and two-sided test produce the same outcome (a p-value close to 0).\n\n4.4.1 Computing Monte Carlo Moran’s I\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nQuestion: What statistical conclustion can you draw fro mthe output above?\nAns: The p-value of 0.001 is less than the conventional significance level of 0.05. Therefore, you would reject the null hypothesis of no spatial autocorrelation. The results suggest that the observed spatial pattern in the hunan$GDPPC variable, as measured by Moran’s I, is statistically significant and not likely due to random chance. The positive Moran’s I value indicates positive spatial autocorrelation, meaning similar values are spatially clustered in the dataset.\n\n\n4.4.2 Visualising Monte Carlo Moran’s I\nIt is always a good practice for us the examine the simulated Moran’s I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using the code chunk below.\nIn the code chunk below hist() and abline() of R Graphics are used.\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\n\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\n\n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\nQuestion: What statistical observation can you draw from the output above?\nAns:\n\nThe distribution of simulated Moran’s I values under the null hypothesis provides a reference for what you would expect if spatial autocorrelation were due to random chance.\nThe observed Moran’s I value from the original data falls to the right of the distribution, indicating that the observed spatial autocorrelation is higher than what would be expected by random chance.\nThe summary statistics and histogram support the conclusion drawn from the p-value in the previous analysis. The distribution of simulated Moran’s I values suggests that the observed Moran’s I from the original data is statistically significant and unlikely to be due to random chance. The positive skewness implies that positive spatial autocorrelation is less common under the null hypothesis than would be expected by chance.\n\n\n\nChallenge: Instead of using Base Graph to plot the values, plot the values by using ggplot2 package.\n\n\n# Create a data frame for the simulated Moran's I values\nsim_data &lt;- data.frame(Moran_I = bperm$res)\n\n# Plot the histogram using ggplot2\nggplot(sim_data, aes(x = Moran_I)) +\n  geom_histogram(binwidth = 0.02, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Histogram of Simulated Moran's I\",\n       x = \"Simulated Moran's I\",\n       y = \"Frequency\") +\n  theme_minimal() +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\", size = 1)\n\n\n\n\n\n\n\n4.5 Global Spatial Autocorrelation: Geary’s\nIn this section, you will learn how to perform Geary’s c statistics testing by using appropriate functions of spdep package.\n\n4.5.1 Geary’s C test\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\nQuestion: What statistical conclusion can you draw from the output above?\nAns: The small p-value of 0.0001526 suggests that the observed spatial pattern in the hunan$GDPPC variable, as measured by Geary’s C, is statistically significant. The observed Geary’s C statistic is significantly lower than what would be expected by random chance alone. This implies positive spatial autocorrelation, meaning similar values are clustered together in space. The conclusion is consistent with the alternative hypothesis that the expected spatial pattern is greater than the observed pattern.\n\n\n4.5.2 Computing Monte Carlo Geary’s C\nThe code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep.\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\nQuestion: What statistical conclusion can you draw from the output above?\nAns: The p-value of 0.001 is less than the conventional significance level of 0.05. Therefore, you would reject the null hypothesis of no spatial autocorrelation. The results suggest that the observed spatial pattern in the hunan$GDPPC variable, as measured by Geary’s C, is statistically significant and not likely due to random chance. The positive alternative hypothesis indicates that the expected spatial pattern is greater than the observed pattern, implying positive spatial autocorrelation.\n\n\n4.5.3 Visualising the Monte Carlo Geary’s C\nNext, we will plot a histogram to reveal the distribution of the simulated values by using the code chunk below.\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\n\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\n\n\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\") \n\n\n\n\nQuestion: What statistical observation can you draw from the output?\nAns:\n\nThe mean being close to the expected value (1.0) under the null hypothesis suggests that the simulation process is generating Geary’s C values consistent with spatial randomness.\nThe histogram shows that Geary’s C values approximates a normal distribution."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex9/Hands-on_Ex9.html#spatial-correlogram",
    "href": "Hands-on_Exercise/Hands-on_Ex9/Hands-on_Ex9.html#spatial-correlogram",
    "title": "Hands-on_Ex9",
    "section": "5 Spatial Correlogram",
    "text": "5 Spatial Correlogram\nSpatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\n\n5.1 Compute Moran’s I correlogram\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran’s I. The plot() of base Graph is then used to plot the output.\n\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\nBy plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nQuestion: What statistical observation can you draw from the plot above?\nAns:\n\nLag 1, 2, 5 have highly significant Moran’s I values (indicated by ‘***’), suggesting significant spatial autocorrelation.\nLag 4 has a p-value of 0.226015, which is not significant at conventional levels (0.05).\n\n\n\n5.2 Compute Geary’s C correlogram and plot\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary’s C. The plot() of base Graph is then used to plot the output.\n\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\nSimilar to the previous step, we will print out the analysis report by using the code chunk below.\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex2/Hands-on_Ex2.html",
    "href": "Hands-on_Exercise/Hands-on_Ex2/Hands-on_Ex2.html",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nIn this chapter, I learn how to plot functional and truthful choropleth maps by using an R package called **tmap** package."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex2/Hands-on_Ex2.html#overview",
    "href": "Hands-on_Exercise/Hands-on_Ex2/Hands-on_Ex2.html#overview",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nIn this chapter, I learn how to plot functional and truthful choropleth maps by using an R package called **tmap** package."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex2/Hands-on_Ex2.html#getting-started",
    "href": "Hands-on_Exercise/Hands-on_Ex2/Hands-on_Ex2.html#getting-started",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "2 Getting Started",
    "text": "2 Getting Started\nThe code chunk below install and load sf , tmap, tidyverse packages into R environment (p_load from pacman)\n\nsf is for importing, managing and processing geospatial data\ntmap is for drawing thematic maps\ntidyverse is for importing, wrangling and visualising data\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data\n\n\n\npacman::p_load(sf, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex2/Hands-on_Ex2.html#importing-data-into-r",
    "href": "Hands-on_Exercise/Hands-on_Ex2/Hands-on_Ex2.html#importing-data-into-r",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "3 Importing Data into R",
    "text": "3 Importing Data into R\n\n\n3.1 The Data\nIn Hands-on Exercise 2, there are 2 datasets to be used:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n3.2 Importing Geospatial Data into R\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `W:\\widyayutika\\ISSS624\\Hands-on_Exercise\\Hands-on_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\nNote: only the first ten records will be displayed by default due to the size of the data.\n\n\n3.3 Importing Attribute Data into R\nNext, we will import respopagsex2000to2018.csv file into RStudio and save the file into an R dataframe called popagsex.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\npopdata &lt;- read_csv(\"data/aspatial/respopagesexfa2011to2020.csv\")\n\n\n\n3.4 Data Preparation\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n3.4.1 Data wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n3.4.2 Joining the attribute data and geospatial data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nThing to learn from the code chunk above:\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex2/Hands-on_Ex2.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands-on_Exercise/Hands-on_Ex2/Hands-on_Ex2.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "4 Choropleth Mapping Geospatial Data Using tmap",
    "text": "4 Choropleth Mapping Geospatial Data Using tmap\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n4.1 Plotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n4.2 Things to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)Creating a choropleth map by using tmap’s elements\n\n\n\n4.3 Data classification methods of tmap\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nIn the following sub-section, we will share with you tmap functions that used to plot these elements.\n\n4.3.1 Drawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n4.3.2 Drawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n4.3.3 Drawing a choropleth map using tm_fill() and *tm_border()**\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\n4.4 Data classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n4.4.1 Plotting choropleth maps with built-in classification methods\nThe code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\nDIY: Using what you had learned, prepare choropleth maps by using different classification methods supported by tmap and compare their differences.\n\nIn the code chunk below, pretty break data classification method is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"pretty\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the distribution of pretty break data classification method are very similar with equal data classification method, but using pretty break there are only 4 classes as the the function pretty prioritizes creating breaks at “nice” values that are easy to read on plots(visually appealing).\n\nDIY: Preparing choropleth maps by using similar classification method but with different numbers of classes (i.e. 2, 6, 10, 20). Compare the output maps, what observation can you draw?\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 2,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 10,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 20,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nThe choice of the number of classes significantly influences how the data is visualized and how patterns are perceived:\n\n2 classes: This can result in a binary representation where only high and low values are distinguished. It might oversimplify the data, potentially masking subtle variations within each class.\n5 and 6 classes : This result provides a balanced representation of the data. It allows for a moderate level of detail without overwhelming the viewer.\n10 and 20 classes: Too many classes can result in a visually cluttered map, making it difficult for viewers to discern meaningful patterns.\n\n\n\n4.4.2 Plotting choropleth map with custome break\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.7113  0.7926  0.8561  0.8786 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n4.5 Colour Scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n4.5.1 Using ColourBrewer palette\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the colour scheme has been reversed.\n\n\n\n4.6 Map Layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n4.6.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n4.6.2 Map style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n4.6.3 Cartographic Furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nTo reset the default style, refer to the code chunk below.\n\ntmap_style(\"white\")\n\n\n\n\n4.7 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n4.7.1 By assigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n4.7.2 By defining a group-by variable in tm_facets()\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n4.7.3 By creating multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n4.8 Mappping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex2/Hands-on_Ex2.html#reference",
    "href": "Hands-on_Exercise/Hands-on_Ex2/Hands-on_Ex2.html#reference",
    "title": "Hands-on Exercise 2: Choropleth Mapping with R",
    "section": "5 Reference",
    "text": "5 Reference\n\n5.1 All about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n5.2 Geospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n5.3 Data wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex15/data/geospatial/MPSZ-2019.html",
    "href": "Hands-on_Exercise/Hands-on_Ex15/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624 - Applied Geospatial Analytics",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex1/Hands-on_Ex1.html",
    "href": "Hands-on_Exercise/Hands-on_Ex1/Hands-on_Ex1.html",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "",
    "text": "In this hands-on exercise, I learn how to import and wrangling geospatial data using appropriate R packages."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex1/Hands-on_Ex1.html#overview",
    "href": "Hands-on_Exercise/Hands-on_Ex1/Hands-on_Ex1.html#overview",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "",
    "text": "In this hands-on exercise, I learn how to import and wrangling geospatial data using appropriate R packages."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex1/Hands-on_Ex1.html#getting-started",
    "href": "Hands-on_Exercise/Hands-on_Ex1/Hands-on_Ex1.html#getting-started",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "2 Getting Started",
    "text": "2 Getting Started\nThe code chunk below install and load sf and tidyverse packages into R environment (p_load from pacman)\n\nsf is for importing, managing and processing geospatial data\ntidyverse is for importing, wrangling and visualising data\n\n\npacman:: p_load(sf, tidyverse)"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex1/Hands-on_Ex1.html#importing-geospatial-data",
    "href": "Hands-on_Exercise/Hands-on_Ex1/Hands-on_Ex1.html#importing-geospatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "3 Importing Geospatial Data",
    "text": "3 Importing Geospatial Data\nIn this section, using st_read() of sf function, we will import several datasets:\n\nMP14_SUBZONE_WEB_PL, a polygon feature layer in ESRI shapefile format,\nCyclingPath, a line feature layer in ESRI shapefile format, and\nPreSchool, a point feature layer in kml file format.\n\nNote: st_read() is used to read simple features from file/database\n\n3.1 Importing polygon feature data in shapefile format\n\nmpsz &lt;- st_read(dsn=\"data/geospatial\", layer=\"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `W:\\widyayutika\\ISSS624\\Hands-on_Exercise\\Hands-on_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nNote: dsn is used to define data path while layer is used to provide the shapefile name.\n\n\n3.2 Importing polyline feature data in shapefile format\n\ncyclingpath &lt;- st_read(dsn=\"data/geospatial\", layer=\"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `W:\\widyayutika\\ISSS624\\Hands-on_Exercise\\Hands-on_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2558 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\n\n\n\n3.3 Importing GIS data in kml format\n\npreschool = st_read(\"data/geospatial/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `W:\\widyayutika\\ISSS624\\Hands-on_Exercise\\Hands-on_Ex1\\data\\geospatial\\PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex1/Hands-on_Ex1.html#checking-the-content-of-a-simple-feature-data-frame",
    "href": "Hands-on_Exercise/Hands-on_Ex1/Hands-on_Ex1.html#checking-the-content-of-a-simple-feature-data-frame",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "4 Checking the Content of A Simple Feature Data Frame",
    "text": "4 Checking the Content of A Simple Feature Data Frame\n\n4.1 Working with st_geometry()\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nNote: st_geometry is used to display basic information of feature class such as type of geometry , the geographic extend of the features and the coordinate system of the data.\n\n\n4.2 Working with glimpse()\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\nNote: glimpse() report from dplyr is used to display the data type of each fields.\n\n\n4.3 Working with head()\n\nhead(mpsz, n=5)\n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\nNote: head() is to display the first n records."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex1/Hands-on_Ex1.html#plotting-the-geospatial-data",
    "href": "Hands-on_Exercise/Hands-on_Ex1/Hands-on_Ex1.html#plotting-the-geospatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "5 Plotting the Geospatial Data",
    "text": "5 Plotting the Geospatial Data\n\nplot(mpsz)\n\n\n\n\nNote: plot() is used to plot the geospatial object for a quick look. By default, all the attributes of an sf object will be plotted up to reasonable maximum as shown above.\n\nplot(st_geometry(mpsz))\n\n\n\n\nNote: plot(st_geometry()) is used to plot only the geometry\n\nplot(mpsz[\"PLN_AREA_N\"])\n\n\n\n\nNote: plot(mpsz[“attribute”]) is used to plot specific attribute"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex1/Hands-on_Ex1.html#working-with-projection",
    "href": "Hands-on_Exercise/Hands-on_Ex1/Hands-on_Ex1.html#working-with-projection",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "6 Working with Projection",
    "text": "6 Working with Projection\nIn this section, we will learn how to project a simple feature data frame from one coordinate system to another coordinate system a.k.a. projection transformation.\n\n6.1 Assigning EPSG code to a simple feature data frame\nCommon issue in importing geospatial data into R: coordinate system of the source data was either missing (due to missing .proj for ESRI shapefile, etc.) or wrongly assigned.\nCoordinate system of mpsz is shown as below:\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nNote: st_crs() is used to retrieve coordinate reference system from object.\nmpsz is projected in svy21 as shown from the second line, but at the last line, it is mentioned that the EPSG is 9001. This is wrongly assigned because the correct EPSG code for svy21 is 3414.\nThen, we will assign the correct EPSG code to mpsz dataframe using st_set_crs().\n\nmpsz3414 &lt;- st_set_crs(mpsz, 3414)\n\nThen, let us recheck the CSR again.\n\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNote: There is a warning mentioning replacing crs does not reproject the data, and if we want to reproject the data, we can use st_tranform().\n\n\n6.2 Transforming the projection of preschool from wgs84 to svy21\nThe reason why it is essential to transform the original data from geographic coordinate system to projected coordinated system is because geographic coordinate system is not appropriate if the analysis need to use distance or/and area measurements.\n\nst_geometry(preschool)\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nFrom the print above, we notice the we need to reproject preschool from one coordinate system to another coordinate system mathematically using st_tranform().\n\npreschool3414 &lt;- st_transform(preschool, crs=3414)\n\nNote: In practice, we need to find out appropriate project coordinate system to use, but in the case about we want to project to svy21 with EPSG code 3414.\n\nst_geometry(preschool3414)\n\nGeometry set for 2290 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11810.03 ymin: 25596.33 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\n\nNote: The bounding box values are greater than 0-360 range of decimal degree which is commonly used by most geogrpahic coordinate system."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex1/Hands-on_Ex1.html#importing-and-converting-an-aspatial-data",
    "href": "Hands-on_Exercise/Hands-on_Ex1/Hands-on_Ex1.html#importing-and-converting-an-aspatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "7 Importing and Converting An Aspatial Data",
    "text": "7 Importing and Converting An Aspatial Data\nAspatial Data contains no geospatial data but there exists 2 field which capture the x- and y-coordinates of the data points.\nIn this section, we will learn how to import an aspatial data (i.e. listings data from Airbnb) into R environment and save it as tibble dataframe.\n\n7.1 Importing the aspatial data\n\nlistings &lt;- read_csv(\"data/aspatial/listings.csv\")\n\nTo check whether the data files are imported correctly, we check using below code\n\nlist(listings)\n\n[[1]]\n# A tibble: 3,483 × 18\n       id name      host_id host_name neighbourhood_group neighbourhood latitude\n    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;               &lt;chr&gt;            &lt;dbl&gt;\n 1  71609 Villa in…  367042 Belinda   East Region         Tampines          1.35\n 2  71896 Home in …  367042 Belinda   East Region         Tampines          1.35\n 3  71903 Home in …  367042 Belinda   East Region         Tampines          1.35\n 4 275343 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 5 275344 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 6 289234 Home in …  367042 Belinda   East Region         Tampines          1.34\n 7 294281 Rental u… 1521514 Elizabeth Central Region      Newton            1.31\n 8 324945 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 9 330095 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n10 369141 Place to… 1521514 Elizabeth Central Region      Newton            1.31\n# ℹ 3,473 more rows\n# ℹ 11 more variables: longitude &lt;dbl&gt;, room_type &lt;chr&gt;, price &lt;dbl&gt;,\n#   minimum_nights &lt;dbl&gt;, number_of_reviews &lt;dbl&gt;, last_review &lt;date&gt;,\n#   reviews_per_month &lt;dbl&gt;, calculated_host_listings_count &lt;dbl&gt;,\n#   availability_365 &lt;dbl&gt;, number_of_reviews_ltm &lt;dbl&gt;, license &lt;chr&gt;\n\n\nTwo useful fields are latitute and longitude in decimal format.\n\n\n7.2 Creating a simple feature data frame from an aspatial data frame\n\nlistings_sf &lt;- st_as_sf(listings,\n                        coords = c(\"longitude\",\"latitude\"),\n                        crs=4326) %&gt;%\n  st_transform(crs=3414)\n\nNotes:\n\ncoords requires you to provide column name of the x- and y-coordinates respectively.\ncrs requires you to provide the coordinate system in espg format.\n\nEPSG: 4326 is wgs84\nEPSG: 3414 is svy21\n\n%&gt;% is used to nest st_transform() to transform the newly created sumple feature data frame into svy21 projected coordinates sytem.\n\nLet us examine the content of the newly created simple feature data frame.\n\nglimpse(listings_sf)\n\nRows: 3,483\nColumns: 17\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           &lt;chr&gt; \"Villa in Singapore · ★4.44 · 2 bedroom…\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat…\n$ price                          &lt;dbl&gt; 150, 80, 80, 55, 69, 220, 85, 75, 45, 7…\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 92, 60, 60, 92,…\n$ number_of_reviews              &lt;dbl&gt; 20, 24, 47, 22, 17, 12, 133, 18, 6, 81,…\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              &lt;dbl&gt; 0.14, 0.16, 0.31, 0.17, 0.12, 0.09, 0.9…\n$ calculated_host_listings_count &lt;dbl&gt; 5, 5, 5, 52, 52, 5, 7, 52, 52, 7, 7, 1,…\n$ availability_365               &lt;dbl&gt; 89, 89, 89, 275, 274, 89, 365, 365, 365…\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 3, 0, 0, 1, 3, 0, 0, 0, 0, …\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n$ geometry                       &lt;POINT [m]&gt; POINT (41972.5 36390.05), POINT (…\n\n\nNote: new column geometry is created and “longitude” and “latitude” columns are dropped."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex1/Hands-on_Ex1.html#geoprocessing-with-sf-package",
    "href": "Hands-on_Exercise/Hands-on_Ex1/Hands-on_Ex1.html#geoprocessing-with-sf-package",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "8 Geoprocessing with sf package",
    "text": "8 Geoprocessing with sf package\nIn this section, we will learn how to perform geoprocessing (a.k.a. GIS analysis) functions buffering and point in polygon count.\n\n8.1 Buffering\nThe scenario:\nThe authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on the both sides of the current cycling path. You are tasked to determine the extend of the land need to be acquired and their total area.\n\nbuffer_cycling &lt;-st_buffer(cyclingpath, dist=5, nQuadSegs=30)\n\nNote: st_buffer() of sf package is used to compute the 5-meter buffers around cycling paths\n\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\n\nNote: the code above calculates the ares of the buffers\n\nsum(buffer_cycling$AREA)\n\n1774367 [m^2]\n\n\nNote: sum() is used to derive the total land involved.\n\n\n8.2 Point-in-polygon count\nThe scenario:\nA pre-school service group want to find out the numbers of pre-schools in each Planning Subzone.\n\nmpsz3414$`PreSch Count`&lt;- lengths(st_intersects(mpsz3414, preschool3414))\n\nNote: the above code is to identify preschool located inside each Planning Subzone using st_intersect () and to calculate numbers of preschools that fall in each Planning Subzone using length()\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\nNote: summary() is used to check the summary statistics.\n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\nNote: top_n is used to list the top n numbers of planning subzone with the most number of preschools.\nNext, we want to calculate the density of preschools by planning subzone.\n\nmpsz3414$Area &lt;- mpsz3414 %&gt;%\n  st_area()\n\nNote: st_area() is used to derive the area of each planning subzone.\n\nmpsz3414 &lt;- mpsz3414 %&gt;%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)\n\nNote: mutate() is used to compute the density."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex1/Hands-on_Ex1.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Exercise/Hands-on_Ex1/Hands-on_Ex1.html#exploratory-data-analysis-eda",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "9 Exploratory Data Analysis (EDA)",
    "text": "9 Exploratory Data Analysis (EDA)\n\n9.1 Histrogram\n\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\nThe graph above is the basic, to obtain a higher quality plot, we can use ggplot().\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\n\n9.2 Scatterplot\nBelow are the relationship between preschool density and preschool count with scatterplot.\n\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`, \n           x= as.numeric(`PreSch Density`)))+\n  geom_point(color=\"black\", \n             fill=\"light blue\") +\n  xlim(0, 40) +\n  ylim(0, 40) +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website is a learning journey of a student from Singapore Management University(SMU) in Master of IT in Business (MITB) program.\nThe course materials are from ISSS624 - Applied Geospatial Analytics."
  },
  {
    "objectID": "Hands-on_Ex1.html",
    "href": "Hands-on_Ex1.html",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "This is the overview paragraph."
  },
  {
    "objectID": "Hands-on_Ex1.html#overview",
    "href": "Hands-on_Ex1.html#overview",
    "title": "Hands-on Exercise 1",
    "section": "",
    "text": "This is the overview paragraph."
  },
  {
    "objectID": "Hands-on_Ex1.html#getting-started",
    "href": "Hands-on_Ex1.html#getting-started",
    "title": "Hands-on Exercise 1",
    "section": "2 Getting Started",
    "text": "2 Getting Started\nThis is the getting start paragraph."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex10/Hands-on_Ex10.html",
    "href": "Hands-on_Exercise/Hands-on_Ex10/Hands-on_Ex10.html",
    "title": "Hands-on_Ex10",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to compute Global and Local Measure of Spatial Autocorrelation (GLSA) by using spdep package. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\ncompute Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions spdep package;\ncompute Getis-Ord’s Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of spdep package; and\nto visualise the analysis output by using tmap package."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex10/Hands-on_Ex10.html#overview",
    "href": "Hands-on_Exercise/Hands-on_Ex10/Hands-on_Ex10.html#overview",
    "title": "Hands-on_Ex10",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to compute Global and Local Measure of Spatial Autocorrelation (GLSA) by using spdep package. By the end to this hands-on exercise, you will be able to:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\ncompute Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions spdep package;\ncompute Getis-Ord’s Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of spdep package; and\nto visualise the analysis output by using tmap package."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex10/Hands-on_Ex10.html#getting-started",
    "href": "Hands-on_Exercise/Hands-on_Ex10/Hands-on_Ex10.html#getting-started",
    "title": "Hands-on_Ex10",
    "section": "2 Getting Started",
    "text": "2 Getting Started\n\n2.1 The analytical question\nIn spatial policy, one of the main development objective of the local govenment and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is No. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is yes, then our next question will be “where are these clusters?”\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of Hunan Provice, People Republic of China.(https://en.wikipedia.org/wiki/Hunan)\n\n\n2.2 The Study Area and Data\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\n2.3 Setting the Analytical Tools\nBefore we get started, we need to ensure that spdep, sf, tmap and tidyverse packages of R are currently installed in your R.\n\nsf is use for importing and handling geospatial data in R,\ntidyverse is mainly use for wrangling attribute data in R,\nspdep will be used to compute spatial weights, global and local spatial autocorrelation statistics, and\ntmap will be used to prepare cartographic quality chropleth map.\n\nThe code chunk below is used to perform the following tasks:\n\ncreating a package list containing the necessary R packages,\nchecking if the R packages in the package list have been installed in R,\n\nif they have yet to be installed, RStudio will installed the missing packages,\n\nlaunching the packages into R environment.\n\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex10/Hands-on_Ex10.html#getting-the-data-into-r-environment",
    "href": "Hands-on_Exercise/Hands-on_Ex10/Hands-on_Ex10.html#getting-the-data-into-r-environment",
    "title": "Hands-on_Ex10",
    "section": "3 Getting the Data Into R Environment",
    "text": "3 Getting the Data Into R Environment\nIn this section, you will learn how to bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n3.1 Import shapefile into r environment\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `W:\\widyayutika\\ISSS624\\Hands-on_Exercise\\Hands-on_Ex10\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n3.2 Import csv file into r environment\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R data frame class.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n3.3 Performing relational join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\n3.4 Visualising Regional Development Indicator\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex10/Hands-on_Ex10.html#global-spatial-autocorrelation",
    "href": "Hands-on_Exercise/Hands-on_Ex10/Hands-on_Ex10.html#global-spatial-autocorrelation",
    "title": "Hands-on_Ex10",
    "section": "4 Global Spatial Autocorrelation",
    "text": "4 Global Spatial Autocorrelation\nIn this section, you will learn how to compute global spatial autocorrelation statistics and to perform spatial complete randomness test for global spatial autocorrelation.\n\n4.1 Computing Contiguity Spatial Weights\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\nMore specifically, the code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\n\n\n4.2 Row-standardised weights matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nThe input of nb2listw() must be an object of class nb. The syntax of the function has two major arguments, namely style and zero.poly.\n\nstyle can take values “W”, “B”, “C”, “U”, “minmax” and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nIf zero policy is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice.\n\n\n\n4.3 Global Spatial Autocorrelation: Moran’s I\nIn this section, you will learn how to perform Moran’s I statistics testing by using moran.test() of spdep.\n\n\n4.4 Maron’s I test\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep.\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nQuestion: What statistical conclusion can you draw from the output above?\nAns:\nH0: The GDPPC values are randomly distributed across Hunan following a completely random process\nThe small p-value and the positive Moran’s I statistic suggest that there is significant positive spatial autocorrelation in the hunan$GDPPC variable based on the provided spatial weights matrix. In other words, regions with similar GDPPC values tend to be spatially clustered in this dataset. The observed spatial pattern is unlikely to occur by random chance alone.\nNote: It implements a two-sided test as opposed to the one-sided test adopted in the above example (i.e. alternative = \"greater\"). A two-sided p-value is nothing more than twice the one-sided p-value. However, there is no important distinction in any of the documentation provided. Fortunately, the GDPPC value is so strongly clustered that both a one-sided and two-sided test produce the same outcome (a p-value close to 0).\n\n4.4.1 Computing Monte Carlo Moran’s I\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nQuestion: What statistical conclustion can you draw fro mthe output above?\nAns: The p-value of 0.001 is less than the conventional significance level of 0.05. Therefore, you would reject the null hypothesis of no spatial autocorrelation. The results suggest that the observed spatial pattern in the hunan$GDPPC variable, as measured by Moran’s I, is statistically significant and not likely due to random chance. The positive Moran’s I value indicates positive spatial autocorrelation, meaning similar values are spatially clustered in the dataset.\n\n\n4.4.2 Visualising Monte Carlo Moran’s I\nIt is always a good practice for us the examine the simulated Moran’s I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using the code chunk below.\nIn the code chunk below hist() and abline() of R Graphics are used.\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\n\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\n\n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\nQuestion: What statistical observation can you draw from the output above?\nAns:\n\nThe distribution of simulated Moran’s I values under the null hypothesis provides a reference for what you would expect if spatial autocorrelation were due to random chance.\nThe observed Moran’s I value from the original data falls to the right of the distribution, indicating that the observed spatial autocorrelation is higher than what would be expected by random chance.\nThe summary statistics and histogram support the conclusion drawn from the p-value in the previous analysis. The distribution of simulated Moran’s I values suggests that the observed Moran’s I from the original data is statistically significant and unlikely to be due to random chance. The positive skewness implies that positive spatial autocorrelation is less common under the null hypothesis than would be expected by chance.\n\n\n\nChallenge: Instead of using Base Graph to plot the values, plot the values by using ggplot2 package.\n\n\n# Create a data frame for the simulated Moran's I values\nsim_data &lt;- data.frame(Moran_I = bperm$res)\n\n# Plot the histogram using ggplot2\nggplot(sim_data, aes(x = Moran_I)) +\n  geom_histogram(binwidth = 0.02, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Histogram of Simulated Moran's I\",\n       x = \"Simulated Moran's I\",\n       y = \"Frequency\") +\n  theme_minimal() +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\", size = 1)\n\n\n\n\n\n\n\n4.5 Global Spatial Autocorrelation: Geary’s\nIn this section, you will learn how to perform Geary’s c statistics testing by using appropriate functions of spdep package.\n\n4.5.1 Geary’s C test\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\nQuestion: What statistical conclusion can you draw from the output above?\nAns: The small p-value of 0.0001526 suggests that the observed spatial pattern in the hunan$GDPPC variable, as measured by Geary’s C, is statistically significant. The observed Geary’s C statistic is significantly lower than what would be expected by random chance alone. This implies positive spatial autocorrelation, meaning similar values are clustered together in space. The conclusion is consistent with the alternative hypothesis that the expected spatial pattern is greater than the observed pattern.\n\n\n4.5.2 Computing Monte Carlo Geary’s C\nThe code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep.\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\nQuestion: What statistical conclusion can you draw from the output above?\nAns: The p-value of 0.001 is less than the conventional significance level of 0.05. Therefore, you would reject the null hypothesis of no spatial autocorrelation. The results suggest that the observed spatial pattern in the hunan$GDPPC variable, as measured by Geary’s C, is statistically significant and not likely due to random chance. The positive alternative hypothesis indicates that the expected spatial pattern is greater than the observed pattern, implying positive spatial autocorrelation.\n\n\n4.5.3 Visualising the Monte Carlo Geary’s C\nNext, we will plot a histogram to reveal the distribution of the simulated values by using the code chunk below.\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\n\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\n\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\n\n\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\") \n\n\n\n\nQuestion: What statistical observation can you draw from the output?\nAns:\n\nThe mean being close to the expected value (1.0) under the null hypothesis suggests that the simulation process is generating Geary’s C values consistent with spatial randomness.\nThe histogram shows that Geary’s C values approximates a normal distribution."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex10/Hands-on_Ex10.html#spatial-correlogram",
    "href": "Hands-on_Exercise/Hands-on_Ex10/Hands-on_Ex10.html#spatial-correlogram",
    "title": "Hands-on_Ex10",
    "section": "5 Spatial Correlogram",
    "text": "5 Spatial Correlogram\nSpatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\n\n5.1 Compute Moran’s I correlogram\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran’s I. The plot() of base Graph is then used to plot the output.\n\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\nBy plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nQuestion: What statistical observation can you draw from the plot above?\nAns:\n\nLag 1, 2, 5 have highly significant Moran’s I values (indicated by ‘***’), suggesting significant spatial autocorrelation.\nLag 4 has a p-value of 0.226015, which is not significant at conventional levels (0.05).\n\n\n\n5.2 Compute Geary’s C correlogram and plot\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary’s C. The plot() of base Graph is then used to plot the output.\n\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\nSimilar to the previous step, we will print out the analysis report by using the code chunk below.\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex10/Hands-on_Ex10.html#cluster-and-outlier-analysis",
    "href": "Hands-on_Exercise/Hands-on_Ex10/Hands-on_Ex10.html#cluster-and-outlier-analysis",
    "title": "Hands-on_Ex10",
    "section": "6 Cluster and Outlier Analysis",
    "text": "6 Cluster and Outlier Analysis\nLocal Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable. For instance if we are studying cancer rates among census tracts in a given city local clusters in the rates mean that there are areas that have higher or lower rates than is to be expected by chance alone; that is, the values occurring are above or below those of a random distribution in space.\nIn this section, you will learn how to apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran’I to detect cluster and/or outlier from GDP per capita 2012 of Hunan Province, PRC.\n\n6.1 Computing local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\nThe code chunks below are used to compute local Moran’s I of GDPPC2012 at the county level.\n\nfips &lt;- order(hunan$County)\nlocalMI &lt;- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\nThe code chunk below list the content of the local Moran matrix derived by using printCoefmat().\n\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]),\n  check.names=FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830\n\n\n\n6.1.1 Mapping the local Moran’s I\nBefore mapping the local Moran’s I map, it is wise to append the local Moran’s I dataframe (i.e. localMI) onto hunan SpatialPolygonDataFrame. The code chunks below can be used to perform the task. The out SpatialPolygonDataFrame is called hunan.localMI.\n\nhunan.localMI &lt;- cbind(hunan,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\n\n6.1.2 Mapping local Moran’s I values\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chinks below.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n6.1.3 Mapping local Moran’s I p-values\nThe choropleth shows there is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values, as consider above.\nThe code chunks below produce a choropleth map of Moran’s I p-values by using functions of tmap package.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n6.1.4 Mapping both local Moran’s I values and p-values\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\nlocalMI.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex10/Hands-on_Ex10.html#creating-a-lisa-cluster-map",
    "href": "Hands-on_Exercise/Hands-on_Ex10/Hands-on_Ex10.html#creating-a-lisa-cluster-map",
    "title": "Hands-on_Ex10",
    "section": "7 Creating a LISA Cluster Map",
    "text": "7 Creating a LISA Cluster Map\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\n\n7.1 Plotting Moran scatterplot\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code chunk below plots the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\nnci &lt;- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\nNotice that the plot is split in 4 quadrants. The top right corner belongs to areas that have high GDPPC and are surrounded by other areas that have the average level of GDPPC. This are the high-high locations in the lesson slide.\n\n\n7.2 Plotting Moran scatterplot with standardised variable\nFirst we will use scale() to centers and scales the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.\n\nhunan$Z.GDPPC &lt;- scale(hunan$GDPPC) %&gt;% \n  as.vector \n\nThe as.vector() added to the end is to make sure that the data type we get out of this is a vector, that map neatly into out dataframe.\nNow, we are ready to plot the Moran scatterplot again by using the code chunk below.\n\nnci2 &lt;- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")\n\n\n\n\n\n\n7.3 Preparing LISA map classes\nThe code chunks below show the steps to prepare a LISA cluster map.\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\n\nNext, derives the spatially lagged variable of interest (i.e. GDPPC) and centers the spatially lagged variable around its mean.\n\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \n\nThis is follow by centering the local Moran’s around the mean.\n\nLM_I &lt;- localMI[,1] - mean(localMI[,1])    \n\nNext, we will set a statistical significance level for the local Moran.\n\nsignif &lt;- 0.05       \n\nThese four command lines define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\n\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4      \n\nLastly, places non-significant Moran in the category 0.\n\nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\nIn fact, we can combined all the steps into one single code chunk as shown below:\n\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \nLM_I &lt;- localMI[,1]   \nsignif &lt;- 0.05       \nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4    \nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\n\n\n7.4 Plotting LISA map\nNow, we can build the LISA map by using the code chunks below.\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)\n\n\n\n\nWe can also include the local Moran’s I map and p-value map as shown below for easy comparison.\n\nQuestion: What statistical observations can you draw from the LISA map above?\nAns: The LISA map categorizes each observation into one of four quadrants: High-High, Insignificant, High-Low, and Low-High.\n\nHigh-High (HH): High values surrounded by high values.\nInsignificant : Low values surrounded by low values. (Mostly are insignificant)\nHigh-Low (HL): High values surrounded by low values.\nLow-High (LH): Low values surrounded by high values."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex10/Hands-on_Ex10.html#hot-spot-and-cold-spot-area-analysis",
    "href": "Hands-on_Exercise/Hands-on_Ex10/Hands-on_Ex10.html#hot-spot-and-cold-spot-area-analysis",
    "title": "Hands-on_Ex10",
    "section": "8 Hot Spot and Cold Spot Area Analysis",
    "text": "8 Hot Spot and Cold Spot Area Analysis\nBeside detecting cluster and outliers, localised spatial statistics can be also used to detect hot spot and/or cold spot areas.\nThe term ‘hot spot’ has been used generically across disciplines to describe a region or value that is higher relative to its surroundings (Lepers et al 2005, Aben et al 2012, Isobe et al 2015).\n\n8.1 Getis and Ord’s G-Statistics\nAn alternative spatial statistics to detect spatial anomalies is the Getis and Ord’s G-statistics (Getis and Ord, 1972; Ord and Getis, 1995). It looks at neighbours within a defined proximity to identify where either high or low values clutser spatially. Here, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values too.\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\n\n\n\n8.2 Deriving distance-based weight matrix\nFirst, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two type of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix.\n\n\n8.2.1 Deriving the centroid\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid() on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid(). We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid() function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\n\n\n8.2.2 Determine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n8.2.3 Computing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nwm62_lw &lt;- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\nThe output spatial weights object is called wm62_lw.\n\n\n\n8.3 Computing adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex10/Hands-on_Ex10.html#computing-gi-statistics",
    "href": "Hands-on_Exercise/Hands-on_Ex10/Hands-on_Ex10.html#computing-gi-statistics",
    "title": "Hands-on_Ex10",
    "section": "9 Computing Gi statistics",
    "text": "9 Computing Gi statistics\n\n9.1 Gi statistics using fixed distance\n\nfips &lt;- order(hunan$County)\ngi.fixed &lt;- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)        Z(Gi) Pr(z != E(Gi))\n [1,] 0.064192949 0.05747126 2.375922e-04  0.436075843   6.627817e-01\n [2,] 0.042300020 0.04597701 1.917951e-04 -0.265505650   7.906200e-01\n [3,] 0.044961480 0.04597701 1.933486e-04 -0.073033665   9.417793e-01\n [4,] 0.039475779 0.03448276 1.461473e-04  0.413017033   6.795941e-01\n [5,] 0.049767939 0.04597701 1.927263e-04  0.273070579   7.847990e-01\n [6,] 0.008825335 0.01149425 4.998177e-05 -0.377510776   7.057941e-01\n [7,] 0.050807266 0.02298851 9.435398e-05  2.863898821   4.184617e-03\n [8,] 0.083966739 0.04597701 1.848292e-04  2.794350420   5.200409e-03\n [9,] 0.115751554 0.04597701 1.789361e-04  5.216125401   1.827045e-07\n[10,] 0.049115587 0.04597701 1.891013e-04  0.228236603   8.194623e-01\n[11,] 0.045819180 0.03448276 1.420884e-04  0.951035346   3.415864e-01\n[12,] 0.049183846 0.05747126 2.387633e-04 -0.536334231   5.917276e-01\n[13,] 0.048429181 0.04597701 1.924532e-04  0.176761556   8.596957e-01\n[14,] 0.034733752 0.02298851 9.651140e-05  1.195564020   2.318667e-01\n[15,] 0.011262043 0.01149425 4.945294e-05 -0.033020610   9.736582e-01\n[16,] 0.065131196 0.04597701 1.931870e-04  1.378081093   1.681783e-01\n[17,] 0.027587075 0.03448276 1.385862e-04 -0.585756761   5.580390e-01\n[18,] 0.029409313 0.03448276 1.461397e-04 -0.419680565   6.747188e-01\n[19,] 0.061466754 0.05747126 2.383385e-04  0.258805141   7.957856e-01\n[20,] 0.057656917 0.05747126 2.371303e-04  0.012056111   9.903808e-01\n[21,] 0.066518379 0.06896552 2.820326e-04 -0.145716531   8.841452e-01\n[22,] 0.045599896 0.04597701 1.928108e-04 -0.027158687   9.783332e-01\n[23,] 0.030646753 0.03448276 1.449523e-04 -0.318615290   7.500183e-01\n[24,] 0.035635552 0.04597701 1.906613e-04 -0.748946051   4.538897e-01\n[25,] 0.032606647 0.04597701 1.932888e-04 -0.961700582   3.362000e-01\n[26,] 0.035001352 0.04597701 1.897172e-04 -0.796851342   4.255374e-01\n[27,] 0.012746354 0.02298851 9.812587e-05 -1.033949773   3.011596e-01\n[28,] 0.061287917 0.06896552 2.773884e-04 -0.460979158   6.448136e-01\n[29,] 0.014277403 0.02298851 9.683314e-05 -0.885240161   3.760271e-01\n[30,] 0.009622875 0.01149425 4.924586e-05 -0.266671512   7.897221e-01\n[31,] 0.014258398 0.02298851 9.705244e-05 -0.886168613   3.755267e-01\n[32,] 0.005453443 0.01149425 4.986245e-05 -0.855476971   3.922871e-01\n[33,] 0.043283712 0.05747126 2.367109e-04 -0.922143185   3.564539e-01\n[34,] 0.020763514 0.03448276 1.393165e-04 -1.162328599   2.451020e-01\n[35,] 0.081261843 0.06896552 2.794398e-04  0.735582222   4.619850e-01\n[36,] 0.057419907 0.05747126 2.338437e-04 -0.003358489   9.973203e-01\n[37,] 0.013497133 0.02298851 9.624821e-05 -0.967459309   3.333145e-01\n[38,] 0.019289310 0.03448276 1.455643e-04 -1.259299080   2.079223e-01\n[39,] 0.025996272 0.04597701 1.892938e-04 -1.452256513   1.464303e-01\n[40,] 0.016092694 0.03448276 1.424776e-04 -1.540671121   1.233968e-01\n[41,] 0.035952614 0.05747126 2.379439e-04 -1.395011407   1.630124e-01\n[42,] 0.031690963 0.05747126 2.350604e-04 -1.681505286   9.266481e-02\n[43,] 0.018750079 0.03448276 1.433314e-04 -1.314110709   1.888090e-01\n[44,] 0.015449080 0.02298851 9.638666e-05 -0.767944457   4.425202e-01\n[45,] 0.065760689 0.06896552 2.760533e-04 -0.192889342   8.470456e-01\n[46,] 0.098966900 0.05747126 2.326002e-04  2.720804542   6.512325e-03\n[47,] 0.085415780 0.05747126 2.385746e-04  1.809191360   7.042128e-02\n[48,] 0.038816536 0.05747126 2.343951e-04 -1.218469473   2.230456e-01\n[49,] 0.038931873 0.04597701 1.893501e-04 -0.511984469   6.086619e-01\n[50,] 0.055098610 0.06896552 2.760948e-04 -0.834546363   4.039732e-01\n[51,] 0.033405005 0.04597701 1.916312e-04 -0.908179070   3.637836e-01\n[52,] 0.043040784 0.06896552 2.829941e-04 -1.541081516   1.232969e-01\n[53,] 0.011297699 0.02298851 9.615920e-05 -1.192199867   2.331829e-01\n[54,] 0.040968457 0.05747126 2.356318e-04 -1.075080164   2.823388e-01\n[55,] 0.023629663 0.04597701 1.877170e-04 -1.631075961   1.028743e-01\n[56,] 0.006281129 0.01149425 4.916619e-05 -0.743472246   4.571958e-01\n[57,] 0.063918654 0.05747126 2.369553e-04  0.418842387   6.753313e-01\n[58,] 0.070325003 0.05747126 2.381374e-04  0.832943753   4.048765e-01\n[59,] 0.025947288 0.03448276 1.444058e-04 -0.710289083   4.775249e-01\n[60,] 0.039752578 0.04597701 1.915656e-04 -0.449718820   6.529132e-01\n[61,] 0.049934283 0.05747126 2.334965e-04 -0.493238743   6.218439e-01\n[62,] 0.030964195 0.04597701 1.920248e-04 -1.083386776   2.786368e-01\n[63,] 0.058129184 0.05747126 2.343319e-04  0.042979051   9.657182e-01\n[64,] 0.046096514 0.04597701 1.932637e-04  0.008596093   9.931414e-01\n[65,] 0.012459080 0.01149425 5.008051e-05  0.136337469   8.915545e-01\n[66,] 0.091447733 0.05747126 2.377744e-04  2.203411744   2.756574e-02\n[67,] 0.049575872 0.02298851 9.766513e-05  2.690329952   7.138140e-03\n[68,] 0.107907212 0.04597701 1.933581e-04  4.453703219   8.440175e-06\n[69,] 0.019616151 0.02298851 9.789454e-05 -0.340842743   7.332220e-01\n[70,] 0.032923393 0.03448276 1.454032e-04 -0.129318589   8.971056e-01\n[71,] 0.030317663 0.02298851 9.867859e-05  0.737806634   4.606320e-01\n[72,] 0.019437582 0.03448276 1.455870e-04 -1.246912658   2.124295e-01\n[73,] 0.055245460 0.04597701 1.932838e-04  0.666667559   5.049845e-01\n[74,] 0.074278054 0.05747126 2.383538e-04  1.088613505   2.763244e-01\n[75,] 0.013269580 0.02298851 9.719982e-05 -0.985792573   3.242349e-01\n[76,] 0.049407829 0.03448276 1.463785e-04  1.233609606   2.173484e-01\n[77,] 0.028605749 0.03448276 1.455139e-04 -0.487196415   6.261191e-01\n[78,] 0.039087662 0.02298851 9.801040e-05  1.626174042   1.039126e-01\n[79,] 0.031447120 0.04597701 1.877464e-04 -1.060416797   2.889550e-01\n[80,] 0.064005294 0.05747126 2.359641e-04  0.425361422   6.705732e-01\n[81,] 0.044606529 0.05747126 2.357330e-04 -0.837897118   4.020885e-01\n[82,] 0.063700493 0.06896552 2.801427e-04 -0.314565243   7.530918e-01\n[83,] 0.051142205 0.04597701 1.933560e-04  0.371456331   7.102977e-01\n[84,] 0.102121112 0.04597701 1.610278e-04  4.424392623   9.671399e-06\n[85,] 0.021901462 0.02298851 9.843172e-05 -0.109566928   9.127528e-01\n[86,] 0.064931813 0.04597701 1.929430e-04  1.364597995   1.723794e-01\n[87,] 0.031747344 0.04597701 1.909867e-04 -1.029658605   3.031703e-01\n[88,] 0.015893319 0.02298851 9.765131e-05 -0.718000620   4.727569e-01\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nThe output of localG() is a vector of G or Gstar values, with attributes “gstari” set to TRUE or FALSE, “call” set to the function call, and class “localG”.\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\nNext, we will join the Gi values to their corresponding hunan sf data frame by using the code chunk below.\n\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\nIn fact, the code chunk above performs three tasks. First, it convert the output vector (i.e. gi.fixed) into r matrix object by using as.matrix(). Next, cbind() is used to join hunan@data and gi.fixed matrix to produce a new SpatialPolygonDataFrame called hunan.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename().\n\n\n9.2 Mapping Gi values with fixed distance weights\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nGimap &lt;-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)\n\n\n\n\nQuestion: What statistical observation can you draw from the Gi map above?\nAns: High Gi values may represent hotspots where values are significantly higher than their neighbors, while low Gi values may represent coldspots with significantly lower values.\n\n\n9.3 Gi statistics using adaptive distance\nThe code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e knb_lw).\n\nfips &lt;- order(hunan$County)\ngi.adaptive &lt;- localG(hunan$GDPPC, knn_lw)\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n\n9.4 Mapping Gi values with adaptive distance weights\nIt is time for us to visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc&lt;- qtm(hunan, \"GDPPC\")\n\nGimap &lt;- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, \n             Gimap, \n             asp=1, \n             ncol=2)\n\n\n\n\nQuestion: What statistical observation can you draw from the Gi map above?\nAns: When using adaptive distance weights in spatial analysis, the weights assigned to neighboring observations are not fixed but are adjusted based on the local characteristics of the data. This adaptation is often done to account for spatial heterogeneity, allowing the weights to vary across different regions."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex15/Hands-on_Ex15.html",
    "href": "Hands-on_Exercise/Hands-on_Ex15/Hands-on_Ex15.html",
    "title": "Hands-on_Ex15",
    "section": "",
    "text": "Spatial interaction represent the flow of people, material, or information between locations in geographical space. It encompasses everything from freight shipments, energy flows, and the global trade in rare antiquities, to flight schedules, rush hour woes, and pedestrian foot traffic.\nEach spatial interaction, as an analogy for a set of movements, is composed of a discrete origin/destination pair. Each pair can be represented as a cell in a matrix where rows are related to the locations (centroids) of origin, while columns are related to locations (centroids) of destination. Such a matrix is commonly known as an origin/destination matrix, or a spatial interaction matrix.\nIn this hands-on exercise, you will learn how to build an OD matrix by using Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall. By the end of this hands-on exercise, you will be able:\n\nto import and extract OD data for a selected time interval,\nto import and save geospatial data (i.e. bus stops and mpsz) into sf tibble data frame objects,\nto populate planning subzone code into bus stops sf tibble data frame,\nto construct desire lines geospatial data from the OD data, and\nto visualise passenger volume by origin and destination bus stops by using the desire lines data."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex15/Hands-on_Ex15.html#overview",
    "href": "Hands-on_Exercise/Hands-on_Ex15/Hands-on_Ex15.html#overview",
    "title": "Hands-on_Ex15",
    "section": "",
    "text": "Spatial interaction represent the flow of people, material, or information between locations in geographical space. It encompasses everything from freight shipments, energy flows, and the global trade in rare antiquities, to flight schedules, rush hour woes, and pedestrian foot traffic.\nEach spatial interaction, as an analogy for a set of movements, is composed of a discrete origin/destination pair. Each pair can be represented as a cell in a matrix where rows are related to the locations (centroids) of origin, while columns are related to locations (centroids) of destination. Such a matrix is commonly known as an origin/destination matrix, or a spatial interaction matrix.\nIn this hands-on exercise, you will learn how to build an OD matrix by using Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall. By the end of this hands-on exercise, you will be able:\n\nto import and extract OD data for a selected time interval,\nto import and save geospatial data (i.e. bus stops and mpsz) into sf tibble data frame objects,\nto populate planning subzone code into bus stops sf tibble data frame,\nto construct desire lines geospatial data from the OD data, and\nto visualise passenger volume by origin and destination bus stops by using the desire lines data."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex15/Hands-on_Ex15.html#getting-started",
    "href": "Hands-on_Exercise/Hands-on_Ex15/Hands-on_Ex15.html#getting-started",
    "title": "Hands-on_Ex15",
    "section": "2 Getting Started",
    "text": "2 Getting Started\nFor the purpose of this exercise, four r packages will be used. They are:\n\nsf for importing, integrating, processing and transforming geospatial data.\ntidyverse for importing, integrating, wrangling and visualising data.\ntmap for creating thematic maps.\n\n\npacman::p_load(tmap, sf, DT, stplanr,\n               performance,\n               ggpubr, tidyverse)"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex15/Hands-on_Ex15.html#preparing-the-flow-data",
    "href": "Hands-on_Exercise/Hands-on_Ex15/Hands-on_Ex15.html#preparing-the-flow-data",
    "title": "Hands-on_Ex15",
    "section": "3 Preparing the Flow Data",
    "text": "3 Preparing the Flow Data\n\n3.1 Importing the OD data\nFirstly, we will import the Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall by using read_csv() of readr package.\n\nodbus &lt;- read_csv(\"data/aspatial/origin_destination_bus_202310.csv\")\n\nLet use display the odbus tibble data table by using the code chunk below.\n\nglimpse(odbus)\n\nRows: 5,694,297\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-10\", \"2023-10\", \"2023-10\", \"2023-10\", \"2023-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKENDS/HOLIDAY\", \"WEEKDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;dbl&gt; 16, 16, 14, 14, 17, 17, 17, 7, 14, 14, 10, 20, 20,…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;chr&gt; \"04168\", \"04168\", \"80119\", \"80119\", \"44069\", \"2028…\n$ DESTINATION_PT_CODE &lt;chr&gt; \"10051\", \"10051\", \"90079\", \"90079\", \"17229\", \"2014…\n$ TOTAL_TRIPS         &lt;dbl&gt; 3, 5, 3, 5, 4, 1, 24, 2, 1, 7, 3, 2, 5, 1, 1, 1, 1…\n\n\nA quick check of odbus tibble data frame shows that the values in OROGIN_PT_CODE and DESTINATON_PT_CODE are in numeric data type. Hence, the code chunk below is used to convert these data values into character data type.\n\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE) \n\n\n\n3.2 Extracting the study data\nFor the purpose of this exercise, we will extract commuting flows on weekday and between 6 and 9 o’clock.\n\nodbus6_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE,\n           DESTINATION_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\nTable below shows the content of odbus6_9\n\ndatatable(odbus6_9)\n\n\n\n\n\n\nWe will save the output in rds format for future used.\n\nwrite_rds(odbus6_9, \"data/rds/odbus6_9.rds\")\n\nThe code chunk below will be used to import the save odbus6_9.rds into R environment.\n\nodbus6_9 &lt;- read_rds(\"data/rds/odbus6_9.rds\")"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex15/Hands-on_Ex15.html#working-with-geospatial-data",
    "href": "Hands-on_Exercise/Hands-on_Ex15/Hands-on_Ex15.html#working-with-geospatial-data",
    "title": "Hands-on_Ex15",
    "section": "4 Working with Geospatial Data",
    "text": "4 Working with Geospatial Data\nFor the purpose of this exercise, two geospatial data will be used. They are:\n\nBusStop: This data provides the location of bus stop as at last quarter of 2022.\nMPSZ-2019: This data provides the sub-zone boundary of URA Master Plan 2019.\n\nBoth data sets are in ESRI shapefile format.\n\n4.1 Importing geospatial data\nTwo geospatial data will be used in this exercise, they are:\n\nbusstop &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"BusStop\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `BusStop' from data source \n  `W:\\widyayutika\\ISSS624\\Hands-on_Exercise\\Hands-on_Ex15\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `W:\\widyayutika\\ISSS624\\Hands-on_Exercise\\Hands-on_Ex15\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\nmpsz\n\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                 SUBZONE_N SUBZONE_C       PLN_AREA_N PLN_AREA_C       REGION_N\n1              MARINA EAST    MESZ01      MARINA EAST         ME CENTRAL REGION\n2         INSTITUTION HILL    RVSZ05     RIVER VALLEY         RV CENTRAL REGION\n3           ROBERTSON QUAY    SRSZ01  SINGAPORE RIVER         SR CENTRAL REGION\n4  JURONG ISLAND AND BUKOM    WISZ01  WESTERN ISLANDS         WI    WEST REGION\n5             FORT CANNING    MUSZ02           MUSEUM         MU CENTRAL REGION\n6         MARINA EAST (MP)    MPSZ05    MARINE PARADE         MP CENTRAL REGION\n7                   SUDONG    WISZ03  WESTERN ISLANDS         WI    WEST REGION\n8                  SEMAKAU    WISZ02  WESTERN ISLANDS         WI    WEST REGION\n9           SOUTHERN GROUP    SISZ02 SOUTHERN ISLANDS         SI CENTRAL REGION\n10                 SENTOSA    SISZ01 SOUTHERN ISLANDS         SI CENTRAL REGION\n   REGION_C                       geometry\n1        CR MULTIPOLYGON (((33222.98 29...\n2        CR MULTIPOLYGON (((28481.45 30...\n3        CR MULTIPOLYGON (((28087.34 30...\n4        WR MULTIPOLYGON (((14557.7 304...\n5        CR MULTIPOLYGON (((29542.53 31...\n6        CR MULTIPOLYGON (((35279.55 30...\n7        WR MULTIPOLYGON (((15772.59 21...\n8        WR MULTIPOLYGON (((19843.41 21...\n9        CR MULTIPOLYGON (((30870.53 22...\n10       CR MULTIPOLYGON (((26879.04 26...\n\n\n\nwrite_rds(mpsz, \"data/rds/mpsz.rds\")  \n\n\nNote\n\nst_read() function of sf package is used to import the shapefile into R as sf data frame.\nst_transform() function of sf package is used to transform the projection to crs 3414."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex15/Hands-on_Ex15.html#geospatial-data-wrangling",
    "href": "Hands-on_Exercise/Hands-on_Ex15/Hands-on_Ex15.html#geospatial-data-wrangling",
    "title": "Hands-on_Ex15",
    "section": "5 Geospatial data wrangling",
    "text": "5 Geospatial data wrangling\n\n5.1 Combining Busstop and mpsz\nCode chunk below populates the planning subzone code (i.e. SUBZONE_C) of mpsz sf data frame into busstop sf data frame.\n\nbusstop_mpsz &lt;- st_intersection(busstop, mpsz) %&gt;%\n  select(BUS_STOP_N, SUBZONE_C) %&gt;%\n  st_drop_geometry()\n\nNote\n\nst_intersection() is used to perform point and polygon overly and the output will be in point sf object.\nselect() of dplyr package is then use to retain only BUS_STOP_N and SUBZONE_C in the busstop_mpsz sf data frame.\nfive bus stops are excluded in the resultant data frame because they are outside of Singapore boundary.\n\n\ndatatable(busstop_mpsz)\n\n\n\n\n\n\nBefore moving to the next step, it is wise to save the output into rds format.\n\nwrite_rds(busstop_mpsz, \"data/rds/busstop_mpsz.rds\")  \n\nNext, we are going to append the planning subzone code from busstop_mpsz data frame onto odbus6_9 data frame.\n\nod_data &lt;- left_join(odbus6_9 , busstop_mpsz,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         ORIGIN_SZ = SUBZONE_C,\n         DESTIN_BS = DESTINATION_PT_CODE)\n\nBefore continue, it is a good practice for us to check for duplicating records.\n\nduplicate &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nIf duplicated records are found, the code chunk below will be used to retain the unique records.\n\nod_data &lt;- unique(od_data)\n\nIt will be a good practice to confirm if the duplicating records issue has been addressed fully.\nNext, we will update od_data data frame with the planning subzone codes.\n\nod_data &lt;- left_join(od_data , busstop_mpsz,\n            by = c(\"DESTIN_BS\" = \"BUS_STOP_N\")) \n\n\nduplicate &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\n\nod_data &lt;- unique(od_data)\n\n\nod_data &lt;- od_data %&gt;%\n  rename(DESTIN_SZ = SUBZONE_C) %&gt;%\n  drop_na() %&gt;%\n  group_by(ORIGIN_SZ, DESTIN_SZ) %&gt;%\n  summarise(MORNING_PEAK = sum(TRIPS))\n\nIt is time to save the output into an rds file format.\n\nwrite_rds(od_data, \"data/rds/od_data.rds\")\n\n\nod_data &lt;- read_rds(\"data/rds/od_data.rds\")"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex15/Hands-on_Ex15.html#visualising-spatial-interaction",
    "href": "Hands-on_Exercise/Hands-on_Ex15/Hands-on_Ex15.html#visualising-spatial-interaction",
    "title": "Hands-on_Ex15",
    "section": "6 Visualising Spatial Interaction",
    "text": "6 Visualising Spatial Interaction\nIn this section, you will learn how to prepare a desire line by using stplanr package.\n\n6.1 Removing intra-zonal flows\nWe will not plot the intra-zonal flows. The code chunk below will be used to remove intra-zonal flows.\n\nod_data1 &lt;- od_data[od_data$ORIGIN_SZ!=od_data$DESTIN_SZ,]\n\n\n\n6.2 Creating desire lines\nIn this code chunk below, od2line() of stplanr package is used to create the desire lines.\n\nflowLine &lt;- od2line(flow = od_data1, \n                    zones = mpsz,\n                    zone_code = \"SUBZONE_C\")\n\n\n\n6.3 Visualising the desire lines\nTo visualise the resulting desire lines, the code chunk below is used.\n\ntmap_options(check.and.fix = TRUE)\ntm_shape(mpsz) +\n  tm_polygons() +\nflowLine %&gt;%  \ntm_shape() +\n  tm_lines(lwd = \"MORNING_PEAK\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.3)\n\n\n\n\nWarning\nBe patient, the rendering process takes more time because of the transparency argument (i.e. alpha)\nWhen the flow data are very messy and highly skewed like the one shown above, it is wiser to focus on selected flows, for example flow greater than or equal to 5000 as shown below.\n\ntmap_options(check.and.fix = TRUE)\ntm_shape(mpsz) +\n  tm_polygons() +\nflowLine %&gt;%  \n  filter(MORNING_PEAK &gt;= 200) %&gt;%\ntm_shape() +\n  tm_lines(lwd = \"MORNING_PEAK\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.3)"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex8/Hands-on_Ex8.html",
    "href": "Hands-on_Exercise/Hands-on_Ex8/Hands-on_Ex8.html",
    "title": "Hands-on_Ex8: Spatial Weights and Applications",
    "section": "",
    "text": "In this hands-on exercise, I will learn how to compute spatial weights using R including:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute spatial weights using appropriate functions of spdep package, and\ncalculate spatially lagged variables using appropriate functions of spdep package."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex8/Hands-on_Ex8.html#overview",
    "href": "Hands-on_Exercise/Hands-on_Ex8/Hands-on_Ex8.html#overview",
    "title": "Hands-on_Ex8: Spatial Weights and Applications",
    "section": "",
    "text": "In this hands-on exercise, I will learn how to compute spatial weights using R including:\n\nimport geospatial data using appropriate function(s) of sf package,\nimport csv file using appropriate function of readr package,\nperform relational join using appropriate join function of dplyr package,\ncompute spatial weights using appropriate functions of spdep package, and\ncalculate spatially lagged variables using appropriate functions of spdep package."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex8/Hands-on_Ex8.html#the-study-area-and-data",
    "href": "Hands-on_Exercise/Hands-on_Ex8/Hands-on_Ex8.html#the-study-area-and-data",
    "title": "Hands-on_Ex8: Spatial Weights and Applications",
    "section": "2 The Study Area and Data",
    "text": "2 The Study Area and Data\nTwo data sets will be used in this hands-on exercise, they are:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n2.1 Getting Started\nBefore we get started, we need to ensure that spdep, sf, tmap, knitr and tidyverse packages of R are currently installed in your R.\n\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex8/Hands-on_Ex8.html#getting-the-data-into-r-environment",
    "href": "Hands-on_Exercise/Hands-on_Ex8/Hands-on_Ex8.html#getting-the-data-into-r-environment",
    "title": "Hands-on_Ex8: Spatial Weights and Applications",
    "section": "3 Getting the Data Into R Environment",
    "text": "3 Getting the Data Into R Environment\nIn this section, you will learn how to bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n3.1 Import shapefile into r environment\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `W:\\widyayutika\\ISSS624\\Hands-on_Exercise\\Hands-on_Ex8\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n3.2 Import csv file into r environment\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R dataframe class.\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n3.3 Performing relational join\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan,hunan2012)%&gt;%\n  select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex8/Hands-on_Ex8.html#visualising-regional-development-indicator",
    "href": "Hands-on_Exercise/Hands-on_Ex8/Hands-on_Ex8.html#visualising-regional-development-indicator",
    "title": "Hands-on_Ex8: Spatial Weights and Applications",
    "section": "4 Visualising Regional Development Indicator",
    "text": "4 Visualising Regional Development Indicator\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nbasemap &lt;- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5)\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex8/Hands-on_Ex8.html#computing-contiguity-spatial-weights",
    "href": "Hands-on_Exercise/Hands-on_Ex8/Hands-on_Ex8.html#computing-contiguity-spatial-weights",
    "title": "Hands-on_Ex8: Spatial Weights and Applications",
    "section": "5 Computing Contiguity Spatial Weights",
    "text": "5 Computing Contiguity Spatial Weights\nIn this section, you will learn how to use poly2nb() of spdep package to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\n\n5.1 Computing (QUEEN) contiguity based neighbours\nBelow are the concept of QUEEN contiguity.\n\nThe code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. For example, to see the neighbors for the first polygon in the object, type:\n\nwm_q[[1]]\n\n[1]  2  3  4 57 85\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.\nWe can retrieve the county name of Polygon ID=1 by using the code chunk below:\n\nhunan$County[1]\n\n[1] \"Anxiang\"\n\n\nThe output reveals that Polygon ID=1 is Anxiang county.\nTo reveal the county names of the five neighboring polygons, the code chunk will be used:\n\n#hunan$NAME_3[c(2,3,4,57,85)]\n\nWe can retrieve the GDPPC of these five countries by using the code chunk below.\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen’s method are 20981, 34592, 24473, 21311 and 22879 respectively.\nYou can display the complete weight matrix by using str().\n\nstr(wm_q)\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE\n\n\nBe warned: The output might cut across several pages. Save the trees if you are going to print out the report.\n\n\n5.2 Creating (ROOK) contiguity based neighbours\nThe code chunk below is used to compute Rook contiguity weight matrix.\n\nwm_r &lt;- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one heighbours.\n\n\n5.3 Visualising contiguity weights\nA connectivity graph takes a point and displays a line to each neighboring point. We are working with polygons at the moment, so we will need to get points in order to make our connectivity graphs. The most typically method for this will be polygon centroids. We will calculate these in the sf package before moving onto the graphs. Getting Latitude and Longitude of Polygon Centroids\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid. We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\ncoords &lt;- cbind(longitude, latitude)\n\nWe check the first few observations to see if things are formatted correctly.\n\nhead(coords)\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\n\n5.3.1 Plotting Queen contiguity based neighbours map\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\n\n\n\n\nNotes:\npch: plot character (pch=19 means solid circles as plotting symbol)\ncex: character expansion (True: add points to existing plot, FALSE: start a new plot)\n\n\n5.3.2 Plotting Rook contiguity based neighbours map\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\n\n5.3.3 Plotting both Queen and Rook contiguity based neighbours maps\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"Queen Contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\nplot(hunan$geometry, border=\"lightgrey\", main=\"Rook Contiguity\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex8/Hands-on_Ex8.html#computing-distance-based-neighbours",
    "href": "Hands-on_Exercise/Hands-on_Ex8/Hands-on_Ex8.html#computing-distance-based-neighbours",
    "title": "Hands-on_Ex8: Spatial Weights and Applications",
    "section": "6 Computing distance based neighbours",
    "text": "6 Computing distance based neighbours\nIn this section, you will learn how to derive distance-based weight matrices by using dnearneigh() of spdep package.\nThe function identifies neighbours of region points by Euclidean distance with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument. If unprojected coordinates are used and either specified in the coordinates object x or with x as a two column matrix and longlat=TRUE, great circle distances in km will be calculated assuming the WGS84 reference ellipsoid.\n\n6.1 Determine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n6.2 Computing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nQuiz: What is the meaning of “Average number of links: 3.681818” shown above?\nAns: On average, each region is connected to around 3.681818 other regions within the specified distance threshold.\nNext, we will use str() to display the content of wm_d62 weight matrix.\n\nstr(wm_d62)\n\nList of 88\n $ : int [1:5] 3 4 5 57 64\n $ : int [1:4] 57 58 78 85\n $ : int [1:4] 1 4 5 57\n $ : int [1:3] 1 3 5\n $ : int [1:4] 1 3 4 85\n $ : int 69\n $ : int [1:2] 67 84\n $ : int [1:4] 9 46 47 78\n $ : int [1:4] 8 46 68 84\n $ : int [1:4] 16 22 70 72\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:2] 11 17\n $ : int 13\n $ : int [1:4] 10 17 22 83\n $ : int [1:3] 11 14 16\n $ : int [1:3] 20 22 63\n $ : int [1:5] 20 21 73 74 82\n $ : int [1:5] 18 19 21 22 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:4] 10 16 18 20\n $ : int [1:3] 41 77 82\n $ : int [1:4] 25 28 31 54\n $ : int [1:4] 24 28 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:2] 26 29\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:2] 27 37\n $ : int 33\n $ : int [1:2] 24 36\n $ : int 50\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:5] 31 34 45 56 80\n $ : int [1:2] 29 42\n $ : int [1:3] 44 77 79\n $ : int [1:4] 40 42 43 81\n $ : int [1:3] 39 45 79\n $ : int [1:5] 23 35 45 79 82\n $ : int [1:5] 26 37 39 43 81\n $ : int [1:3] 39 42 44\n $ : int [1:2] 38 43\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:5] 8 9 35 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:4] 48 49 50 52\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:2] 48 55\n $ : int [1:5] 24 28 49 50 52\n $ : int [1:4] 48 50 53 75\n $ : int 36\n $ : int [1:5] 1 2 3 58 64\n $ : int [1:5] 2 57 64 66 68\n $ : int [1:3] 60 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:5] 12 60 62 63 87\n $ : int [1:4] 61 63 77 87\n $ : int [1:5] 12 18 61 62 83\n $ : int [1:4] 1 57 58 76\n $ : int 76\n $ : int [1:5] 58 67 68 76 84\n $ : int [1:2] 7 66\n $ : int [1:4] 9 58 66 84\n $ : int [1:2] 6 75\n $ : int [1:3] 10 72 73\n $ : int [1:2] 73 74\n $ : int [1:3] 10 11 70\n $ : int [1:4] 19 70 71 74\n $ : int [1:5] 19 21 71 73 86\n $ : int [1:2] 55 69\n $ : int [1:3] 64 65 66\n $ : int [1:3] 23 38 62\n $ : int [1:2] 2 8\n $ : int [1:4] 38 40 41 45\n $ : int [1:5] 34 35 36 45 47\n $ : int [1:5] 25 26 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:4] 12 13 16 63\n $ : int [1:4] 7 9 66 68\n $ : int [1:2] 2 5\n $ : int [1:4] 21 46 47 74\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language dnearneigh(x = coords, d1 = 0, d2 = 62, longlat = TRUE)\n - attr(*, \"dnn\")= num [1:2] 0 62\n - attr(*, \"bounds\")= chr [1:2] \"GE\" \"LE\"\n - attr(*, \"nbtype\")= chr \"distance\"\n - attr(*, \"sym\")= logi TRUE\n\n\nAnother way to display the structure of the weight matrix is to combine table() and card() of spdep.\n\ntable(hunan$County, card(wm_d62))\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\n\nn_comp &lt;- n.comp.nb(wm_d62)\nn_comp$nc\n\n[1] 1\n\n\n\ntable(n_comp$comp.id)\n\n\n 1 \n88 \n\n\n\n6.2.1 Plotting fixed distance weight matrix\nNext, we will plot the distance weight matrix by using the code chunk below.\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\n\n\n\n\nThe red lines show the links of 1st nearest neighbours and the black lines show the links of neighbours within the cut-off distance of 62km.\nAlternatively, we can plot both of them next to each other by using the code chunk below.\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"1st nearest neighbours\")\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\nplot(hunan$geometry, border=\"lightgrey\", main=\"Distance link\")\nplot(wm_d62, coords, add=TRUE, pch = 19, cex = 0.6)\n\n\n\n\n\n\n\n6.3 Computing adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn6 &lt;- knn2nb(knearneigh(coords, k=6))\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nSimilarly, we can display the content of the matrix by using str().\n\nstr(knn6)\n\nList of 88\n $ : int [1:6] 2 3 4 5 57 64\n $ : int [1:6] 1 3 57 58 78 85\n $ : int [1:6] 1 2 4 5 57 85\n $ : int [1:6] 1 3 5 6 69 85\n $ : int [1:6] 1 3 4 6 69 85\n $ : int [1:6] 3 4 5 69 75 85\n $ : int [1:6] 9 66 67 71 74 84\n $ : int [1:6] 9 46 47 78 80 86\n $ : int [1:6] 8 46 66 68 84 86\n $ : int [1:6] 16 19 22 70 72 73\n $ : int [1:6] 10 14 16 17 70 72\n $ : int [1:6] 13 15 60 61 63 83\n $ : int [1:6] 12 15 60 61 63 83\n $ : int [1:6] 11 15 16 17 72 83\n $ : int [1:6] 12 13 14 17 60 83\n $ : int [1:6] 10 11 17 22 72 83\n $ : int [1:6] 10 11 14 16 72 83\n $ : int [1:6] 20 22 23 63 77 83\n $ : int [1:6] 10 20 21 73 74 82\n $ : int [1:6] 18 19 21 22 23 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:6] 10 16 18 19 20 83\n $ : int [1:6] 18 20 41 77 79 82\n $ : int [1:6] 25 28 31 52 54 81\n $ : int [1:6] 24 28 31 33 54 81\n $ : int [1:6] 25 27 29 33 42 81\n $ : int [1:6] 26 29 30 37 42 81\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:6] 26 27 37 42 43 81\n $ : int [1:6] 26 27 28 33 49 81\n $ : int [1:6] 24 25 36 39 40 54\n $ : int [1:6] 24 31 50 54 55 56\n $ : int [1:6] 25 26 28 30 49 81\n $ : int [1:6] 36 40 41 45 56 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:6] 26 27 29 42 43 44\n $ : int [1:6] 23 43 44 62 77 79\n $ : int [1:6] 25 40 42 43 44 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:6] 26 27 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:6] 37 38 39 42 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:6] 8 9 35 47 78 86\n $ : int [1:6] 8 21 35 46 80 86\n $ : int [1:6] 49 50 51 52 53 55\n $ : int [1:6] 28 33 48 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:6] 28 48 49 50 52 54\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:6] 48 50 51 52 55 75\n $ : int [1:6] 24 28 49 50 51 52\n $ : int [1:6] 32 48 50 52 53 75\n $ : int [1:6] 32 34 36 78 80 85\n $ : int [1:6] 1 2 3 58 64 68\n $ : int [1:6] 2 57 64 66 68 78\n $ : int [1:6] 12 13 60 61 87 88\n $ : int [1:6] 12 13 59 61 63 87\n $ : int [1:6] 12 13 60 62 63 87\n $ : int [1:6] 12 38 61 63 77 87\n $ : int [1:6] 12 18 60 61 62 83\n $ : int [1:6] 1 3 57 58 68 76\n $ : int [1:6] 58 64 66 67 68 76\n $ : int [1:6] 9 58 67 68 76 84\n $ : int [1:6] 7 65 66 68 76 84\n $ : int [1:6] 9 57 58 66 78 84\n $ : int [1:6] 4 5 6 32 75 85\n $ : int [1:6] 10 16 19 22 72 73\n $ : int [1:6] 7 19 73 74 84 86\n $ : int [1:6] 10 11 14 16 17 70\n $ : int [1:6] 10 19 21 70 71 74\n $ : int [1:6] 19 21 71 73 84 86\n $ : int [1:6] 6 32 50 53 55 69\n $ : int [1:6] 58 64 65 66 67 68\n $ : int [1:6] 18 23 38 61 62 63\n $ : int [1:6] 2 8 9 46 58 68\n $ : int [1:6] 38 40 41 43 44 45\n $ : int [1:6] 34 35 36 41 45 47\n $ : int [1:6] 25 26 28 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:6] 12 13 15 16 22 63\n $ : int [1:6] 7 9 66 68 71 74\n $ : int [1:6] 2 3 4 5 56 69\n $ : int [1:6] 8 9 21 46 47 74\n $ : int [1:6] 59 60 61 62 63 88\n $ : int [1:6] 59 60 61 62 63 87\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language knearneigh(x = coords, k = 6)\n - attr(*, \"sym\")= logi FALSE\n - attr(*, \"type\")= chr \"knn\"\n - attr(*, \"knn-k\")= num 6\n - attr(*, \"class\")= chr \"nb\"\n\n\nNotice that each county has six neighbours, no less no more!\n\n6.3.1 Plotting distance based neighbours\nWe can plot the weight matrix using the code chunk below.\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex8/Hands-on_Ex8.html#weights-based-on-idw",
    "href": "Hands-on_Exercise/Hands-on_Ex8/Hands-on_Ex8.html#weights-based-on-idw",
    "title": "Hands-on_Ex8: Spatial Weights and Applications",
    "section": "7 Weights based on IDW",
    "text": "7 Weights based on IDW\nIn this section, you will learn how to derive a spatial weight matrix based on Inversed Distance method.\nFirst, we will compute the distances between areas by using nbdists() of spdep.\n\ndist &lt;- nbdists(wm_q, coords, longlat = TRUE)\nids &lt;- lapply(dist, function(x) 1/(x))\nids\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n[[2]]\n[1] 0.01535405 0.01764308 0.01925924 0.02323898 0.01719350\n\n[[3]]\n[1] 0.03916350 0.02822040 0.03695795 0.01395765\n\n[[4]]\n[1] 0.01820896 0.02822040 0.03414741 0.01539065\n\n[[5]]\n[1] 0.03695795 0.03414741 0.01524598 0.01618354\n\n[[6]]\n[1] 0.015390649 0.015245977 0.021748129 0.011883901 0.009810297\n\n[[7]]\n[1] 0.01708612 0.01473997 0.01150924 0.01872915\n\n[[8]]\n[1] 0.02022144 0.03453056 0.02529256 0.01036340 0.02284457 0.01500600 0.01515314\n\n[[9]]\n[1] 0.02022144 0.01574888 0.02109502 0.01508028 0.02902705 0.01502980\n\n[[10]]\n[1] 0.02281552 0.01387777 0.01538326 0.01346650 0.02100510 0.02631658 0.01874863\n[8] 0.01500046\n\n[[11]]\n[1] 0.01882869 0.02243492 0.02247473\n\n[[12]]\n[1] 0.02779227 0.02419652 0.02333385 0.02986130 0.02335429\n\n[[13]]\n[1] 0.02779227 0.02650020 0.02670323 0.01714243\n\n[[14]]\n[1] 0.01882869 0.01233868 0.02098555\n\n[[15]]\n[1] 0.02650020 0.01233868 0.01096284 0.01562226\n\n[[16]]\n[1] 0.02281552 0.02466962 0.02765018 0.01476814 0.01671430\n\n[[17]]\n[1] 0.01387777 0.02243492 0.02098555 0.01096284 0.02466962 0.01593341 0.01437996\n\n[[18]]\n[1] 0.02039779 0.02032767 0.01481665 0.01473691 0.01459380\n\n[[19]]\n[1] 0.01538326 0.01926323 0.02668415 0.02140253 0.01613589 0.01412874\n\n[[20]]\n[1] 0.01346650 0.02039779 0.01926323 0.01723025 0.02153130 0.01469240 0.02327034\n\n[[21]]\n[1] 0.02668415 0.01723025 0.01766299 0.02644986 0.02163800\n\n[[22]]\n[1] 0.02100510 0.02765018 0.02032767 0.02153130 0.01489296\n\n[[23]]\n[1] 0.01481665 0.01469240 0.01401432 0.02246233 0.01880425 0.01530458 0.01849605\n\n[[24]]\n[1] 0.02354598 0.01837201 0.02607264 0.01220154 0.02514180\n\n[[25]]\n[1] 0.02354598 0.02188032 0.01577283 0.01949232 0.02947957\n\n[[26]]\n[1] 0.02155798 0.01745522 0.02212108 0.02220532\n\n[[27]]\n[1] 0.02155798 0.02490625 0.01562326\n\n[[28]]\n[1] 0.01837201 0.02188032 0.02229549 0.03076171 0.02039506\n\n[[29]]\n[1] 0.02490625 0.01686587 0.01395022\n\n[[30]]\n[1] 0.02090587\n\n[[31]]\n[1] 0.02607264 0.01577283 0.01219005 0.01724850 0.01229012 0.01609781 0.01139438\n[8] 0.01150130\n\n[[32]]\n[1] 0.01220154 0.01219005 0.01712515 0.01340413 0.01280928 0.01198216 0.01053374\n[8] 0.01065655\n\n[[33]]\n[1] 0.01949232 0.01745522 0.02229549 0.02090587 0.01979045\n\n[[34]]\n[1] 0.03113041 0.03589551 0.02882915\n\n[[35]]\n[1] 0.01766299 0.02185795 0.02616766 0.02111721 0.02108253 0.01509020\n\n[[36]]\n[1] 0.01724850 0.03113041 0.01571707 0.01860991 0.02073549 0.01680129\n\n[[37]]\n[1] 0.01686587 0.02234793 0.01510990 0.01550676\n\n[[38]]\n[1] 0.01401432 0.02407426 0.02276151 0.01719415\n\n[[39]]\n[1] 0.01229012 0.02172543 0.01711924 0.02629732 0.01896385\n\n[[40]]\n[1] 0.01609781 0.01571707 0.02172543 0.01506473 0.01987922 0.01894207\n\n[[41]]\n[1] 0.02246233 0.02185795 0.02205991 0.01912542 0.01601083 0.01742892\n\n[[42]]\n[1] 0.02212108 0.01562326 0.01395022 0.02234793 0.01711924 0.01836831 0.01683518\n\n[[43]]\n[1] 0.01510990 0.02629732 0.01506473 0.01836831 0.03112027 0.01530782\n\n[[44]]\n[1] 0.01550676 0.02407426 0.03112027 0.01486508\n\n[[45]]\n[1] 0.03589551 0.01860991 0.01987922 0.02205991 0.02107101 0.01982700\n\n[[46]]\n[1] 0.03453056 0.04033752 0.02689769\n\n[[47]]\n[1] 0.02529256 0.02616766 0.04033752 0.01949145 0.02181458\n\n[[48]]\n[1] 0.02313819 0.03370576 0.02289485 0.01630057 0.01818085\n\n[[49]]\n[1] 0.03076171 0.02138091 0.02394529 0.01990000\n\n[[50]]\n[1] 0.01712515 0.02313819 0.02551427 0.02051530 0.02187179\n\n[[51]]\n[1] 0.03370576 0.02138091 0.02873854\n\n[[52]]\n[1] 0.02289485 0.02394529 0.02551427 0.02873854 0.03516672\n\n[[53]]\n[1] 0.01630057 0.01979945 0.01253977\n\n[[54]]\n[1] 0.02514180 0.02039506 0.01340413 0.01990000 0.02051530 0.03516672\n\n[[55]]\n[1] 0.01280928 0.01818085 0.02187179 0.01979945 0.01882298\n\n[[56]]\n[1] 0.01036340 0.01139438 0.01198216 0.02073549 0.01214479 0.01362855 0.01341697\n\n[[57]]\n[1] 0.028079221 0.017643082 0.031423501 0.029114131 0.013520292 0.009903702\n\n[[58]]\n[1] 0.01925924 0.03142350 0.02722997 0.01434859 0.01567192\n\n[[59]]\n[1] 0.01696711 0.01265572 0.01667105 0.01785036\n\n[[60]]\n[1] 0.02419652 0.02670323 0.01696711 0.02343040\n\n[[61]]\n[1] 0.02333385 0.01265572 0.02343040 0.02514093 0.02790764 0.01219751 0.02362452\n\n[[62]]\n[1] 0.02514093 0.02002219 0.02110260\n\n[[63]]\n[1] 0.02986130 0.02790764 0.01407043 0.01805987\n\n[[64]]\n[1] 0.02911413 0.01689892\n\n[[65]]\n[1] 0.02471705\n\n[[66]]\n[1] 0.01574888 0.01726461 0.03068853 0.01954805 0.01810569\n\n[[67]]\n[1] 0.01708612 0.01726461 0.01349843 0.01361172\n\n[[68]]\n[1] 0.02109502 0.02722997 0.03068853 0.01406357 0.01546511\n\n[[69]]\n[1] 0.02174813 0.01645838 0.01419926\n\n[[70]]\n[1] 0.02631658 0.01963168 0.02278487\n\n[[71]]\n[1] 0.01473997 0.01838483 0.03197403\n\n[[72]]\n[1] 0.01874863 0.02247473 0.01476814 0.01593341 0.01963168\n\n[[73]]\n[1] 0.01500046 0.02140253 0.02278487 0.01838483 0.01652709\n\n[[74]]\n[1] 0.01150924 0.01613589 0.03197403 0.01652709 0.01342099 0.02864567\n\n[[75]]\n[1] 0.011883901 0.010533736 0.012539774 0.018822977 0.016458383 0.008217581\n\n[[76]]\n[1] 0.01352029 0.01434859 0.01689892 0.02471705 0.01954805 0.01349843 0.01406357\n\n[[77]]\n[1] 0.014736909 0.018804247 0.022761507 0.012197506 0.020022195 0.014070428\n[7] 0.008440896\n\n[[78]]\n[1] 0.02323898 0.02284457 0.01508028 0.01214479 0.01567192 0.01546511 0.01140779\n\n[[79]]\n[1] 0.01530458 0.01719415 0.01894207 0.01912542 0.01530782 0.01486508 0.02107101\n\n[[80]]\n[1] 0.01500600 0.02882915 0.02111721 0.01680129 0.01601083 0.01982700 0.01949145\n[8] 0.01362855\n\n[[81]]\n[1] 0.02947957 0.02220532 0.01150130 0.01979045 0.01896385 0.01683518\n\n[[82]]\n[1] 0.02327034 0.02644986 0.01849605 0.02108253 0.01742892\n\n[[83]]\n[1] 0.023354289 0.017142433 0.015622258 0.016714303 0.014379961 0.014593799\n[7] 0.014892965 0.018059871 0.008440896\n\n[[84]]\n[1] 0.01872915 0.02902705 0.01810569 0.01361172 0.01342099 0.01297994\n\n[[85]]\n [1] 0.011451133 0.017193502 0.013957649 0.016183544 0.009810297 0.010656545\n [7] 0.013416965 0.009903702 0.014199260 0.008217581 0.011407794\n\n[[86]]\n[1] 0.01515314 0.01502980 0.01412874 0.02163800 0.01509020 0.02689769 0.02181458\n[8] 0.02864567 0.01297994\n\n[[87]]\n[1] 0.01667105 0.02362452 0.02110260 0.02058034\n\n[[88]]\n[1] 0.01785036 0.02058034\n\n\n\n7.1 Row-standardised weights matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nThe zero.policy=TRUE option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset however, a zero.policy of FALSE would return an error.\nTo see the weight of the first polygon’s eight neighbors type:\n\nrswm_q$weights[10]\n\n[[1]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n\nEach neighbor is assigned a 0.125 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by 0.125 before being tallied.\nUsing the same method, we can also derive a row standardised distance weight matrix by using the code chunk below.\n\nrswm_ids &lt;- nb2listw(wm_q, glist=ids, style=\"B\", zero.policy=TRUE)\nrswm_ids\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\n\n\nrswm_ids$weights[1]\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n\n\nsummary(unlist(rswm_ids$weights))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338 \n\n\nNotes: B is the basic binary coding, W is row standardised (sums over all links to n)."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex8/Hands-on_Ex8.html#application-of-spatial-weight-matrix",
    "href": "Hands-on_Exercise/Hands-on_Ex8/Hands-on_Ex8.html#application-of-spatial-weight-matrix",
    "title": "Hands-on_Ex8: Spatial Weights and Applications",
    "section": "8 Application of Spatial Weight Matrix",
    "text": "8 Application of Spatial Weight Matrix\nIn this section, you will learn how to create four different spatial lagged variables, they are:\n\nspatial lag with row-standardized weights,\nspatial lag as a sum of neighbouring values,\nspatial window average, and\nspatial window sum.\n\n\n8.1 Spatial lag with row-standardized weights\nFinally, we’ll compute the average neighbor GDPPC value for each polygon. These values are often referred to as spatially lagged values.\n\nGDPPC.lag &lt;- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nRecalled in the previous section, we retrieved the GDPPC of these five countries by using the code chunk below.\n\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\nQuestion: Can you see the meaning of Spatial lag with row-standardized weights now?\nAns: Yes, the spatial lag with row-standardized weights provides a measure of spatial autocorrelation, indicating how the GDPPC values in each region are related to the values in neighboring regions, with weights that account for the spatial structure in the data.\nWe can append the spatially lag GDPPC values onto hunan sf data frame by using the code chunk below.\n\nlag.list &lt;- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag GDPPC\")\nhunan &lt;- left_join(hunan,lag.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\nThe following table shows the average neighboring income values (stored in the Inc.lag object) for each county.\n\nhead(hunan)\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_gdppc &lt;- qtm(hunan, \"lag GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n8.2 Spatial lag as a sum of neighboring values\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist = in the nb2listw function to explicitly assign these weights.\nWe start by applying a function that will assign a value of 1 per each neighbor. This is done with lapply, which we have been using to manipulate the neighbors structure throughout the past notebooks. Basically it applies a function across each value in the neighbors structure.\n\nb_weights &lt;- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith the proper weights assigned, we can use lag.listw to compute a lag variable from our weight and GDPPC.\n\nlag_sum &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag_sum)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")\n\nFirst, let us examine the result by using the code chunk below.\n\nlag_sum\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 124236 113624  96573 110950 109081 106244 174988 235079 273907 256221\n[11]  98013 104050 102846  92017 133831 158446 141883 119508 150757 153324\n[21] 113593 129594 142149 100119  82884  74668  43184  99244  46549  20518\n[31] 140576 121601  92069  43258 144567 132119  51694  59024  69349  73780\n[41]  94651 100680  69398  52798 140472 118623 180933  82798  83090  97356\n[51]  59482  77334  38777 111463  74715 174391 150558 122144  68012  84575\n[61] 143045  51394  98279  47671  26360 236917 220631 185290  64640  70046\n[71] 126971 144693 129404 284074 112268 203611 145238 251536 108078 238300\n[81] 108870 108085 262835 248182 244850 404456  67608  33860\n\n\nQuestion: Can you understand the meaning of Spatial lag as a sum of neighboring values now?\nAns: Yes, this provides a measure of spatial dependence, indicating how the GDPPC values in each region are related to the sum of GDPPC values in neighboring regions.\nNext, we will append the lag_sum GDPPC field into hunan sf data frame by using the code chunk below.\n\nhunan &lt;- left_join(hunan, lag.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\nNow, We can plot both the GDPPC and Spatial Lag Sum GDPPC for comparison using the code chunk below.\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_sum_gdppc &lt;- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n8.3 Spatial window average\nThe spatial window average uses row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\n\nNotice that the Number of nonzero links, Percentage nonzero weights and Average number of links are 536, 6.921488 and 6.090909 respectively as compared to wm_q of 448, 5.785124 and 5.090909\nLet us take a good look at the neighbour list of area [1] by using the code chunk below.\n\nwm_qs[[1]]\n\n[1]  1  2  3  4 57 85\n\n\nNotice that now [1] has six neighbours instead of five.\nNow we obtain weights with nb2listw()\n\nwm_qs &lt;- nb2listw(wm_qs)\nwm_qs\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\nLastly, we just need to create the lag variable from our weight structure and GDPPC variable.\n\nlag_w_avg_gpdpc &lt;- lag.listw(wm_qs, \n                             hunan$GDPPC)\nlag_w_avg_gpdpc\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\nlag.list.wm_qs &lt;- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nlag_wm_qs.res &lt;- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) &lt;- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\nNote: The third command line on the code chunk above renames the field names of lag_wm_q1.res object into NAME_3 and lag_window_avg GDPPC respectively.\nNext, the code chunk below will be used to append lag_window_avg GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, lag_wm_qs.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunan %&gt;%\n  select(\"County\", \n         \"lag GDPPC\", \n         \"lag_window_avg GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n43747.00\n47621.20\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n33582.71\n37160.12\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n45651.17\n49224.71\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n32027.62\n29886.89\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n32671.00\n26627.50\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n20810.00\n22690.17\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n25711.50\n25366.40\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n30672.33\n25825.75\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n33457.75\n30329.00\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n31689.20\n32682.83\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n20269.00\n25948.62\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n23901.60\n23987.67\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n25126.17\n25463.14\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n21903.43\n21904.38\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n22718.60\n23127.50\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n25918.80\n25949.83\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n20307.00\n20018.75\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n20023.80\n19524.17\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n16576.80\n18955.00\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n18667.00\n17800.40\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n14394.67\n15883.00\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n19848.80\n18831.33\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n15516.33\n14832.50\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518.00\n17965.00\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n17572.00\n17159.89\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n15200.12\n16199.44\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n18413.80\n18764.50\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n14419.33\n26878.75\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n24094.50\n23188.86\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n22019.83\n20788.14\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n12923.50\n12365.20\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n14756.00\n15985.00\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n13869.80\n13764.83\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n12296.67\n11907.43\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n15775.17\n17128.14\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n14382.86\n14593.62\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n11566.33\n11644.29\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n13199.50\n12706.00\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n23412.00\n21712.29\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n39541.00\n43548.25\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n36186.60\n35049.00\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n16559.60\n16226.83\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n20772.50\n19294.40\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n19471.20\n18156.00\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n19827.33\n19954.75\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n15466.80\n18145.17\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n12925.67\n12132.75\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n18577.17\n18419.29\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n14943.00\n14050.83\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n24913.00\n23619.75\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n25093.00\n24552.71\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n24428.80\n24733.67\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n17003.00\n16762.60\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n21143.75\n20932.60\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n20435.00\n19467.75\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n17131.33\n18334.00\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n24569.75\n22541.00\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n23835.50\n26028.00\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360.00\n29128.50\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n47383.40\n46569.00\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n55157.75\n47576.60\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n37058.00\n36545.50\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n21546.67\n20838.50\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n23348.67\n22531.00\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n42323.67\n42115.50\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n28938.60\n27619.00\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n25880.80\n27611.33\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n47345.67\n44523.29\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n18711.33\n18127.43\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n29087.29\n28746.38\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n20748.29\n20734.50\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n35933.71\n33880.62\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n15439.71\n14716.38\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n29787.50\n28516.22\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n18145.00\n18086.14\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n21617.00\n21244.50\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n29203.89\n29568.80\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n41363.67\n48119.71\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n22259.09\n22310.75\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n44939.56\n43151.60\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n16902.00\n17133.40\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n16930.00\n17009.33\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_gdppc and w_ave_gdppc maps next to each other for quick comparison.\n\nw_avg_gdppc &lt;- qtm(hunan, \"lag_window_avg GDPPC\")\ntmap_arrange(lag_gdppc, w_avg_gdppc, asp=1, ncol=2)\n\n\n\n\nNote: For more effective comparison, it is advicible to use the core tmap mapping functions.\n\n\n8.4 Spatial window sum\nThe spatial window sum is the counter part of the window average, but without using row-standardized weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\nwm_qs &lt;- include.self(wm_q)\nwm_qs\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNext, we will assign binary weights to the neighbour structure that includes the diagonal element.\n\nb_weights &lt;- lapply(wm_qs, function(x) 0*x + 1)\nb_weights[1]\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nNotice that now [1] has six neighbours instead of five.\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\n\nb_weights2 &lt;- nb2listw(wm_qs, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nWith our new weight structure, we can compute the lag variable with lag.listw().\n\nw_sum_gdppc &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nw_sum_gdppc\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) &lt;- c(\"NAME_3\", \"w_sum GDPPC\")\n\nNote: The second command line on the code chunk above renames the field names of w_sum_gdppc.res object into NAME_3 and w_sum GDPPC respectively.\nNext, the code chunk below will be used to append w_sum GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\nhunan &lt;- left_join(hunan, w_sum_gdppc.res)\n\nJoining with `by = join_by(NAME_3)`\n\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\nhunan %&gt;%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n174988\n238106\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n235079\n297281\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n273907\n344573\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n256221\n268982\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n98013\n106510\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n104050\n136141\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n102846\n126832\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n92017\n103303\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n133831\n151645\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n158446\n196097\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n141883\n207589\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n119508\n143926\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n150757\n178242\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n153324\n175235\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n113593\n138765\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n129594\n155699\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n142149\n160150\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n100119\n117145\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n82884\n113730\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n74668\n89002\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n43184\n63532\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n99244\n112988\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n46549\n59330\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518\n35930\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n140576\n154439\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n121601\n145795\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n92069\n112587\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n43258\n107515\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n144567\n162322\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n132119\n145517\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n51694\n61826\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n59024\n79925\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n69349\n82589\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n73780\n83352\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n94651\n119897\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n100680\n116749\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n69398\n81510\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n52798\n63530\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n140472\n151986\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n118623\n174193\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n180933\n210294\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n82798\n97361\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n83090\n96472\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n97356\n108936\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n59482\n79819\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n77334\n108871\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n38777\n48531\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n111463\n128935\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n74715\n84305\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n174391\n188958\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n150558\n171869\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n122144\n148402\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n68012\n83813\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n84575\n104663\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n143045\n155742\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n51394\n73336\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n98279\n112705\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n47671\n78084\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360\n58257\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n236917\n279414\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n220631\n237883\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n185290\n219273\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n64640\n83354\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n70046\n90124\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n126971\n168462\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n144693\n165714\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n129404\n165668\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n284074\n311663\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n112268\n126892\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n203611\n229971\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n145238\n165876\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n251536\n271045\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n108078\n117731\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n238300\n256646\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n108870\n126603\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n108085\n127467\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n262835\n295688\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n248182\n336838\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n244850\n267729\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n404456\n431516\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n67608\n85667\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n33860\n51028\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_sum GDPPC and w_sum_gdppc maps next to each other for quick comparison.\n\nw_sum_gdppc &lt;- qtm(hunan, \"w_sum GDPPC\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)\n\n\n\n\nNote: For more effective comparison, it is advicible to use the core tmap mapping functions."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex8/Hands-on_Ex8.html#references",
    "href": "Hands-on_Exercise/Hands-on_Ex8/Hands-on_Ex8.html#references",
    "title": "Hands-on_Ex8: Spatial Weights and Applications",
    "section": "9 References",
    "text": "9 References\n\nCreating Neighbours using sf objects"
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex1/data/geospatial/MPSZ-2019.html",
    "href": "In-class_Exercise/In-class_Ex1/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624 - Applied Geospatial Analytics",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex2/In-class_Ex2.html",
    "href": "In-class_Exercise/In-class_Ex2/In-class_Ex2.html",
    "title": "In-class_Ex2",
    "section": "",
    "text": "FiveR packages will be used for this in-class exercise, they are: sf, sfdep, tmap, tidyverse, and knitr.\n\npacman::p_load( sf, sfdep, tmap, tidyverse, knitr)"
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex2/In-class_Ex2.html#getting-started",
    "href": "In-class_Exercise/In-class_Ex2/In-class_Ex2.html#getting-started",
    "title": "In-class_Ex2",
    "section": "",
    "text": "FiveR packages will be used for this in-class exercise, they are: sf, sfdep, tmap, tidyverse, and knitr.\n\npacman::p_load( sf, sfdep, tmap, tidyverse, knitr)"
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex2/In-class_Ex2.html#the-data",
    "href": "In-class_Exercise/In-class_Ex2/In-class_Ex2.html#the-data",
    "title": "In-class_Ex2",
    "section": "2 The Data",
    "text": "2 The Data\nFor the purpose of this in-class exercise, the Hunan data sets will be used. There are two data sets in this use case, they are:\n\nHunan, a geospatial data set in ESRI shapefile format, and\nHunan_2012, an attribute data set in csv format."
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex2/In-class_Ex2.html#getting-data-in-r-environment",
    "href": "In-class_Exercise/In-class_Ex2/In-class_Ex2.html#getting-data-in-r-environment",
    "title": "In-class_Ex2",
    "section": "3 Getting Data in R Environment",
    "text": "3 Getting Data in R Environment\n\n3.1 Importing geospatial data\n\nhunan &lt;- st_read(dsn = \"data/geospatial\", layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `W:\\widyayutika\\ISSS624\\In-class_Exercise\\In-class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n3.2 Importing attribute table\n\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n3.3 Combining both data frame by using left join\n\nhunan_GDPPC &lt;- left_join(hunan,hunan2012)%&gt;%\n  select(1:4, 7, 15)\n\nIn order to retain the geospatial properties, the left data frame must be the sf data.frame(i.e. hunan)"
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex2/In-class_Ex2.html#deriving-continuity-spatial-weights",
    "href": "In-class_Exercise/In-class_Ex2/In-class_Ex2.html#deriving-continuity-spatial-weights",
    "title": "In-class_Ex2",
    "section": "4 Deriving Continuity Spatial Weights",
    "text": "4 Deriving Continuity Spatial Weights"
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex2/In-class_Ex2.html#deriving-continuity-spatial-weights-queens-method",
    "href": "In-class_Exercise/In-class_Ex2/In-class_Ex2.html#deriving-continuity-spatial-weights-queens-method",
    "title": "In-class_Ex2",
    "section": "5 Deriving Continuity Spatial Weights: Queen’s Method",
    "text": "5 Deriving Continuity Spatial Weights: Queen’s Method\nIn the code below, queen method is used to derive the contiguity weights.\n\nwm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb, style = \"W\"),\n         .before=1)\n\nNotes: ,before1 -&gt; put nb and wt at the front of the tibble dataset\n\n5.1 Computing local Moran’s I:\n\nlisa &lt;- wm_q %&gt;%\n  mutate(local_moran = local_moran(\n    GDPPC, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran)\n\n\npacman::p_load( sf, sfdep, tmap, tidyverse, knitr, plotly)\n\n\nGDPPC &lt;- read_csv(\"data/aspatial/Hunan_GDPPC.csv\")\n\n\n\n5.2 Creating a Time Series Cube\n\nGDPPC_st &lt;- spacetime(GDPPC, hunan, .loc_col =\"County\",\n                      .time_col =\"Year\")\n\nNote: spacetime is used to create a spacetime cube.\n\nis_spacetime_cube(GDPPC_st)\n\n[1] TRUE\n\n\n\n\n5.3 Identifying Neighbours and Derive Inverse Distance Weights\n\nGDPPC_nb &lt;- GDPPC_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb =include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb,geometry,\n                                  scale = 1,\n                                  alpha=1),\n         .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\nNotes:\n\nactivate() of dplyr package is used to activate the geometry context.\nmutate() of dplyr package is used to create two new columns nb and wt.\nThen, we will activate the data context again and copy over the b and wt columns to each time-slic using set_nbs () and set_wts()\n\n\n\n5.4 Computing Gi*\n\ngi_stars &lt;- GDPPC_nb %&gt;%\n  group_by(Year) %&gt;%\n  mutate(gi_star = local_gstar_perm(\n    GDPPC, nb, wt)) %&gt;%\n  tidyr::unnest(gi_star)\n\n\n\n5.5 Performing Emerging Hotspot Analysis\n\nehsa &lt;- emerging_hotspot_analysis(\n  x = GDPPC_st,\n  .var = \"GDPPC\",\n  k = 1,\n nsim = 99\n)\n\n\n\n5.6 Visualization\n\nhunan_ehsa &lt;-hunan %&gt;%\n  left_join(ehsa,\n            by= join_by(County==location))\nehsa_sig &lt;- hunan_ehsa %&gt;%\n  filter(p_value &lt;0.05)\ntmap_mode(\"plot\")\ntm_shape(hunan_ehsa) +\n  tm_polygons()+\n  tm_borders(alpha=0.5)+\ntm_shape(ehsa_sig)+\n  tm_fill('classification')+\n  tm_borders(alpha=0.4)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS624",
    "section": "",
    "text": "Welcome to ISSS624 Applied Geospatial Analytics!\nIn this webpage, I am going to share with you my learning journey of geospatial analytics."
  },
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#overview",
    "href": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#overview",
    "title": "Take-home_Ex1: Public Bus Passengers in Singapore",
    "section": "",
    "text": "The increasing digitization of urban infrastructures, including buses, taxis, mass rapid transit, public utilities and roads, has generated vast datasets capturing movement patterns over space and time. This data, facilitated by technologies such as GPS and RFID, offers valuable insights into human mobility within cities. Smart cards and GPS devices on public buses, for instance, have enabled the collection of routes and ridership data, providing a rich source for understanding urban movement.\nDespite the wealth of data collected, its utilization often remains limited to basic tracking and mapping using Geographic Information System (GIS) applications. This limitation is attributed to the inadequacy of conventional GIS functions in effectively analyzing and modeling spatial and spatio-temporal data.\nThe objectives of this study are centered around employing Exploratory Spatial Data Analysis (ESDA) techniques, specifically Local Indicators of Spatial Association (LISA) and Emerging Hot Spot Analysis (EHSA), to unveil the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore."
  },
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#section",
    "href": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#section",
    "title": "Take-home_Ex1: Public Bus Passengers in Singapore",
    "section": "12 ",
    "text": "12"
  },
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#geovisualisation-and-analysis",
    "href": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#geovisualisation-and-analysis",
    "title": "Take-home_Ex1: Public Bus Passengers in Singapore",
    "section": "9 Geovisualisation and Analysis",
    "text": "9 Geovisualisation and Analysis\nAfter the data preparation, I will first plot the distribution of passenger trips using ggplot() of tidyverse package. To consolidate these distributions into a single plot, it is necessary to introduce grouping variable to each dataframe. Within this unified plot, key summary statistics including Q1, median, Q3, and mean highligthed by a red circle will be presented. This approach aims to provide a comprehensive comparison of passenger trip characteristics across various peak hour groups.\n\n\nShow the code\n# Add a grouping variable to each dataframe\ntotal_trips_per_grid_wdmp$Peak_Hour_Group &lt;- \"wdmp\"\ntotal_trips_per_grid_wdap$Peak_Hour_Group &lt;- \"wdap\"\ntotal_trips_per_grid_wemp$Peak_Hour_Group &lt;- \"wemp\"\ntotal_trips_per_grid_weep$Peak_Hour_Group &lt;- \"weep\"\n\n# Combine dataframes into a single dataframe\ncombined_peak_hour &lt;- rbind(total_trips_per_grid_wdmp, total_trips_per_grid_wdap, total_trips_per_grid_wemp, total_trips_per_grid_weep) %&gt;%\n  filter(!is.na(total_trips))\n\n# Create a box plot using ggplot\nggplot(combined_peak_hour, aes(x = Peak_Hour_Group, y = total_trips)) +\n  geom_boxplot() +\n  stat_summary(fun = \"mean\", geom = \"point\", shape = 18, size = 3, color = \"red\")+\n  stat_boxplot(geom = 'errorbar', width = 0.5, color = 'blue', size = 1)+\n  stat_summary(\n    geom = 'text',\n    fun.min = function(x) quantile(x, 0.25),\n    fun = median,\n    fun.max = function(x) quantile(x, 0.75),\n    aes(label = sprintf(\"Q3: %.2f\\nMedian: %.2f\\nQ1: %.2f\", ..ymax.., ..y.., ..ymin..)),\n    vjust = -5,\n    hjust= -0.08,\n    position = position_dodge(width = 0.75),\n    size = 3\n  ) +\n  stat_summary(geom = 'text', fun = mean, aes(label = sprintf(\"Mean: %.2f\", ..y..)), vjust = -3, hjust=-0.08 ,  position = position_dodge(width = 0.9), color = 'red', size =3)+\n  \n  labs(title = \"Boxplot for Total Trips in each Peak Hour Group\", x=\"Peak Hour Group\",y = \"Passenger Trips\") +\n  theme_minimal()\n\n\n\n\n\nThe box plot analysis above reveal the patterns in Singapore’s bus passenger trips. Notably, the mean passenger trips during weekdays significantly surpass those on weekends and holidays, suggesting higher demand for bus services during typical workdays. This aligns with a common observation that commuting is more crowded on workdays, reflecting the daily hustle and bustle of the workforce.\nFurthermore, the observation that the mean of all peak hour groups exceeds their respective medians indicates a right-skewed distribution. This skewness implies that on average, there are more instances of relatively small number of passenger trips during peak hour with occasional instances of significantly higher demand. This distribution pattern underscores the challenges faced by commuters during peak hours, where a substantial portion of bus rides may experience higher congestion.\nSome hexagonal grid has a total of more than 400,000 passenger trips highlighting that specific areas with exceptionally high demand during the weekdays. These areas are likely represent key commuting regions with concentrated commercial and residential activities and are possibly areas that are not easily accessible by MRT. To gain clearer insights on the concentrations of high passenger trips I will leverage tmap package.\n\n9.1 Weekday Morning Peak\n\n\nShow the code\ntmap_mode(\"view\")\n\ntm_shape(total_trips_per_grid_wdmp) +\n  tm_fill(\n    col = \"total_trips\",\n    palette = c(\"#C5FFF8\", \"#FF4B91\"),\n    style = \"cont\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.6,\n    popup.vars = c(\"grid_id\",\"total_trips\"),\n    popup.format = list(\n      grid_id = list(format = \"f\", digits = 0),\n      total_trips = list(format = \"f\", digits = 0))\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)\n\n\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\nIt has been observed that certain bus stops are located outside the boundaries of Singapore, particularly in Johor.The influx of weekday morning peak passenger trips from Johor are qhigh surpassing 100,000 for October 2023. This significant amount shows the cross border commuting activity between Johor and Singapore suggesting a preference among some individuals to reside in Johor may be due to cost consideration while working in Singapore.\nThe Central Business District (CBD) area displays a comparatively lower passenger trips generated by origin bus stop. This is attributed to the absence of major bus interchanges within the CBD, suggesting that commuters in this central business hub may rely on alternative modes of transportation, such as the Mass Rapid Transit (MRT) system. In addition, it is worth noting that lower passenger trips may also be influences by the fact that bus routes typically do not commence within CBD area, even though the bus routes pass through CBD.\nAreas that are exhibiting a high concentration during weekday morning peak are associated with prominent bus interchanges such as Woodlands and Boon Lay with more than 300,000 passenger trips within a single month on weekdays. Additionally, other bus interchanges including Bishan, Ang Mo Kio, Toa Payoh, Clementi, Punggol, Tampines, and Bedok also shows high passenger trips.\n\n\n9.2 Weekday Afternoon Peak\n\n\nShow the code\ntmap_mode(\"plot\")\n\ntm_shape(total_trips_per_grid_wdap) +\n  tm_fill(\n    col = \"total_trips\",\n    palette = c(\"#C5FFF8\", \"#FF4B91\"),\n    style = \"cont\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.6,\n    popup.vars = c(\"grid_id\",\"total_trips\"),\n    popup.format = list(\n      grid_id = list(format = \"f\", digits = 0),\n      total_trips = list(format = \"f\", digits = 0))\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\nDuring the weekday afternoon peak, Woodlands and Boon Lay continue to exhibit an impressive volume of passenger trips exceeding 400,000 as compared to the patterns observed during morning peak. In addition, there are additional areas such as Ang Mo Kio, Tampines, and Bedok which emerge as significant contributors to the passenger trips during the later peak period.\nTypically the passengers volume during weekday afternoon are higher than weekday morning with Boon Lay with the highest contributor of more than 500,000 passenger trips. Boon Lay serves as a transportation hub for workers, residents and students from NTU. Moreover, the areas near Boon Lay are currently not accessible through MRT as the development of MRT is still in progress.\n\n\n9.3 Weekends/Holiday Morning Peak\n\n\nShow the code\ntmap_mode(\"plot\")\ntm_shape(total_trips_per_grid_wemp) +\n  tm_fill(\n    col = \"total_trips\",\n    palette = c(\"#C5FFF8\", \"#FF4B91\"),\n    style = \"cont\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.6,\n    popup.vars = c(\"grid_id\",\"total_trips\"),\n    popup.format = list(\n      grid_id = list(format = \"f\", digits = 0),\n      total_trips = list(format = \"f\", digits = 0))\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\nGiven that weekends consist of only two days, the passenger trip numbers exhibit lower in total numbers but comparable to those recorded during five weekdays, following a similar pattern. Notably, major bus interchanges, such as Woodlands, Boon Lay, Bedok and Tampines continue to play a pivotal role contributing significantly to the overall passenger trips during both weekdays and weekends.\nDuring weekend and holidays, the morning peak hour (11am to 2pm) is later as compared to weekday morning (6 to 9am). This temporal shift can be attributed to social dynamic where individuals engaging in lunchtime activity with family and friends during weekends. This distinctive pattern underscores the influence of social interactions on commuter behaviors during non-working days.\n\n\n9.4 Weekends/Holiday Evening Peak\n\n\nShow the code\ntmap_mode(\"plot\")\ntm_shape(total_trips_per_grid_weep) +\n  tm_fill(\n    col = \"total_trips\",\n    palette = c(\"#C5FFF8\", \"#FF4B91\"),\n    style = \"cont\",\n    title = \"Number of Trips\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.6,\n    popup.vars = c(\"grid_id\",\"total_trips\"),\n    popup.format = list(\n      grid_id = list(format = \"f\", digits = 0),\n      total_trips = list(format = \"f\", digits = 0))\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)\n\n\n\n\n\nShow the code\ntmap_mode(\"plot\")\n\n\nSimilar with the other peak hours, the passenger trips are mostly contributed by major bus interchange as the origin.\nTypically , the afternoon/evening peak hours witnesses a higher volume of passenger trips both on weekdays and weekend as compared to morning peak hours. This preference may be due to individuals opting for MRT during hours as it provide punctual mode of transportation as compared to bus which might be affected by congestion and could potentially lead to delays in reaching to work/schools/leisure activities.\nDuring weekend and holidays, the evening peak hour (4 to 7pm) is slightly earlier as compared to weekday morning afternoon (5 to 8pm). This shift in timing could be attributed to the altered schedules and leisurely activities that individuals typically engage in during weekends and holidays. People might be more inclined to initiate their evening commutes earlier to accommodate social plans, family gatherings, or recreational activities that are common during non-working days."
  },
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#distribution-of-passenger-trips-generate-by-origin-at-hexagon-level",
    "href": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#distribution-of-passenger-trips-generate-by-origin-at-hexagon-level",
    "title": "Take-home_Ex1: Public Bus Passengers in Singapore",
    "section": "6 Distribution of passenger trips generate by origin at hexagon level",
    "text": "6 Distribution of passenger trips generate by origin at hexagon level\n\n6.1 Defining the Neighbourhood\n\n6.1.1 Contiguity Weight Matrix (QUEEN)\n\nwm_q &lt;- poly2nb(total_trips_per_grid_wdmp, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 1492 \nNumber of nonzero links: 6714 \nPercentage nonzero weights: 0.3016086 \nAverage number of links: 4.5 \n12 regions with no links:\n276 296 454 550 713 964 1030 1387 1477 1480 1484 1492\nLink number distribution:\n\n  0   1   2   3   4   5   6 \n 12  40 105 206 285 358 486 \n40 least connected regions:\n1 7 22 38 98 166 183 184 185 191 207 214 253 257 260 551 595 629 683 695 719 738 755 771 855 990 1004 1005 1029 1069 1194 1436 1443 1454 1472 1473 1475 1478 1482 1491 with 1 link\n486 most connected regions:\n10 13 16 17 24 25 31 35 42 43 48 53 55 60 63 67 73 77 80 81 84 85 87 88 91 92 97 102 107 111 117 121 127 132 138 139 141 145 146 147 151 152 153 154 160 161 162 170 171 172 179 180 181 187 188 189 190 196 197 198 201 202 203 204 212 225 235 239 240 242 252 268 272 280 287 289 290 291 293 299 300 302 303 306 311 314 315 317 327 328 329 330 333 342 345 353 358 380 381 390 392 393 397 404 408 413 415 421 426 427 428 430 433 440 441 442 450 456 457 458 459 463 467 470 471 478 482 483 485 491 492 496 502 503 506 507 512 518 523 528 532 537 538 539 541 545 547 553 557 562 563 565 566 570 579 580 583 587 588 592 593 597 603 607 611 613 620 623 624 625 635 636 637 641 642 644 645 646 656 657 658 664 667 668 669 674 675 677 678 687 688 691 692 693 700 703 704 711 714 715 716 727 728 742 744 745 747 758 761 762 763 764 769 770 774 775 776 779 780 781 782 786 787 792 793 796 797 798 799 805 806 809 810 811 816 817 818 827 829 830 832 833 834 836 837 838 839 840 846 849 851 852 853 857 858 862 863 864 866 867 868 870 871 874 877 879 882 885 888 890 891 895 899 904 906 911 912 913 915 916 920 928 929 930 931 932 933 938 942 943 946 947 952 953 955 956 957 961 967 968 969 970 971 973 979 980 981 982 987 994 995 996 997 1007 1008 1009 1011 1012 1019 1020 1021 1025 1033 1034 1037 1039 1040 1045 1046 1047 1049 1050 1051 1052 1059 1061 1062 1063 1066 1072 1076 1083 1084 1085 1088 1089 1093 1094 1100 1103 1104 1105 1111 1116 1117 1118 1119 1124 1125 1127 1128 1129 1130 1131 1133 1139 1140 1141 1145 1146 1147 1149 1151 1152 1153 1154 1158 1159 1160 1161 1162 1166 1168 1171 1172 1173 1174 1175 1181 1182 1183 1184 1185 1186 1187 1190 1191 1197 1198 1199 1200 1201 1207 1213 1214 1215 1219 1224 1225 1231 1233 1234 1235 1240 1244 1245 1250 1251 1252 1256 1259 1261 1267 1276 1279 1280 1293 1298 1299 1300 1301 1303 1304 1305 1308 1309 1311 1317 1326 1328 1329 1337 1338 1340 1343 1344 1349 1352 1353 1354 1356 1360 1363 1365 1367 1370 1378 1380 1384 1389 1390 1392 1396 1397 1398 1399 1400 1405 1406 1407 1408 1412 1413 1414 1418 1419 1421 1423 1425 1428 1429 1431 1432 1433 1434 1441 with 6 links\n\n\nThere are 10 hexagonal grid with 0 neighbours and the most connected grid has 6 links.\n\n\n6.1.2 Contiguity Weight Matrix (ROOK)\n\nwm_r &lt;- poly2nb(total_trips_per_grid_wdmp, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 1492 \nNumber of nonzero links: 6714 \nPercentage nonzero weights: 0.3016086 \nAverage number of links: 4.5 \n12 regions with no links:\n276 296 454 550 713 964 1030 1387 1477 1480 1484 1492\nLink number distribution:\n\n  0   1   2   3   4   5   6 \n 12  40 105 206 285 358 486 \n40 least connected regions:\n1 7 22 38 98 166 183 184 185 191 207 214 253 257 260 551 595 629 683 695 719 738 755 771 855 990 1004 1005 1029 1069 1194 1436 1443 1454 1472 1473 1475 1478 1482 1491 with 1 link\n486 most connected regions:\n10 13 16 17 24 25 31 35 42 43 48 53 55 60 63 67 73 77 80 81 84 85 87 88 91 92 97 102 107 111 117 121 127 132 138 139 141 145 146 147 151 152 153 154 160 161 162 170 171 172 179 180 181 187 188 189 190 196 197 198 201 202 203 204 212 225 235 239 240 242 252 268 272 280 287 289 290 291 293 299 300 302 303 306 311 314 315 317 327 328 329 330 333 342 345 353 358 380 381 390 392 393 397 404 408 413 415 421 426 427 428 430 433 440 441 442 450 456 457 458 459 463 467 470 471 478 482 483 485 491 492 496 502 503 506 507 512 518 523 528 532 537 538 539 541 545 547 553 557 562 563 565 566 570 579 580 583 587 588 592 593 597 603 607 611 613 620 623 624 625 635 636 637 641 642 644 645 646 656 657 658 664 667 668 669 674 675 677 678 687 688 691 692 693 700 703 704 711 714 715 716 727 728 742 744 745 747 758 761 762 763 764 769 770 774 775 776 779 780 781 782 786 787 792 793 796 797 798 799 805 806 809 810 811 816 817 818 827 829 830 832 833 834 836 837 838 839 840 846 849 851 852 853 857 858 862 863 864 866 867 868 870 871 874 877 879 882 885 888 890 891 895 899 904 906 911 912 913 915 916 920 928 929 930 931 932 933 938 942 943 946 947 952 953 955 956 957 961 967 968 969 970 971 973 979 980 981 982 987 994 995 996 997 1007 1008 1009 1011 1012 1019 1020 1021 1025 1033 1034 1037 1039 1040 1045 1046 1047 1049 1050 1051 1052 1059 1061 1062 1063 1066 1072 1076 1083 1084 1085 1088 1089 1093 1094 1100 1103 1104 1105 1111 1116 1117 1118 1119 1124 1125 1127 1128 1129 1130 1131 1133 1139 1140 1141 1145 1146 1147 1149 1151 1152 1153 1154 1158 1159 1160 1161 1162 1166 1168 1171 1172 1173 1174 1175 1181 1182 1183 1184 1185 1186 1187 1190 1191 1197 1198 1199 1200 1201 1207 1213 1214 1215 1219 1224 1225 1231 1233 1234 1235 1240 1244 1245 1250 1251 1252 1256 1259 1261 1267 1276 1279 1280 1293 1298 1299 1300 1301 1303 1304 1305 1308 1309 1311 1317 1326 1328 1329 1337 1338 1340 1343 1344 1349 1352 1353 1354 1356 1360 1363 1365 1367 1370 1378 1380 1384 1389 1390 1392 1396 1397 1398 1399 1400 1405 1406 1407 1408 1412 1413 1414 1418 1419 1421 1423 1425 1428 1429 1431 1432 1433 1434 1441 with 6 links\n\n\nThere are 10 hexagonal grid with 0 neighbours and the most connected grid has 6 links. So we should use distance-based neighbours.\n\n\n\n6.2 Visualising contiguity weights\n\ncentroid &lt;- st_centroid(total_trips_per_grid_wdmp$area_honeycomb_grid)\ncentroid\n\nGeometry set for 1492 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48470.12 ymax: 52895.88\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\n\n\nlongitude &lt;- map_dbl(total_trips_per_grid_wdmp$area_honeycomb_grid, ~st_centroid(.x)[[1]])  \nlatitude &lt;- map_dbl(total_trips_per_grid_wdmp$area_honeycomb_grid, ~st_centroid(.x)[[2]])  \ncoords &lt;- cbind(longitude, latitude) \ntail(coords)\n\n        longitude latitude\n[1487,]  46470.12 40338.51\n[1488,]  46470.12 41204.53\n[1489,]  46720.12 39905.49\n[1490,]  46720.12 40771.52\n[1491,]  46970.12 39472.48\n[1492,]  48470.12 33410.30\n\n\n\nsummary(coords)\n\n   longitude        latitude    \n Min.   : 3970   Min.   :26482  \n 1st Qu.:19220   1st Qu.:32977  \n Median :27470   Median :36008  \n Mean   :26321   Mean   :36939  \n 3rd Qu.:33720   3rd Qu.:40014  \n Max.   :48470   Max.   :52896  \n\n\n\n6.2.1 Plotting Queen and Rook Contiguity Based on Neighbours Map\n\npar(mfrow=c(1,2))\nplot(total_trips_per_grid_wdmp$area_honeycomb_grid, border=\"lightgrey\", main=\"Queen Contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\nplot(total_trips_per_grid_wdmp$area_honeycomb_grid, border=\"lightgrey\", main=\"Rook Contiguity\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\n\n\n6.3 Computing distance based neighbours\n\n\n6.4 Determine the cut-off distance\n\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    319    5404    9647    8922   12016   16223 \n\n\n\n\n6.5 Computing fixed distance weight matrix\n\nwm_d500 &lt;- dnearneigh(coords, 0, 500, longlat = TRUE)\nwm_d500\n\nNeighbour list object:\nNumber of regions: 1492 \nNumber of nonzero links: 7528 \nPercentage nonzero weights: 0.3381754 \nAverage number of links: 5.045576 \n92 regions with no links:\n1 4 12 23 39 84 183 189 327 353 425 480 489 525 535 546 556 600 611 612\n613 622 625 635 636 637 643 645 646 655 656 657 658 667 670 676 677 678\n679 690 691 702 703 704 705 713 714 726 727 728 729 743 746 761 762 763\n778 780 796 797 798 804 805 815 818 826 836 837 838 846 856 858 859 873\n874 875 891 905 906 936 952 964 965 978 992 1005 1126 1166 1171 1195\n1428 1458\n\n\n\n6.5.1 Adaptive Distance Weight Matrix\n\nknn6 &lt;- knn2nb(knearneigh(coords, k=6))\nknn6\n\nNeighbour list object:\nNumber of regions: 1492 \nNumber of nonzero links: 8952 \nPercentage nonzero weights: 0.4021448 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\n\n\n6.5.2 Plotting distance based neighbours\n\nplot(total_trips_per_grid_wdmp$area_honeycomb_grid, border=\"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\n\n6.5.3 Inversed Distance"
  },
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#task-2-local-indicators-of-spatial-association-lisa-analysis",
    "href": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#task-2-local-indicators-of-spatial-association-lisa-analysis",
    "title": "Take-home_Ex1: Public Bus Passengers in Singapore",
    "section": "12 Task 2: Local Indicators of Spatial Association (LISA) Analysis",
    "text": "12 Task 2: Local Indicators of Spatial Association (LISA) Analysis\n\n12.1 Deriving Continuity Spatial Weights: Queen’s Method\nwm_q &lt;- hunan_GDPPC %&gt;% mutate(nb = st_contiguity(geometry), wt = st_weights(nb, style = “W”), .before=1)\nLocal Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable.\nIn this section, you will learn how to apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran’I to detect cluster and/or outlier from passenger trips generate by origin at hexagon level.\nThe code chunks below are used to compute local Moran’s I of Total Trips at the hexagonal grid level.\nfips &lt;- order(total_trips_per_grid_wdmp$grid_id) localMI &lt;- localmoran(hunan$GDPPC, rswm_q) head(localMI)\nReproducible:\njust change the read_csv and the file"
  },
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1_dump.html",
    "href": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1_dump.html",
    "title": "Take-home_Ex1_dump",
    "section": "",
    "text": "This section provides a comprehensive step-by step walkthrough to calculate the number of trips within each hexagonal grid during Weekday Morning Peak Hour with a subsequent plan to replicate the same process for Weekday Afternoon Peak Hour, Weekends/Holiday Morning Peak, and Weekends/Holiday Evening Peak in the subsequent section\n\n\nThe code chunk below will be used to calculate the the number of trips in each origin bus stop.\n{r} odbus_weekday_6_9_ori &lt;- odbus_weekday_6_9 %&gt;%   group_by(ORIGIN_PT_CODE) %&gt;%   summarise(WEEKDAY_MORNING_ORI = sum(TRIPS))%&gt;%    mutate(ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE))}\nThe code chunk below will be used to calculate the the number of trips in each destination bus stop.\n{r} odbus_weekday_6_9_des &lt;- odbus_weekday_6_9 %&gt;%   group_by(DESTINATION_PT_CODE) %&gt;%   summarise(WEEKDAY_MORNING_DES = sum(TRIPS))%&gt;%    mutate(DESTINATION_PT_CODE = as.factor(DESTINATION_PT_CODE))}\nThe code chunk below will be used to combine both origin and destination trips in each bus stop and rename the ORIGIN_PT_CODE to BUS_STOP_ID.\n{r} odbus_weekday_6_9_data &lt;- full_join(odbus_weekday_6_9_ori , odbus_weekday_6_9_des,                                     by =c(\"ORIGIN_PT_CODE\"=\"DESTINATION_PT_CODE\"))%&gt;%   rename(BUS_STOP_ID = ORIGIN_PT_CODE)}\nNext, I will calculate the total trips from both origin and destination trips by summing both the columns.\n{r} odbus_weekday_6_9_data$WEEKDAY_MORNING_TRIPS &lt;- with(odbus_weekday_6_9_data,    ifelse(is.na(WEEKDAY_MORNING_ORI), 0, WEEKDAY_MORNING_ORI) +   ifelse(is.na(WEEKDAY_MORNING_DES), 0, WEEKDAY_MORNING_DES) )}\nTaking a glimpse of the odbus_weekday_6_9_data to make sure that the code chunk above are correct.\n{r} glimpse(odbus_weekday_6_9_data)}\n\n\n\nThe code chunk below will be used to join the busstop3414 SpatialPolygonsDataframe and odbus_weekday_6_9_data by BUS_STOP_N for busstop3414 and BUS_STOP_ID for original_destination_bus). This is performed by using left_join() of dplyr package.\n{r} total_trips_per_busstop_wdmp &lt;- left_join(busstop3414, odbus_weekday_6_9_data, by = c(\"BUS_STOP_N\" = \"BUS_STOP_ID\"))}\n\n\n\nThe code chunk below will be used to join the total_trips_per_busstop and honeycomb grid spatially and remove the grid without any busstop (remove rows with NA value on the BUS_STOP_N).\n{r} total_trips_per_busstop_honeycomb_wdmp &lt;- st_join(honeycomb_grid_sf,total_trips_per_busstop_wdmp) %&gt;%   filter(!is.na(BUS_STOP_N))}\n\n\n\nThe code chunk below will be used to calculate the total trips in a hexagonal grid.\n{r} total_trips_per_grid_wdmp &lt;- total_trips_per_busstop_honeycomb_wdmp %&gt;%   group_by(grid_id) %&gt;%   summarise(total_trips = sum(WEEKDAY_MORNING_TRIPS))}"
  },
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1_dump.html#task-1-geovisualisation-and-analysis",
    "href": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1_dump.html#task-1-geovisualisation-and-analysis",
    "title": "Take-home_Ex1_dump",
    "section": "1 Task 1: Geovisualisation and Analysis",
    "text": "1 Task 1: Geovisualisation and Analysis\n\n1.1 Weekday Afternoon Peak\n{r} tmap_mode(\"view\")  map_honeycomb = tm_shape(total_trips_per_grid_wdmp) +   tm_fill(     col = \"total_trips\",     palette = \"Reds\",     style = \"cont\",     title = \"Number of Trips\",     id = \"grid_id\",     showNA = FALSE,     alpha = 0.6,     popup.vars = c(\"total_trips\"),     popup.format = list(       total_trips = list(format = \"f\", digits = 0))   ) +   tm_borders(col = \"grey40\", lwd = 0.7)  map_honeycomb}\n\n\n1.2 Weekday Afternoon Peak\n{r} tmap_mode(\"view\")  map_honeycomb = tm_shape(total_trips_per_grid_wdap) +   tm_fill(     col = \"total_trips\",     palette = \"Reds\",     style = \"cont\",     title = \"Number of Trips\",     id = \"grid_id\",     showNA = FALSE,     alpha = 0.6,     popup.vars = c(\"total_trips\"),     popup.format = list(       total_trips = list(format = \"f\", digits = 0))   ) +   tm_borders(col = \"grey40\", lwd = 0.7)  map_honeycomb}\n\n\n1.3 Weekends/Holiday Morning Peak\n{r} map_honeycomb = tm_shape(total_trips_per_grid_wemp) +   tm_fill(     col = \"total_trips\",     palette = \"Reds\",     style = \"cont\",     title = \"Number of Trips\",     id = \"grid_id\",     showNA = FALSE,     alpha = 0.6,     popup.vars = c(\"total_trips\"),     popup.format = list(       total_trips = list(format = \"f\", digits = 0))   ) +   tm_borders(col = \"grey40\", lwd = 0.7)  map_honeycomb}\n\n\n1.4 Weekends/Holiday Evening Peak\n{r} map_honeycomb = tm_shape(total_trips_per_grid_weep) +   tm_fill(     col = \"total_trips\",     palette = \"Reds\",     style = \"cont\",     title = \"Number of Trips\",     id = \"grid_id\",     showNA = FALSE,     alpha = 0.6,     popup.vars = c(\"total_trips\"),     popup.format = list(       total_trips = list(format = \"f\", digits = 0))   ) +   tm_borders(col = \"grey40\", lwd = 0.7)  map_honeycomb}\nFrom the above, we can see there are more number of trip in the morning..??????"
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex3/In-class_Ex3.html",
    "href": "In-class_Exercise/In-class_Ex3/In-class_Ex3.html",
    "title": "In-class_Ex3:Calibrating Spatial Interaction Models with R",
    "section": "",
    "text": "Spatial Interaction Models (SIMs) are mathematical models for estimating flows between spatial entities developed by Alan Wilson in the late 1960s and early 1970, with considerable uptake and refinement for transport modelling since then Boyce and Williams (2015).\nThere are four main types of traditional SIMs (Wilson 1971):\n\nUnconstrained\nProduction-constrained\nAttraction-constrained\nDoubly-constrained\n\nOrdinary least square (OLS), log-normal, Poisson and negative binomial (NB) regression methods have been used extensively to calibrate OD flow models by processing flow data as different types of dependent variables. In this chapter, you will gain hands-on experiences on using appropriate R packages to calibrate SIM by using there four regression methods."
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex3/In-class_Ex3.html#overview",
    "href": "In-class_Exercise/In-class_Ex3/In-class_Ex3.html#overview",
    "title": "In-class_Ex3:Calibrating Spatial Interaction Models with R",
    "section": "",
    "text": "Spatial Interaction Models (SIMs) are mathematical models for estimating flows between spatial entities developed by Alan Wilson in the late 1960s and early 1970, with considerable uptake and refinement for transport modelling since then Boyce and Williams (2015).\nThere are four main types of traditional SIMs (Wilson 1971):\n\nUnconstrained\nProduction-constrained\nAttraction-constrained\nDoubly-constrained\n\nOrdinary least square (OLS), log-normal, Poisson and negative binomial (NB) regression methods have been used extensively to calibrate OD flow models by processing flow data as different types of dependent variables. In this chapter, you will gain hands-on experiences on using appropriate R packages to calibrate SIM by using there four regression methods."
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex3/In-class_Ex3.html#the-case-study-and-data",
    "href": "In-class_Exercise/In-class_Ex3/In-class_Ex3.html#the-case-study-and-data",
    "title": "In-class_Ex3:Calibrating Spatial Interaction Models with R",
    "section": "2 The Case Study and Data",
    "text": "2 The Case Study and Data\nIn this exercise, we are going to calibrate SIM to determine factors affecting the public bus passenger flows during the morning peak in Singapore."
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex3/In-class_Ex3.html#getting-started",
    "href": "In-class_Exercise/In-class_Ex3/In-class_Ex3.html#getting-started",
    "title": "In-class_Ex3:Calibrating Spatial Interaction Models with R",
    "section": "3 Getting Started",
    "text": "3 Getting Started\nFor the purpose of this exercise, four r packages will be used. They are:\n\nsf for importing, integrating, processing and transforming geospatial data.\ntidyverse for importing, integrating, wrangling and visualising data.\ntmap for creating thematic maps.\nDT: for dynamic table\nsp: for spatial data handling\nperformance: for creating performance objects\nreshape2: for reshaping data\nggpubr: for creating multiple plot into 1\n\n\npacman::p_load(tmap, sf, sp, DT,\n               performance, reshape2,\n               ggpubr, tidyverse)"
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex3/In-class_Ex3.html#the-data",
    "href": "In-class_Exercise/In-class_Ex3/In-class_Ex3.html#the-data",
    "title": "In-class_Ex3:Calibrating Spatial Interaction Models with R",
    "section": "4 The Data",
    "text": "4 The Data\nThis exercise is a continuation of Chapter 15: Processing and Visualising Flow Data and the following data will be used:\n\nod_data.rds, weekday morning peak passenger flows at planning subzone level.\nmpsz.rds, URA Master Plan 2019 Planning Subzone boundary in simple feature tibble data frame format.\n\nBeside these two data sets, an additional attribute data file called pop.csv will be provided."
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex3/In-class_Ex3.html#computing-distance-matrix",
    "href": "In-class_Exercise/In-class_Ex3/In-class_Ex3.html#computing-distance-matrix",
    "title": "In-class_Ex3:Calibrating Spatial Interaction Models with R",
    "section": "5 Computing Distance Matrix",
    "text": "5 Computing Distance Matrix\nIn spatial interaction, a distance matrix is a table that shows the distance between pairs of locations. For example, in the table below we can see an Euclidean distance of 3926.0025 between MESZ01 and RVSZ05, of 3939.1079 between MESZ01 and SRSZ01, and so on. By definition, an location’s distance from itself, which is shown in the main diagonal of the table, is 0.\nIn this section, you will learn how to compute a distance matrix by using URA Master Plan 2019 Planning Subzone boundary in which you saved as an rds file called mpsz.\nFirst, let us import mpsz.rds into R environemnt by using the code chunk below.\n\nmpsz &lt;- read_rds(\"data/rds/mpsz.rds\")\nmpsz\n\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                 SUBZONE_N SUBZONE_C       PLN_AREA_N PLN_AREA_C       REGION_N\n1              MARINA EAST    MESZ01      MARINA EAST         ME CENTRAL REGION\n2         INSTITUTION HILL    RVSZ05     RIVER VALLEY         RV CENTRAL REGION\n3           ROBERTSON QUAY    SRSZ01  SINGAPORE RIVER         SR CENTRAL REGION\n4  JURONG ISLAND AND BUKOM    WISZ01  WESTERN ISLANDS         WI    WEST REGION\n5             FORT CANNING    MUSZ02           MUSEUM         MU CENTRAL REGION\n6         MARINA EAST (MP)    MPSZ05    MARINE PARADE         MP CENTRAL REGION\n7                   SUDONG    WISZ03  WESTERN ISLANDS         WI    WEST REGION\n8                  SEMAKAU    WISZ02  WESTERN ISLANDS         WI    WEST REGION\n9           SOUTHERN GROUP    SISZ02 SOUTHERN ISLANDS         SI CENTRAL REGION\n10                 SENTOSA    SISZ01 SOUTHERN ISLANDS         SI CENTRAL REGION\n   REGION_C                       geometry\n1        CR MULTIPOLYGON (((33222.98 29...\n2        CR MULTIPOLYGON (((28481.45 30...\n3        CR MULTIPOLYGON (((28087.34 30...\n4        WR MULTIPOLYGON (((14557.7 304...\n5        CR MULTIPOLYGON (((29542.53 31...\n6        CR MULTIPOLYGON (((35279.55 30...\n7        WR MULTIPOLYGON (((15772.59 21...\n8        WR MULTIPOLYGON (((19843.41 21...\n9        CR MULTIPOLYGON (((30870.53 22...\n10       CR MULTIPOLYGON (((26879.04 26...\n\n\nNotice that it is a sf tibble dataframe object class.\n\n5.1 Converting from sf data.table to SpatialPolygonsDataFrame\nThere are at least two ways to compute the required distance matrix. One is based on sf and the other is based on sp. Past experience shown that computing distance matrix by using sf function took relatively longer time that sp method especially the data set is large. In view of this, sp method is used in the code chunks below.\nFirst as.Spatial() will be used to convert mpsz from sf tibble data frame to SpatialPolygonsDataFrame of sp object as shown in the code chunk below.\n\nmpsz_sp &lt;- as(mpsz, \"Spatial\")\nmpsz_sp\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 332 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 6\nnames       : SUBZONE_N, SUBZONE_C, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C \nmin values  : ADMIRALTY,    AMSZ01, ANG MO KIO,         AM, CENTRAL REGION,       CR \nmax values  :    YUNNAN,    YSSZ09,     YISHUN,         YS,    WEST REGION,       WR \n\n\n\n#mpsz_sp_selected &lt;- mpsz_sp %&gt;%\n#  select(mpsz@data$SUBZONE)\n\n\n\n5.2 Computing the distance matrix\nNext, spDists() of sp package will be used to compute the Euclidean distance between the centroids of the planning subzones.\n\n\n\n\n\n\nQ&A\n\n\n\nDo you know why the distance is calculated between two centroids of a pair of spatial polygons?\n\n\n\ndist &lt;- spDists(mpsz_sp, \n                longlat = FALSE)\nhead(dist, n=c(10, 10))\n\n           [,1]       [,2]      [,3]      [,4]       [,5]      [,6]      [,7]\n [1,]     0.000  3926.0025  3939.108 20252.964  2989.9839  1431.330 19211.836\n [2,]  3926.003     0.0000   305.737 16513.865   951.8314  5254.066 16242.523\n [3,]  3939.108   305.7370     0.000 16412.062  1045.9088  5299.849 16026.146\n [4,] 20252.964 16513.8648 16412.062     0.000 17450.3044 21665.795  7229.017\n [5,]  2989.984   951.8314  1045.909 17450.304     0.0000  4303.232 17020.916\n [6,]  1431.330  5254.0664  5299.849 21665.795  4303.2323     0.000 20617.082\n [7,] 19211.836 16242.5230 16026.146  7229.017 17020.9161 20617.082     0.000\n [8,] 14960.942 12749.4101 12477.871 11284.279 13336.0421 16281.453  5606.082\n [9,]  7515.256  7934.8082  7649.776 18427.503  7801.6163  8403.896 14810.930\n[10,]  6391.342  4975.0021  4669.295 15469.566  5226.8731  7707.091 13111.391\n           [,8]      [,9]     [,10]\n [1,] 14960.942  7515.256  6391.342\n [2,] 12749.410  7934.808  4975.002\n [3,] 12477.871  7649.776  4669.295\n [4,] 11284.279 18427.503 15469.566\n [5,] 13336.042  7801.616  5226.873\n [6,] 16281.453  8403.896  7707.091\n [7,]  5606.082 14810.930 13111.391\n [8,]     0.000  9472.024  8575.490\n [9,]  9472.024     0.000  3780.800\n[10,]  8575.490  3780.800     0.000\n\n\nNotice that the output dist is a matrix object class of R. Also notice that the column heanders and row headers are not labeled with the planning subzone codes.\n\n\n5.3 Labelling column and row heanders of a distance matrix\nFirst, we will create a list sorted according to the the distance matrix by planning sub-zone code.\n\nsz_names &lt;- mpsz$SUBZONE_C\n\nNext we will attach SUBZONE_C to row and column for distance matrix matching ahead\n\ncolnames(dist) &lt;- paste0(sz_names)\nrownames(dist) &lt;- paste0(sz_names)\n\n\n\n5.4 Pivoting distance value by SUBZONE_C\nNext, we will pivot the distance matrix into a long table by using the row and column subzone codes as show in the code chunk below.\n\ndistPair &lt;- melt(dist) %&gt;%\n  rename(dist = value)\nhead(distPair, 10)\n\n     Var1   Var2      dist\n1  MESZ01 MESZ01     0.000\n2  RVSZ05 MESZ01  3926.003\n3  SRSZ01 MESZ01  3939.108\n4  WISZ01 MESZ01 20252.964\n5  MUSZ02 MESZ01  2989.984\n6  MPSZ05 MESZ01  1431.330\n7  WISZ03 MESZ01 19211.836\n8  WISZ02 MESZ01 14960.942\n9  SISZ02 MESZ01  7515.256\n10 SISZ01 MESZ01  6391.342\n\n\n\n\n5.5 Updating intra-zonal distances\nIn this section, we are going to append a constant value to replace the intra-zonal distance of 0.\nFirst, we will select and find out the minimum value of the distance by using summary().\n\ndistPair %&gt;%\n  filter(dist &gt; 0) %&gt;%\n  summary()\n\n      Var1             Var2             dist        \n MESZ01 :   331   MESZ01 :   331   Min.   :  173.8  \n RVSZ05 :   331   RVSZ05 :   331   1st Qu.: 7149.5  \n SRSZ01 :   331   SRSZ01 :   331   Median :11890.0  \n WISZ01 :   331   WISZ01 :   331   Mean   :12229.4  \n MUSZ02 :   331   MUSZ02 :   331   3rd Qu.:16401.7  \n MPSZ05 :   331   MPSZ05 :   331   Max.   :49894.4  \n (Other):107906   (Other):107906                    \n\n\nNext, a constant distance value of 50m is added into intra-zones distance.\n\ndistPair$dist &lt;- ifelse(distPair$dist == 0,\n                        50, distPair$dist)\n\nThe code chunk below will be used to check the result data.frame.\n\ndistPair %&gt;%\n  summary()\n\n      Var1             Var2             dist      \n MESZ01 :   332   MESZ01 :   332   Min.   :   50  \n RVSZ05 :   332   RVSZ05 :   332   1st Qu.: 7097  \n SRSZ01 :   332   SRSZ01 :   332   Median :11864  \n WISZ01 :   332   WISZ01 :   332   Mean   :12193  \n MUSZ02 :   332   MUSZ02 :   332   3rd Qu.:16388  \n MPSZ05 :   332   MPSZ05 :   332   Max.   :49894  \n (Other):108232   (Other):108232                  \n\n\nThe code chunk below is used to rename the origin and destination fields.\n\ndistPair &lt;- distPair %&gt;%\n  rename(orig = Var1,\n         dest = Var2)\n\nLastly, the code chunk below is used to save the dataframe for future use.\n\nwrite_rds(distPair, \"data/rds/distPair.rds\")"
  },
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/dump.html",
    "href": "Take-home_Exercise/Take-home_Ex1/dump.html",
    "title": "Untitled",
    "section": "",
    "text": "yps tm_shape(mpsz) +   tm_polygons() %&gt;%   tm_shape() +   tm_lines(lwd = \"MORNING_PEAK\",            style = \"quantile\",            scale = c(0.1, 1, 3, 5, 7, 10),            n = 6,            alpha = 0.3)}\nyps tmap_mode(\"view\")  map_honeycomb = tm_shape(odbus_weekday_6_9_data) +   tm_fill(     col = \"MORNING_PEAK\",     palette = \"Reds\",     style = \"cont\",     title = \"Number of Trips\",     id = \"grid_id\",     showNA = FALSE,     alpha = 0.6,     popup.vars = c(\"MORNING_PEAK\"),     popup.format = list(       MORNING_PEAK = list(format = \"f\", digits = 0))   ) +   tm_borders(col = \"grey40\", lwd = 0.7)  map_honeycomb}\n\n\nThe code chunk below will be used to calculate the the number of trips in each bus stop.\nyps total_trips_in_origin_busstop &lt;-odbus %&gt;%   group_by(ORIGIN_PT_CODE) %&gt;%   summarise(TOTAL_TRIPS=sum(TOTAL_TRIPS))}\n\n\n\nThe code chunk below will be used to join the busstop SpatialPolygonsDataframe and original_destination_bus by the bus stop id (BUS_STOP_N for busstop and ORIGIN_PT_CODE for original_destination_bus). This is performed by using left_join() of dplyr package.\nyps total_trips_in_origin_busstop &lt;- left_join(busstop3414, total_trips_in_origin_busstop, by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\"))}\n\n\n\nThe code chunk below will be used to join the total_trips_in_origin_busstop and honeycomb grid spatially.\n\n\n\n\nThe code chunk below will be used to remove the grid id without\nyps # Remove rows with NA values in the 'bus_stop_N' column total_trips_in_origin_busstop_honeycomb &lt;- total_trips_in_origin_busstop_honeycomb %&gt;%   filter(!is.na(BUS_STOP_N))}\n\n\n\nThe code chunk below is used to calculate the sum of trips in each grid. Some grids consist of more than 1 bus stop and thus the number of trips will be summed.\nyps trips_per_grid &lt;- total_trips_in_origin_busstop_honeycomb %&gt;%   group_by(grid_id) %&gt;%   summarise(total_trips = sum(TOTAL_TRIPS))}\n\n\n\nyps tmap_mode(\"view\")  map_honeycomb = tm_shape(trips_per_grid) +   tm_fill(     col = \"total_trips\",     palette = \"Reds\",     style = \"cont\",     title = \"Number of Trips\",     id = \"grid_id\",     showNA = FALSE,     alpha = 0.6,     popup.vars = c(\"total_trips\"),     popup.format = list(       total_trips = list(format = \"f\", digits = 0))   ) +   tm_borders(col = \"grey40\", lwd = 0.7)  map_honeycomb}\n\n\n\nI will further analyse on the peak hours:\n\n\n\n\nThe chunk code below is to ???\nyps weekday_morning &lt;- origin_destination_bus %&gt;%   filter((TIME_PER_HOUR &gt;= 6 & TIME_PER_HOUR &lt;= 9) & DAY_TYPE == 'WEEKDAY')%&gt;%   group_by(ORIGIN_PT_CODE) %&gt;%   summarise(TOTAL_TRIPS=sum(TOTAL_TRIPS))  weekday_afternoon &lt;- origin_destination_bus %&gt;%   filter((TIME_PER_HOUR &gt;= 17 & TIME_PER_HOUR &lt;= 20) & DAY_TYPE == 'WEEKDAY')%&gt;%   group_by(ORIGIN_PT_CODE) %&gt;%   summarise(TOTAL_TRIPS=sum(TOTAL_TRIPS))  weekend_holiday_morning &lt;- origin_destination_bus %&gt;%   filter((TIME_PER_HOUR &gt;= 11 & TIME_PER_HOUR &lt;= 14) & DAY_TYPE == 'WEEKENDS/HOLIDAY')%&gt;%   group_by(ORIGIN_PT_CODE) %&gt;%   summarise(TOTAL_TRIPS=sum(TOTAL_TRIPS))  weekend_holiday__evening &lt;- origin_destination_bus %&gt;%   filter((TIME_PER_HOUR &gt;= 16 & TIME_PER_HOUR &lt;= 19) & DAY_TYPE == 'WEEKENDS/HOLIDAY')%&gt;%   group_by(ORIGIN_PT_CODE) %&gt;%   summarise(TOTAL_TRIPS=sum(TOTAL_TRIPS))}\n\n\n\nyps weekday_morning &lt;- left_join(busstop, weekday_morning, by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\"))  weekday_afternoon &lt;- left_join(busstop, weekday_afternoon, by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\"))  weekend_holiday_morning &lt;- left_join(busstop, weekend_holiday_morning, by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\"))  weekend_holiday__evening &lt;- left_join(busstop, weekend_holiday__evening, by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\"))}\n\n\n\nyps # Spatially join the trip data with the honeycomb grid #busstop_weekday_morning_honeycomb &lt;- st_join(honeycomb_grid_sf,busstop_weekday_morning)  # Remove rows with NA values in the 'bus_stop_N' column #busstop_weekday_morning_honeycomb &lt;- busstop_weekday_morning_honeycomb %&gt;% #  filter(!is.na(BUS_STOP_N))}\nyps # Group by grid ID and calculate the sum of TOTAL_TRIPS #trips_per_grid_busstop_weekday_morning &lt;- busstop_weekday_morning_honeycomb %&gt;% #  group_by(grid_id) %&gt;%  # summarise(total_trips = sum(TOTAL_TRIPS))}\nyps #tmap_mode(\"view\")  #map_honeycomb = tm_shape(trips_per_grid_busstop_weekday_morning) + #  tm_fill(  #   col = \"total_trips\",  #   palette = \"Reds\",   #  style = \"cont\",   #  title = \"Number of Trips\",   #  id = \"grid_id\",    # showNA = FALSE,   #  alpha = 0.6,   #  popup.vars = c(\"total_trips\"),   #  popup.format = list(   #    total_trips = list(format = \"f\", digits = 0))  ## ) +  # tm_borders(col = \"grey40\", lwd = 0.7)  #map_honeycomb}\n\nyps  #filtered_df &lt;- origin_destination_bus %&gt;%  # filter(   #  (TIME_PER_HOUR %in% peak_hours$`WeekdayMorningPeak` & DAY_TYPE == 'WEEKDAY') |    # (TIME_PER_HOUR %in% peak_hours$`WeekdayAfternoonPeak` & DAY_TYPE == 'WEEKDAY') |     #(TIME_PER_HOUR %in% peak_hours$`Weekend/holidayMorningPeak` & DAY_TYPE == 'WEEKENDS/HOLIDAY') |     #(TIME_PER_HOUR %in% peak_hours$`Weekend/holidayEveningPeak` & DAY_TYPE == 'WEEKENDS/HOLIDAY')   #)}"
  },
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/dump.html#task-1-geovisualisation-and-analysis",
    "href": "Take-home_Exercise/Take-home_Ex1/dump.html#task-1-geovisualisation-and-analysis",
    "title": "Untitled",
    "section": "",
    "text": "yps tm_shape(mpsz) +   tm_polygons() %&gt;%   tm_shape() +   tm_lines(lwd = \"MORNING_PEAK\",            style = \"quantile\",            scale = c(0.1, 1, 3, 5, 7, 10),            n = 6,            alpha = 0.3)}\nyps tmap_mode(\"view\")  map_honeycomb = tm_shape(odbus_weekday_6_9_data) +   tm_fill(     col = \"MORNING_PEAK\",     palette = \"Reds\",     style = \"cont\",     title = \"Number of Trips\",     id = \"grid_id\",     showNA = FALSE,     alpha = 0.6,     popup.vars = c(\"MORNING_PEAK\"),     popup.format = list(       MORNING_PEAK = list(format = \"f\", digits = 0))   ) +   tm_borders(col = \"grey40\", lwd = 0.7)  map_honeycomb}\n\n\nThe code chunk below will be used to calculate the the number of trips in each bus stop.\nyps total_trips_in_origin_busstop &lt;-odbus %&gt;%   group_by(ORIGIN_PT_CODE) %&gt;%   summarise(TOTAL_TRIPS=sum(TOTAL_TRIPS))}\n\n\n\nThe code chunk below will be used to join the busstop SpatialPolygonsDataframe and original_destination_bus by the bus stop id (BUS_STOP_N for busstop and ORIGIN_PT_CODE for original_destination_bus). This is performed by using left_join() of dplyr package.\nyps total_trips_in_origin_busstop &lt;- left_join(busstop3414, total_trips_in_origin_busstop, by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\"))}\n\n\n\nThe code chunk below will be used to join the total_trips_in_origin_busstop and honeycomb grid spatially.\n\n\n\n\nThe code chunk below will be used to remove the grid id without\nyps # Remove rows with NA values in the 'bus_stop_N' column total_trips_in_origin_busstop_honeycomb &lt;- total_trips_in_origin_busstop_honeycomb %&gt;%   filter(!is.na(BUS_STOP_N))}\n\n\n\nThe code chunk below is used to calculate the sum of trips in each grid. Some grids consist of more than 1 bus stop and thus the number of trips will be summed.\nyps trips_per_grid &lt;- total_trips_in_origin_busstop_honeycomb %&gt;%   group_by(grid_id) %&gt;%   summarise(total_trips = sum(TOTAL_TRIPS))}\n\n\n\nyps tmap_mode(\"view\")  map_honeycomb = tm_shape(trips_per_grid) +   tm_fill(     col = \"total_trips\",     palette = \"Reds\",     style = \"cont\",     title = \"Number of Trips\",     id = \"grid_id\",     showNA = FALSE,     alpha = 0.6,     popup.vars = c(\"total_trips\"),     popup.format = list(       total_trips = list(format = \"f\", digits = 0))   ) +   tm_borders(col = \"grey40\", lwd = 0.7)  map_honeycomb}\n\n\n\nI will further analyse on the peak hours:\n\n\n\n\nThe chunk code below is to ???\nyps weekday_morning &lt;- origin_destination_bus %&gt;%   filter((TIME_PER_HOUR &gt;= 6 & TIME_PER_HOUR &lt;= 9) & DAY_TYPE == 'WEEKDAY')%&gt;%   group_by(ORIGIN_PT_CODE) %&gt;%   summarise(TOTAL_TRIPS=sum(TOTAL_TRIPS))  weekday_afternoon &lt;- origin_destination_bus %&gt;%   filter((TIME_PER_HOUR &gt;= 17 & TIME_PER_HOUR &lt;= 20) & DAY_TYPE == 'WEEKDAY')%&gt;%   group_by(ORIGIN_PT_CODE) %&gt;%   summarise(TOTAL_TRIPS=sum(TOTAL_TRIPS))  weekend_holiday_morning &lt;- origin_destination_bus %&gt;%   filter((TIME_PER_HOUR &gt;= 11 & TIME_PER_HOUR &lt;= 14) & DAY_TYPE == 'WEEKENDS/HOLIDAY')%&gt;%   group_by(ORIGIN_PT_CODE) %&gt;%   summarise(TOTAL_TRIPS=sum(TOTAL_TRIPS))  weekend_holiday__evening &lt;- origin_destination_bus %&gt;%   filter((TIME_PER_HOUR &gt;= 16 & TIME_PER_HOUR &lt;= 19) & DAY_TYPE == 'WEEKENDS/HOLIDAY')%&gt;%   group_by(ORIGIN_PT_CODE) %&gt;%   summarise(TOTAL_TRIPS=sum(TOTAL_TRIPS))}\n\n\n\nyps weekday_morning &lt;- left_join(busstop, weekday_morning, by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\"))  weekday_afternoon &lt;- left_join(busstop, weekday_afternoon, by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\"))  weekend_holiday_morning &lt;- left_join(busstop, weekend_holiday_morning, by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\"))  weekend_holiday__evening &lt;- left_join(busstop, weekend_holiday__evening, by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\"))}\n\n\n\nyps # Spatially join the trip data with the honeycomb grid #busstop_weekday_morning_honeycomb &lt;- st_join(honeycomb_grid_sf,busstop_weekday_morning)  # Remove rows with NA values in the 'bus_stop_N' column #busstop_weekday_morning_honeycomb &lt;- busstop_weekday_morning_honeycomb %&gt;% #  filter(!is.na(BUS_STOP_N))}\nyps # Group by grid ID and calculate the sum of TOTAL_TRIPS #trips_per_grid_busstop_weekday_morning &lt;- busstop_weekday_morning_honeycomb %&gt;% #  group_by(grid_id) %&gt;%  # summarise(total_trips = sum(TOTAL_TRIPS))}\nyps #tmap_mode(\"view\")  #map_honeycomb = tm_shape(trips_per_grid_busstop_weekday_morning) + #  tm_fill(  #   col = \"total_trips\",  #   palette = \"Reds\",   #  style = \"cont\",   #  title = \"Number of Trips\",   #  id = \"grid_id\",    # showNA = FALSE,   #  alpha = 0.6,   #  popup.vars = c(\"total_trips\"),   #  popup.format = list(   #    total_trips = list(format = \"f\", digits = 0))  ## ) +  # tm_borders(col = \"grey40\", lwd = 0.7)  #map_honeycomb}\n\nyps  #filtered_df &lt;- origin_destination_bus %&gt;%  # filter(   #  (TIME_PER_HOUR %in% peak_hours$`WeekdayMorningPeak` & DAY_TYPE == 'WEEKDAY') |    # (TIME_PER_HOUR %in% peak_hours$`WeekdayAfternoonPeak` & DAY_TYPE == 'WEEKDAY') |     #(TIME_PER_HOUR %in% peak_hours$`Weekend/holidayMorningPeak` & DAY_TYPE == 'WEEKENDS/HOLIDAY') |     #(TIME_PER_HOUR %in% peak_hours$`Weekend/holidayEveningPeak` & DAY_TYPE == 'WEEKENDS/HOLIDAY')   #)}"
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex3/In-class_Ex3.html#preparing-flow-data",
    "href": "In-class_Exercise/In-class_Ex3/In-class_Ex3.html#preparing-flow-data",
    "title": "In-class_Ex3:Calibrating Spatial Interaction Models with R",
    "section": "6 Preparing flow data",
    "text": "6 Preparing flow data\nThe code chunk below is used import od_data save in Chapter 15 into R environment.\n\nod_data &lt;- read_rds(\"data/rds/od_data.rds\")\n\nNext, we will compute the total passenger trip between and within planning subzones by using the code chunk below. The output is all flow_data.\n\nflow_data &lt;- od_data %&gt;%\n  group_by(ORIGIN_SZ, DESTIN_SZ) %&gt;% \n  summarize(TRIPS = sum(MORNING_PEAK))\n\nUse the code chunk below to display flow_data dataframe.\n\nhead(flow_data, 10)\n\n# A tibble: 10 × 3\n# Groups:   ORIGIN_SZ [1]\n   ORIGIN_SZ DESTIN_SZ TRIPS\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;\n 1 AMSZ01    AMSZ01     2694\n 2 AMSZ01    AMSZ02    10591\n 3 AMSZ01    AMSZ03    14980\n 4 AMSZ01    AMSZ04     3106\n 5 AMSZ01    AMSZ05     7734\n 6 AMSZ01    AMSZ06     2306\n 7 AMSZ01    AMSZ07     1824\n 8 AMSZ01    AMSZ08     2734\n 9 AMSZ01    AMSZ09     2300\n10 AMSZ01    AMSZ10      164\n\n\n\n6.1 Separating intra-flow from passenger volume df\nCode chunk below is used to add three new fields in flow_data dataframe.\n\nflow_data$FlowNoIntra &lt;- ifelse(\n  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, \n  0, flow_data$TRIPS)\nflow_data$offset &lt;- ifelse(\n  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, \n  0.000001, 1)\n\n\n\n6.2 Combining passenger volume data with distance value\nBefore we can join flow_data and distPair, we need to convert data value type of ORIGIN_SZ and DESTIN_SZ fields of flow_data dataframe into factor data type.\n\nflow_data$ORIGIN_SZ &lt;- as.factor(flow_data$ORIGIN_SZ)\nflow_data$DESTIN_SZ &lt;- as.factor(flow_data$DESTIN_SZ)\n\nNow, left_join() of dplyr will be used to flow_data dataframe and distPair dataframe. The output is called flow_data1.\n\nflow_data1 &lt;- flow_data %&gt;%\n  left_join (distPair,\n             by = c(\"ORIGIN_SZ\" = \"orig\",\n                    \"DESTIN_SZ\" = \"dest\"))"
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex3/In-class_Ex3.html#preparing-origin-and-destination-attributes",
    "href": "In-class_Exercise/In-class_Ex3/In-class_Ex3.html#preparing-origin-and-destination-attributes",
    "title": "In-class_Ex3:Calibrating Spatial Interaction Models with R",
    "section": "7 Preparing Origin and Destination Attributes",
    "text": "7 Preparing Origin and Destination Attributes\n\n7.1 Importing population data\n\npop &lt;- read_csv(\"data/aspatial/pop.csv\")\n\n\n\n7.2 Geospatial data wrangling\n\npop &lt;- pop %&gt;%\n  left_join(mpsz,\n            by = c(\"PA\" = \"PLN_AREA_N\",\n                   \"SZ\" = \"SUBZONE_N\")) %&gt;%\n  select(1:6) %&gt;%\n  rename(SZ_NAME = SZ,\n         SZ = SUBZONE_C)\n\n\n\n7.3 Preparing origin attribute\n\nflow_data1 &lt;- flow_data1 %&gt;%\n  left_join(pop,\n            by = c(ORIGIN_SZ = \"SZ\")) %&gt;%\n  rename(ORIGIN_AGE7_12 = AGE7_12,\n         ORIGIN_AGE13_24 = AGE13_24,\n         ORIGIN_AGE25_64 = AGE25_64) %&gt;%\n  select(-c(PA, SZ_NAME))\n\n\n\n7.4 Preparing destination attribute\n\nflow_data1 &lt;- flow_data1 %&gt;%\n  left_join(pop,\n            by = c(DESTIN_SZ = \"SZ\")) %&gt;%\n  rename(DESTIN_AGE7_12 = AGE7_12,\n         DESTIN_AGE13_24 = AGE13_24,\n         DESTIN_AGE25_64 = AGE25_64) %&gt;%\n  select(-c(PA, SZ_NAME))\n\nWe will called the output data file SIM_data. it is in rds data file format.\n\nwrite_rds(flow_data1, \"data/rds/SIM_data\")"
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex3/In-class_Ex3.html#calibrating-spatial-interaction-models",
    "href": "In-class_Exercise/In-class_Ex3/In-class_Ex3.html#calibrating-spatial-interaction-models",
    "title": "In-class_Ex3:Calibrating Spatial Interaction Models with R",
    "section": "8 Calibrating Spatial Interaction Models",
    "text": "8 Calibrating Spatial Interaction Models\nIn this section, you will learn how to calibrate Spatial Interaction Models by using Poisson Regression method.\n\n8.1 16.8.1 Importing the modelling data\nFirstly, let us import the modelling data by using the code chunk below.\n\nSIM_data &lt;- read_rds(\"data/rds/SIM_data.rds\")\n\n\n\n8.2 Visualising the dependent variable\nFirstly, let us plot the distribution of the dependent variable (i.e. TRIPS) by using histogram method by using the code chunk below.\n\nggplot(data = SIM_data,\n       aes(x = TRIPS)) +\n  geom_histogram()\n\n\n\n\nNotice that the distribution is highly skewed and not resemble bell shape or also known as normal distribution.\nNext, let us visualise the relation between the dependent variable and one of the key independent variable in Spatial Interaction Model, namely distance.\n\nggplot(data = SIM_data,\n       aes(x = dist,\n           y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n\nNotice that their relationship hardly resemble linear relationship.\nOn the other hand, if we plot the scatter plot by using the log transformed version of both variables, we can see that their relationship is more resemble linear relationship.\n\nggplot(data = SIM_data,\n       aes(x = log(dist),\n           y = log(TRIPS))) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n\n\n\n8.3 Checking for variables with zero values\nSince Poisson Regression is based of log and log 0 is undefined, it is important for us to ensure that no 0 values in the explanatory variables.\nIn the code chunk below, summary() of Base R is used to compute the summary statistics of all variables in SIM_data data frame.\n\nsummary(SIM_data)\n\n  ORIGIN_SZ          DESTIN_SZ             TRIPS           FlowNoIntra      \n Length:14274       Length:14274       Min.   :     1.0   Min.   :     1.0  \n Class :character   Class :character   1st Qu.:    11.0   1st Qu.:    11.0  \n Mode  :character   Mode  :character   Median :    56.0   Median :    56.0  \n                                       Mean   :   664.3   Mean   :   664.3  \n                                       3rd Qu.:   296.0   3rd Qu.:   296.0  \n                                       Max.   :104167.0   Max.   :104167.0  \n     offset       dist         ORIGIN_AGE7_12 ORIGIN_AGE13_24 ORIGIN_AGE25_64\n Min.   :1   Min.   :  173.8   Min.   :   0   Min.   :    0   Min.   :    0  \n 1st Qu.:1   1st Qu.: 3465.4   1st Qu.: 240   1st Qu.:  460   1st Qu.: 2210  \n Median :1   Median : 6121.0   Median : 710   Median : 1400   Median : 7030  \n Mean   :1   Mean   : 6951.8   Mean   :1037   Mean   : 2278   Mean   :10536  \n 3rd Qu.:1   3rd Qu.: 9725.1   3rd Qu.:1500   3rd Qu.: 3282   3rd Qu.:15830  \n Max.   :1   Max.   :26135.8   Max.   :6340   Max.   :16380   Max.   :74610  \n DESTIN_AGE7_12 DESTIN_AGE13_24 DESTIN_AGE25_64\n Min.   :   0   Min.   :    0   Min.   :    0  \n 1st Qu.: 250   1st Qu.:  460   1st Qu.: 2210  \n Median : 720   Median : 1430   Median : 7120  \n Mean   :1040   Mean   : 2305   Mean   :10648  \n 3rd Qu.:1500   3rd Qu.: 3290   3rd Qu.:15830  \n Max.   :6340   Max.   :16380   Max.   :74610  \n\n\nThe print report above reveals that variables ORIGIN_AGE7_12, ORIGIN_AGE13_24, ORIGIN_AGE25_64,DESTIN_AGE7_12, DESTIN_AGE13_24, DESTIN_AGE25_64 consist of 0 values.\nIn view of this, code chunk below will be used to replace zero values to 0.99.\n\nSIM_data$DESTIN_AGE7_12 &lt;- ifelse(\n  SIM_data$DESTIN_AGE7_12 == 0,\n  0.99, SIM_data$DESTIN_AGE7_12)\nSIM_data$DESTIN_AGE13_24 &lt;- ifelse(\n  SIM_data$DESTIN_AGE13_24 == 0,\n  0.99, SIM_data$DESTIN_AGE13_24)\nSIM_data$DESTIN_AGE25_64 &lt;- ifelse(\n  SIM_data$DESTIN_AGE25_64 == 0,\n  0.99, SIM_data$DESTIN_AGE25_64)\nSIM_data$ORIGIN_AGE7_12 &lt;- ifelse(\n  SIM_data$ORIGIN_AGE7_12 == 0,\n  0.99, SIM_data$ORIGIN_AGE7_12)\nSIM_data$ORIGIN_AGE13_24 &lt;- ifelse(\n  SIM_data$ORIGIN_AGE13_24 == 0,\n  0.99, SIM_data$ORIGIN_AGE13_24)\nSIM_data$ORIGIN_AGE25_64 &lt;- ifelse(\n  SIM_data$ORIGIN_AGE25_64 == 0,\n  0.99, SIM_data$ORIGIN_AGE25_64)\n\nYou can run the summary() again.\n\nsummary(SIM_data)\n\n  ORIGIN_SZ          DESTIN_SZ             TRIPS           FlowNoIntra      \n Length:14274       Length:14274       Min.   :     1.0   Min.   :     1.0  \n Class :character   Class :character   1st Qu.:    11.0   1st Qu.:    11.0  \n Mode  :character   Mode  :character   Median :    56.0   Median :    56.0  \n                                       Mean   :   664.3   Mean   :   664.3  \n                                       3rd Qu.:   296.0   3rd Qu.:   296.0  \n                                       Max.   :104167.0   Max.   :104167.0  \n     offset       dist         ORIGIN_AGE7_12    ORIGIN_AGE13_24   \n Min.   :1   Min.   :  173.8   Min.   :   0.99   Min.   :    0.99  \n 1st Qu.:1   1st Qu.: 3465.4   1st Qu.: 240.00   1st Qu.:  460.00  \n Median :1   Median : 6121.0   Median : 710.00   Median : 1400.00  \n Mean   :1   Mean   : 6951.8   Mean   :1036.73   Mean   : 2278.59  \n 3rd Qu.:1   3rd Qu.: 9725.1   3rd Qu.:1500.00   3rd Qu.: 3282.50  \n Max.   :1   Max.   :26135.8   Max.   :6340.00   Max.   :16380.00  \n ORIGIN_AGE25_64    DESTIN_AGE7_12    DESTIN_AGE13_24    DESTIN_AGE25_64   \n Min.   :    0.99   Min.   :   0.99   Min.   :    0.99   Min.   :    0.99  \n 1st Qu.: 2210.00   1st Qu.: 250.00   1st Qu.:  460.00   1st Qu.: 2210.00  \n Median : 7030.00   Median : 720.00   Median : 1430.00   Median : 7120.00  \n Mean   :10535.93   Mean   :1039.98   Mean   : 2305.33   Mean   :10647.95  \n 3rd Qu.:15830.00   3rd Qu.:1500.00   3rd Qu.: 3290.00   3rd Qu.:15830.00  \n Max.   :74610.00   Max.   :6340.00   Max.   :16380.00   Max.   :74610.00  \n\n\n\n\n8.4 Unconstrained Spatial Interaction Model\nIn this section, you will learn how to calibrate an unconstrained spatial interaction model by using glm() of Base Stats. The explanatory variables are origin population by different age cohort, destination population by different age cohort (i.e. ORIGIN_AGE25_64) and distance between origin and destination in km (i.e. dist).\nThe general formula of Unconstrained Spatial Interaction Model\nThe code chunk undefinedsed to calibrate to model is shown below:\n\nuncSIM &lt;- glm(formula = TRIPS ~ \n                log(ORIGIN_AGE25_64) + \n                log(DESTIN_AGE25_64) +\n                log(dist),\n              family = poisson(link = \"log\"),\n              data = SIM_data,\n              na.action = na.exclude)\nuncSIM\n\n\nCall:  glm(formula = TRIPS ~ log(ORIGIN_AGE25_64) + log(DESTIN_AGE25_64) + \n    log(dist), family = poisson(link = \"log\"), data = SIM_data, \n    na.action = na.exclude)\n\nCoefficients:\n         (Intercept)  log(ORIGIN_AGE25_64)  log(DESTIN_AGE25_64)  \n            17.00287               0.21001               0.01289  \n           log(dist)  \n            -1.51785  \n\nDegrees of Freedom: 14273 Total (i.e. Null);  14270 Residual\nNull Deviance:      36120000 \nResidual Deviance: 19960000     AIC: 20040000\n\n\n\n\n8.5 R-squared function\nIn order to measure how much variation of the trips can be accounted by the model we will write a function to calculate R-Squared value as shown below.\n\nCalcRSquared &lt;- function(observed,estimated){\n  r &lt;- cor(observed,estimated)\n  R2 &lt;- r^2\n  R2\n}\n\nNext, we will compute the R-squared of the unconstrained SIM by using the code chunk below.\n\nCalcRSquared(uncSIM$data$TRIPS, uncSIM$fitted.values)\n\n[1] 0.1694734\n\n\n\nr2_mcfadden(uncSIM)\n\n# R2 for Generalized Linear Regression\n       R2: 0.446\n  adj. R2: 0.446\n\n\n\n\n8.6 Origin (Production) constrained SIM\nIn this section, we will fit an origin constrained SIM by using the code3 chunk below.\nThe general formula of Origin Constrained Spatial Interaction Model\n\norcSIM &lt;- glm(formula = TRIPS ~ \n                 ORIGIN_SZ +\n                 log(DESTIN_AGE25_64) +\n                 log(dist),\n              family = poisson(link = \"log\"),\n              data = SIM_data,\n              na.action = na.exclude)\nsummary(orcSIM)\n\n\nCall:\nglm(formula = TRIPS ~ ORIGIN_SZ + log(DESTIN_AGE25_64) + log(dist), \n    family = poisson(link = \"log\"), data = SIM_data, na.action = na.exclude)\n\nCoefficients:\n                       Estimate Std. Error   z value Pr(&gt;|z|)    \n(Intercept)          19.9309957  0.0054015  3689.887  &lt; 2e-16 ***\nORIGIN_SZAMSZ02       0.6805710  0.0052686   129.175  &lt; 2e-16 ***\nORIGIN_SZAMSZ03       0.3597850  0.0054884    65.554  &lt; 2e-16 ***\nORIGIN_SZAMSZ04      -0.1106566  0.0060027   -18.434  &lt; 2e-16 ***\nORIGIN_SZAMSZ05      -0.3140561  0.0067998   -46.186  &lt; 2e-16 ***\nORIGIN_SZAMSZ06       0.0634425  0.0060258    10.528  &lt; 2e-16 ***\nORIGIN_SZAMSZ07      -1.1301580  0.0110298  -102.464  &lt; 2e-16 ***\nORIGIN_SZAMSZ08      -0.6330394  0.0102949   -61.491  &lt; 2e-16 ***\nORIGIN_SZAMSZ09       0.1064915  0.0063450    16.784  &lt; 2e-16 ***\nORIGIN_SZAMSZ10       0.5061899  0.0053889    93.931  &lt; 2e-16 ***\nORIGIN_SZAMSZ11      -1.3167911  0.0144870   -90.895  &lt; 2e-16 ***\nORIGIN_SZAMSZ12      -1.5103004  0.0127453  -118.499  &lt; 2e-16 ***\nORIGIN_SZBDSZ01       1.3626004  0.0051433   264.929  &lt; 2e-16 ***\nORIGIN_SZBDSZ02       0.9554084  0.0059655   160.156  &lt; 2e-16 ***\nORIGIN_SZBDSZ03       1.1476190  0.0054278   211.433  &lt; 2e-16 ***\nORIGIN_SZBDSZ04       2.0110410  0.0046344   433.940  &lt; 2e-16 ***\nORIGIN_SZBDSZ05       1.0658940  0.0053976   197.477  &lt; 2e-16 ***\nORIGIN_SZBDSZ06       1.2719222  0.0054774   232.213  &lt; 2e-16 ***\nORIGIN_SZBDSZ07      -0.5053039  0.0111553   -45.297  &lt; 2e-16 ***\nORIGIN_SZBDSZ08      -0.3556193  0.0102947   -34.544  &lt; 2e-16 ***\nORIGIN_SZBKSZ01      -0.3606399  0.0075473   -47.784  &lt; 2e-16 ***\nORIGIN_SZBKSZ02       0.1357265  0.0061394    22.107  &lt; 2e-16 ***\nORIGIN_SZBKSZ03       0.4101999  0.0058983    69.545  &lt; 2e-16 ***\nORIGIN_SZBKSZ04      -0.3418645  0.0070764   -48.310  &lt; 2e-16 ***\nORIGIN_SZBKSZ05      -0.2986750  0.0074073   -40.322  &lt; 2e-16 ***\nORIGIN_SZBKSZ06      -0.2637855  0.0068739   -38.375  &lt; 2e-16 ***\nORIGIN_SZBKSZ07       0.5498323  0.0051476   106.813  &lt; 2e-16 ***\nORIGIN_SZBKSZ08      -0.0527393  0.0061457    -8.582  &lt; 2e-16 ***\nORIGIN_SZBKSZ09      -0.1564691  0.0067300   -23.249  &lt; 2e-16 ***\nORIGIN_SZBLSZ01      -1.7551329  0.0176599   -99.385  &lt; 2e-16 ***\nORIGIN_SZBLSZ02      -1.9493637  0.0213859   -91.152  &lt; 2e-16 ***\nORIGIN_SZBLSZ03      -2.9057732  0.0535995   -54.213  &lt; 2e-16 ***\nORIGIN_SZBLSZ04      -1.4672066  0.0254726   -57.599  &lt; 2e-16 ***\nORIGIN_SZBMSZ01       0.1806064  0.0060563    29.821  &lt; 2e-16 ***\nORIGIN_SZBMSZ02      -1.4026549  0.0078244  -179.267  &lt; 2e-16 ***\nORIGIN_SZBMSZ03      -0.5976236  0.0063808   -93.660  &lt; 2e-16 ***\nORIGIN_SZBMSZ04      -0.5456513  0.0059061   -92.388  &lt; 2e-16 ***\nORIGIN_SZBMSZ05      -3.1095195  0.0188118  -165.297  &lt; 2e-16 ***\nORIGIN_SZBMSZ06      -3.0273827  0.0194319  -155.794  &lt; 2e-16 ***\nORIGIN_SZBMSZ07      -0.7378197  0.0066865  -110.345  &lt; 2e-16 ***\nORIGIN_SZBMSZ08      -0.9306150  0.0067188  -138.510  &lt; 2e-16 ***\nORIGIN_SZBMSZ09      -1.4137345  0.0101071  -139.876  &lt; 2e-16 ***\nORIGIN_SZBMSZ10      -1.7054195  0.0101582  -167.886  &lt; 2e-16 ***\nORIGIN_SZBMSZ11      -1.2418380  0.0076792  -161.714  &lt; 2e-16 ***\nORIGIN_SZBMSZ12      -1.3746537  0.0109769  -125.231  &lt; 2e-16 ***\nORIGIN_SZBMSZ13      -0.4339494  0.0069335   -62.587  &lt; 2e-16 ***\nORIGIN_SZBMSZ14      -0.9950458  0.0076302  -130.410  &lt; 2e-16 ***\nORIGIN_SZBMSZ15      -0.6544196  0.0068964   -94.892  &lt; 2e-16 ***\nORIGIN_SZBMSZ16      -1.5193747  0.0105329  -144.250  &lt; 2e-16 ***\nORIGIN_SZBMSZ17      -1.6536771  0.0180672   -91.529  &lt; 2e-16 ***\nORIGIN_SZBPSZ01       0.1484355  0.0064734    22.930  &lt; 2e-16 ***\nORIGIN_SZBPSZ02      -0.3602094  0.0073902   -48.741  &lt; 2e-16 ***\nORIGIN_SZBPSZ03      -0.1567975  0.0072226   -21.709  &lt; 2e-16 ***\nORIGIN_SZBPSZ04       0.4504873  0.0058418    77.115  &lt; 2e-16 ***\nORIGIN_SZBPSZ05       0.5028646  0.0053682    93.675  &lt; 2e-16 ***\nORIGIN_SZBPSZ06      -1.0125668  0.0105638   -95.853  &lt; 2e-16 ***\nORIGIN_SZBPSZ07      -0.3859065  0.0098561   -39.154  &lt; 2e-16 ***\nORIGIN_SZBSSZ01       0.1488497  0.0065504    22.724  &lt; 2e-16 ***\nORIGIN_SZBSSZ02       0.4269498  0.0055893    76.387  &lt; 2e-16 ***\nORIGIN_SZBSSZ03      -0.2437385  0.0062020   -39.300  &lt; 2e-16 ***\nORIGIN_SZBTSZ01       0.1987940  0.0066672    29.817  &lt; 2e-16 ***\nORIGIN_SZBTSZ02      -0.4571546  0.0090784   -50.356  &lt; 2e-16 ***\nORIGIN_SZBTSZ03      -0.2697243  0.0077941   -34.606  &lt; 2e-16 ***\nORIGIN_SZBTSZ04      -1.0997236  0.0115225   -95.441  &lt; 2e-16 ***\nORIGIN_SZBTSZ05      -1.0053122  0.0132594   -75.819  &lt; 2e-16 ***\nORIGIN_SZBTSZ06      -1.0841201  0.0102242  -106.035  &lt; 2e-16 ***\nORIGIN_SZBTSZ07      -2.3134497  0.0158499  -145.960  &lt; 2e-16 ***\nORIGIN_SZBTSZ08      -1.1581618  0.0121161   -95.589  &lt; 2e-16 ***\nORIGIN_SZCBSZ01      -1.0805930  0.0577831   -18.701  &lt; 2e-16 ***\nORIGIN_SZCCSZ01      -0.8145372  0.0152638   -53.364  &lt; 2e-16 ***\nORIGIN_SZCHSZ01       0.0377079  0.0133240     2.830 0.004654 ** \nORIGIN_SZCHSZ02      -0.6209553  0.0096388   -64.422  &lt; 2e-16 ***\nORIGIN_SZCHSZ03       1.6790244  0.0069559   241.381  &lt; 2e-16 ***\nORIGIN_SZCKSZ01       0.0839586  0.0059934    14.008  &lt; 2e-16 ***\nORIGIN_SZCKSZ02       0.4379511  0.0062289    70.309  &lt; 2e-16 ***\nORIGIN_SZCKSZ03       0.7956950  0.0051892   153.335  &lt; 2e-16 ***\nORIGIN_SZCKSZ04       1.2740323  0.0053165   239.637  &lt; 2e-16 ***\nORIGIN_SZCKSZ05       0.9326213  0.0061807   150.893  &lt; 2e-16 ***\nORIGIN_SZCKSZ06       0.3976273  0.0085639    46.431  &lt; 2e-16 ***\nORIGIN_SZCLSZ01      -0.7522917  0.0094655   -79.477  &lt; 2e-16 ***\nORIGIN_SZCLSZ02      -1.3937450  0.0153260   -90.940  &lt; 2e-16 ***\nORIGIN_SZCLSZ03      -0.7898683  0.0091016   -86.784  &lt; 2e-16 ***\nORIGIN_SZCLSZ04       0.8451512  0.0051258   164.882  &lt; 2e-16 ***\nORIGIN_SZCLSZ05      -1.6573818  0.0166091   -99.788  &lt; 2e-16 ***\nORIGIN_SZCLSZ06       0.9478181  0.0048182   196.716  &lt; 2e-16 ***\nORIGIN_SZCLSZ07      -0.2499753  0.0064632   -38.677  &lt; 2e-16 ***\nORIGIN_SZCLSZ08       0.1350119  0.0069296    19.483  &lt; 2e-16 ***\nORIGIN_SZCLSZ09      -1.3868782  0.0192743   -71.955  &lt; 2e-16 ***\nORIGIN_SZDTSZ02      -3.7535792  0.0871325   -43.079  &lt; 2e-16 ***\nORIGIN_SZDTSZ03      -3.8462041  0.0840156   -45.780  &lt; 2e-16 ***\nORIGIN_SZDTSZ13      -2.9738127  0.0349241   -85.151  &lt; 2e-16 ***\nORIGIN_SZGLSZ01      -1.5175198  0.0110135  -137.787  &lt; 2e-16 ***\nORIGIN_SZGLSZ02       0.2405712  0.0058742    40.954  &lt; 2e-16 ***\nORIGIN_SZGLSZ03       0.1940241  0.0061989    31.300  &lt; 2e-16 ***\nORIGIN_SZGLSZ04       1.0292572  0.0049028   209.931  &lt; 2e-16 ***\nORIGIN_SZGLSZ05       0.9864552  0.0050898   193.811  &lt; 2e-16 ***\nORIGIN_SZHGSZ01       0.3073609  0.0054307    56.597  &lt; 2e-16 ***\nORIGIN_SZHGSZ02       0.3827293  0.0054555    70.154  &lt; 2e-16 ***\nORIGIN_SZHGSZ03       0.2342580  0.0059240    39.544  &lt; 2e-16 ***\nORIGIN_SZHGSZ04       0.8750090  0.0049639   176.275  &lt; 2e-16 ***\nORIGIN_SZHGSZ05       1.1695280  0.0049468   236.420  &lt; 2e-16 ***\nORIGIN_SZHGSZ06      -0.0462411  0.0063805    -7.247 4.25e-13 ***\nORIGIN_SZHGSZ07       0.4488583  0.0055139    81.404  &lt; 2e-16 ***\nORIGIN_SZHGSZ08       0.2236095  0.0061279    36.490  &lt; 2e-16 ***\nORIGIN_SZHGSZ09      -1.6376674  0.0084442  -193.941  &lt; 2e-16 ***\nORIGIN_SZHGSZ10      -2.9849025  0.0501042   -59.574  &lt; 2e-16 ***\nORIGIN_SZJESZ01       0.3926525  0.0056268    69.783  &lt; 2e-16 ***\nORIGIN_SZJESZ02       0.1230160  0.0056864    21.633  &lt; 2e-16 ***\nORIGIN_SZJESZ03       0.0188276  0.0061020     3.085 0.002032 ** \nORIGIN_SZJESZ04      -1.3611618  0.0117184  -116.156  &lt; 2e-16 ***\nORIGIN_SZJESZ05      -2.0643662  0.0157083  -131.419  &lt; 2e-16 ***\nORIGIN_SZJESZ06       0.1556368  0.0055245    28.172  &lt; 2e-16 ***\nORIGIN_SZJESZ07      -1.7664532  0.0133171  -132.646  &lt; 2e-16 ***\nORIGIN_SZJESZ08      -0.9115981  0.0138203   -65.961  &lt; 2e-16 ***\nORIGIN_SZJESZ09       0.6121916  0.0060381   101.388  &lt; 2e-16 ***\nORIGIN_SZJESZ10      -1.1953045  0.0233216   -51.253  &lt; 2e-16 ***\nORIGIN_SZJESZ11      -1.4088748  0.0220921   -63.773  &lt; 2e-16 ***\nORIGIN_SZJWSZ01       0.5759093  0.0077741    74.081  &lt; 2e-16 ***\nORIGIN_SZJWSZ02       0.9769314  0.0053029   184.227  &lt; 2e-16 ***\nORIGIN_SZJWSZ03       1.3242695  0.0049068   269.882  &lt; 2e-16 ***\nORIGIN_SZJWSZ04       0.5621088  0.0057831    97.199  &lt; 2e-16 ***\nORIGIN_SZJWSZ05      -1.5744341  0.0146904  -107.174  &lt; 2e-16 ***\nORIGIN_SZJWSZ06      -0.9113320  0.0126913   -71.807  &lt; 2e-16 ***\nORIGIN_SZJWSZ07      -2.3083419  0.0357843   -64.507  &lt; 2e-16 ***\nORIGIN_SZJWSZ08       2.0114225  0.0047956   419.429  &lt; 2e-16 ***\nORIGIN_SZJWSZ09       1.9086705  0.0045255   421.759  &lt; 2e-16 ***\nORIGIN_SZKLSZ01       0.2743166  0.0056908    48.204  &lt; 2e-16 ***\nORIGIN_SZKLSZ02      -0.6443386  0.0074521   -86.463  &lt; 2e-16 ***\nORIGIN_SZKLSZ03      -0.3990113  0.0067213   -59.366  &lt; 2e-16 ***\nORIGIN_SZKLSZ04      -2.1413876  0.0138405  -154.719  &lt; 2e-16 ***\nORIGIN_SZKLSZ05      -1.0913697  0.0121512   -89.816  &lt; 2e-16 ***\nORIGIN_SZKLSZ06      -5.6240764  0.1857405   -30.279  &lt; 2e-16 ***\nORIGIN_SZKLSZ07      -1.1885897  0.0096830  -122.750  &lt; 2e-16 ***\nORIGIN_SZKLSZ08      -1.7018593  0.0114317  -148.872  &lt; 2e-16 ***\nORIGIN_SZLKSZ01      -1.6659670  0.0446420   -37.318  &lt; 2e-16 ***\nORIGIN_SZMDSZ01      -1.1210505  0.0318834   -35.161  &lt; 2e-16 ***\nORIGIN_SZMDSZ02      -0.5096299  0.0116645   -43.691  &lt; 2e-16 ***\nORIGIN_SZMDSZ03      -1.9187039  0.0198291   -96.762  &lt; 2e-16 ***\nORIGIN_SZMPSZ01      -0.5260512  0.0094201   -55.844  &lt; 2e-16 ***\nORIGIN_SZMPSZ02      -0.2905084  0.0077974   -37.257  &lt; 2e-16 ***\nORIGIN_SZMPSZ03       0.3342293  0.0063715    52.457  &lt; 2e-16 ***\nORIGIN_SZMUSZ02      -3.8337096  0.1105053   -34.693  &lt; 2e-16 ***\nORIGIN_SZNTSZ01      -2.9845040  0.0397028   -75.171  &lt; 2e-16 ***\nORIGIN_SZNTSZ02      -3.1812985  0.0249470  -127.522  &lt; 2e-16 ***\nORIGIN_SZNTSZ03      -0.9742991  0.0085424  -114.054  &lt; 2e-16 ***\nORIGIN_SZNTSZ05      -4.2086932  0.0579737   -72.597  &lt; 2e-16 ***\nORIGIN_SZNTSZ06      -4.5831822  0.0583494   -78.547  &lt; 2e-16 ***\nORIGIN_SZNVSZ01       0.3186962  0.0052944    60.195  &lt; 2e-16 ***\nORIGIN_SZNVSZ02      -0.5321136  0.0073747   -72.154  &lt; 2e-16 ***\nORIGIN_SZNVSZ03      -0.9911852  0.0090560  -109.451  &lt; 2e-16 ***\nORIGIN_SZNVSZ04      -0.8329721  0.0099590   -83.640  &lt; 2e-16 ***\nORIGIN_SZNVSZ05      -2.1460777  0.0182401  -117.657  &lt; 2e-16 ***\nORIGIN_SZPGSZ01      -0.5604078  0.0151515   -36.987  &lt; 2e-16 ***\nORIGIN_SZPGSZ02      -0.4025139  0.0085135   -47.279  &lt; 2e-16 ***\nORIGIN_SZPGSZ03       0.6975483  0.0055534   125.608  &lt; 2e-16 ***\nORIGIN_SZPGSZ04       1.2175486  0.0051080   238.363  &lt; 2e-16 ***\nORIGIN_SZPGSZ05       0.3895354  0.0069851    55.767  &lt; 2e-16 ***\nORIGIN_SZPLSZ01      -0.5572701  0.0134473   -41.441  &lt; 2e-16 ***\nORIGIN_SZPLSZ02      -0.9854214  0.0172337   -57.180  &lt; 2e-16 ***\nORIGIN_SZPLSZ03      -1.6991954  0.0472629   -35.952  &lt; 2e-16 ***\nORIGIN_SZPLSZ04      -2.2000217  0.0373580   -58.890  &lt; 2e-16 ***\nORIGIN_SZPLSZ05      -1.7086663  0.0260920   -65.486  &lt; 2e-16 ***\nORIGIN_SZPNSZ01       1.5292867  0.0055102   277.535  &lt; 2e-16 ***\nORIGIN_SZPNSZ02       0.7457519  0.0127815    58.346  &lt; 2e-16 ***\nORIGIN_SZPNSZ03      -1.3659046  0.0216180   -63.184  &lt; 2e-16 ***\nORIGIN_SZPNSZ04      -2.0025379  0.0360655   -55.525  &lt; 2e-16 ***\nORIGIN_SZPNSZ05      -0.9157959  0.0320955   -28.533  &lt; 2e-16 ***\nORIGIN_SZPRSZ01       0.0522611  0.0139142     3.756 0.000173 ***\nORIGIN_SZPRSZ02       1.3063371  0.0053809   242.774  &lt; 2e-16 ***\nORIGIN_SZPRSZ03       0.9963670  0.0054293   183.516  &lt; 2e-16 ***\nORIGIN_SZPRSZ04      -0.0300950  0.0088010    -3.419 0.000627 ***\nORIGIN_SZPRSZ05       1.6840313  0.0050839   331.245  &lt; 2e-16 ***\nORIGIN_SZPRSZ06      -0.8277202  0.0131296   -63.042  &lt; 2e-16 ***\nORIGIN_SZPRSZ07      -2.1698449  0.0177362  -122.340  &lt; 2e-16 ***\nORIGIN_SZPRSZ08       0.4559353  0.0072609    62.793  &lt; 2e-16 ***\nORIGIN_SZQTSZ01      -0.3517047  0.0078770   -44.650  &lt; 2e-16 ***\nORIGIN_SZQTSZ02      -0.8199353  0.0071544  -114.605  &lt; 2e-16 ***\nORIGIN_SZQTSZ03      -0.2457614  0.0065555   -37.490  &lt; 2e-16 ***\nORIGIN_SZQTSZ04      -1.2216614  0.0084050  -145.349  &lt; 2e-16 ***\nORIGIN_SZQTSZ05      -0.7219952  0.0072360   -99.778  &lt; 2e-16 ***\nORIGIN_SZQTSZ06      -0.6729363  0.0076658   -87.784  &lt; 2e-16 ***\nORIGIN_SZQTSZ07      -1.4497690  0.0109365  -132.563  &lt; 2e-16 ***\nORIGIN_SZQTSZ08      -0.2770151  0.0070193   -39.465  &lt; 2e-16 ***\nORIGIN_SZQTSZ09      -0.6157554  0.0078739   -78.202  &lt; 2e-16 ***\nORIGIN_SZQTSZ10      -0.3091547  0.0075471   -40.963  &lt; 2e-16 ***\nORIGIN_SZQTSZ11      -1.9698881  0.0151247  -130.243  &lt; 2e-16 ***\nORIGIN_SZQTSZ12      -2.6449643  0.0205857  -128.485  &lt; 2e-16 ***\nORIGIN_SZQTSZ13      -0.3754107  0.0088433   -42.452  &lt; 2e-16 ***\nORIGIN_SZQTSZ14      -1.6537473  0.0134378  -123.067  &lt; 2e-16 ***\nORIGIN_SZQTSZ15      -0.3435351  0.0131956   -26.034  &lt; 2e-16 ***\nORIGIN_SZRCSZ01      -1.7104390  0.0141179  -121.154  &lt; 2e-16 ***\nORIGIN_SZRCSZ06      -1.1250727  0.0094909  -118.542  &lt; 2e-16 ***\nORIGIN_SZRVSZ01      -3.0220116  0.0339694   -88.963  &lt; 2e-16 ***\nORIGIN_SZRVSZ02      -3.6040075  0.0297641  -121.086  &lt; 2e-16 ***\nORIGIN_SZRVSZ03      -3.2345594  0.0259149  -124.814  &lt; 2e-16 ***\nORIGIN_SZRVSZ04      -3.6900313  0.0575908   -64.073  &lt; 2e-16 ***\nORIGIN_SZRVSZ05      -2.9527570  0.0178582  -165.344  &lt; 2e-16 ***\nORIGIN_SZSBSZ01       0.0238445  0.0078563     3.035 0.002405 ** \nORIGIN_SZSBSZ02      -0.5780602  0.0093054   -62.121  &lt; 2e-16 ***\nORIGIN_SZSBSZ03       0.8961719  0.0054586   164.175  &lt; 2e-16 ***\nORIGIN_SZSBSZ04       0.8421798  0.0061888   136.080  &lt; 2e-16 ***\nORIGIN_SZSBSZ05      -0.1682984  0.0078342   -21.482  &lt; 2e-16 ***\nORIGIN_SZSBSZ06      -1.1482701  0.0196421   -58.460  &lt; 2e-16 ***\nORIGIN_SZSBSZ07      -0.8830317  0.0160709   -54.946  &lt; 2e-16 ***\nORIGIN_SZSBSZ08      -1.1039492  0.0174602   -63.226  &lt; 2e-16 ***\nORIGIN_SZSBSZ09      -0.5946691  0.0101961   -58.323  &lt; 2e-16 ***\nORIGIN_SZSESZ02       1.1144933  0.0050948   218.749  &lt; 2e-16 ***\nORIGIN_SZSESZ03       1.1058963  0.0049026   225.574  &lt; 2e-16 ***\nORIGIN_SZSESZ04       0.7427975  0.0056948   130.433  &lt; 2e-16 ***\nORIGIN_SZSESZ05      -0.2812684  0.0069596   -40.414  &lt; 2e-16 ***\nORIGIN_SZSESZ06       0.8168315  0.0055800   146.387  &lt; 2e-16 ***\nORIGIN_SZSESZ07      -2.2842043  0.0231232   -98.784  &lt; 2e-16 ***\nORIGIN_SZSGSZ01      -0.7313790  0.0098957   -73.909  &lt; 2e-16 ***\nORIGIN_SZSGSZ02      -1.1185406  0.0110919  -100.843  &lt; 2e-16 ***\nORIGIN_SZSGSZ03       0.1752618  0.0060508    28.965  &lt; 2e-16 ***\nORIGIN_SZSGSZ04       0.3764395  0.0056165    67.023  &lt; 2e-16 ***\nORIGIN_SZSGSZ05      -1.7203916  0.0118945  -144.637  &lt; 2e-16 ***\nORIGIN_SZSGSZ06       0.4630857  0.0052886    87.563  &lt; 2e-16 ***\nORIGIN_SZSGSZ07      -0.7051233  0.0073133   -96.417  &lt; 2e-16 ***\nORIGIN_SZSKSZ01       0.2053928  0.0100710    20.395  &lt; 2e-16 ***\nORIGIN_SZSKSZ02       1.2630428  0.0063490   198.935  &lt; 2e-16 ***\nORIGIN_SZSKSZ03      -0.3035297  0.0096788   -31.360  &lt; 2e-16 ***\nORIGIN_SZSKSZ04      -1.7952886  0.0359225   -49.977  &lt; 2e-16 ***\nORIGIN_SZSKSZ05      -0.3836861  0.0176686   -21.716  &lt; 2e-16 ***\nORIGIN_SZSLSZ01      -2.5916326  0.0348001   -74.472  &lt; 2e-16 ***\nORIGIN_SZSLSZ04      -0.2251549  0.0088517   -25.436  &lt; 2e-16 ***\nORIGIN_SZSRSZ01      -2.9590365  0.0173638  -170.414  &lt; 2e-16 ***\nORIGIN_SZTHSZ01      -1.9639893  0.0570321   -34.437  &lt; 2e-16 ***\nORIGIN_SZTHSZ03      -1.7281304  0.0272797   -63.349  &lt; 2e-16 ***\nORIGIN_SZTHSZ04      -2.7837906  0.0343179   -81.118  &lt; 2e-16 ***\nORIGIN_SZTHSZ06      -2.1800693  0.0205491  -106.091  &lt; 2e-16 ***\nORIGIN_SZTMSZ01       0.8228136  0.0066824   123.131  &lt; 2e-16 ***\nORIGIN_SZTMSZ02       2.3174781  0.0044978   515.243  &lt; 2e-16 ***\nORIGIN_SZTMSZ03       1.7061757  0.0048615   350.957  &lt; 2e-16 ***\nORIGIN_SZTMSZ04       1.2407899  0.0058389   212.504  &lt; 2e-16 ***\nORIGIN_SZTMSZ05      -0.1000526  0.0124079    -8.064 7.41e-16 ***\nORIGIN_SZTNSZ01      -2.0347519  0.0139596  -145.760  &lt; 2e-16 ***\nORIGIN_SZTNSZ02      -1.8682671  0.0107901  -173.146  &lt; 2e-16 ***\nORIGIN_SZTNSZ03      -2.1737183  0.0146759  -148.115  &lt; 2e-16 ***\nORIGIN_SZTNSZ04      -0.5006452  0.0081501   -61.428  &lt; 2e-16 ***\nORIGIN_SZTPSZ01      -0.6722487  0.0075606   -88.914  &lt; 2e-16 ***\nORIGIN_SZTPSZ02       0.4552916  0.0050191    90.711  &lt; 2e-16 ***\nORIGIN_SZTPSZ03      -0.7865781  0.0072250  -108.869  &lt; 2e-16 ***\nORIGIN_SZTPSZ04      -0.7049044  0.0066456  -106.071  &lt; 2e-16 ***\nORIGIN_SZTPSZ05      -0.5574925  0.0070366   -79.227  &lt; 2e-16 ***\nORIGIN_SZTPSZ06      -0.4247282  0.0068709   -61.815  &lt; 2e-16 ***\nORIGIN_SZTPSZ07      -0.2846984  0.0071030   -40.081  &lt; 2e-16 ***\nORIGIN_SZTPSZ08      -1.0898051  0.0110046   -99.031  &lt; 2e-16 ***\nORIGIN_SZTPSZ09      -0.8092746  0.0079160  -102.232  &lt; 2e-16 ***\nORIGIN_SZTPSZ10      -0.9332072  0.0086809  -107.502  &lt; 2e-16 ***\nORIGIN_SZTPSZ11      -0.0421981  0.0064343    -6.558 5.44e-11 ***\nORIGIN_SZTPSZ12      -0.6330081  0.0078324   -80.819  &lt; 2e-16 ***\nORIGIN_SZTSSZ01      -1.7650409  0.0517357   -34.116  &lt; 2e-16 ***\nORIGIN_SZTSSZ02       1.1707267  0.0094178   124.310  &lt; 2e-16 ***\nORIGIN_SZTSSZ03       0.6581679  0.0095894    68.635  &lt; 2e-16 ***\nORIGIN_SZTSSZ04       0.8736493  0.0104965    83.233  &lt; 2e-16 ***\nORIGIN_SZTSSZ05       0.0957248  0.0178709     5.356 8.49e-08 ***\nORIGIN_SZTSSZ06       1.7581609  0.0206810    85.013  &lt; 2e-16 ***\nORIGIN_SZWCSZ01       0.8097950  0.0105622    76.669  &lt; 2e-16 ***\nORIGIN_SZWCSZ02      -1.9966163  0.0345747   -57.748  &lt; 2e-16 ***\nORIGIN_SZWCSZ03      -5.0687420  0.1474971   -34.365  &lt; 2e-16 ***\nORIGIN_SZWDSZ01       1.4926003  0.0047216   316.124  &lt; 2e-16 ***\nORIGIN_SZWDSZ02       0.9916597  0.0055755   177.859  &lt; 2e-16 ***\nORIGIN_SZWDSZ03       1.5918065  0.0052180   305.062  &lt; 2e-16 ***\nORIGIN_SZWDSZ04       1.3717152  0.0060516   226.669  &lt; 2e-16 ***\nORIGIN_SZWDSZ05       0.6700111  0.0062287   107.569  &lt; 2e-16 ***\nORIGIN_SZWDSZ06       0.8115996  0.0060947   133.165  &lt; 2e-16 ***\nORIGIN_SZWDSZ07      -0.6488914  0.0093567   -69.351  &lt; 2e-16 ***\nORIGIN_SZWDSZ08      -0.3610234  0.0096440   -37.435  &lt; 2e-16 ***\nORIGIN_SZWDSZ09       1.4445461  0.0052279   276.317  &lt; 2e-16 ***\nORIGIN_SZYSSZ01      -0.2039272  0.0069548   -29.322  &lt; 2e-16 ***\nORIGIN_SZYSSZ02       0.8707707  0.0058957   147.697  &lt; 2e-16 ***\nORIGIN_SZYSSZ03       1.8348842  0.0050377   364.231  &lt; 2e-16 ***\nORIGIN_SZYSSZ04       1.0780641  0.0052960   203.564  &lt; 2e-16 ***\nORIGIN_SZYSSZ05       0.3222765  0.0069700    46.237  &lt; 2e-16 ***\nORIGIN_SZYSSZ06      -0.4424689  0.0124866   -35.435  &lt; 2e-16 ***\nORIGIN_SZYSSZ07      -1.0267883  0.0155821   -65.895  &lt; 2e-16 ***\nORIGIN_SZYSSZ08       0.1833117  0.0070935    25.842  &lt; 2e-16 ***\nORIGIN_SZYSSZ09       1.0766070  0.0050451   213.396  &lt; 2e-16 ***\nlog(DESTIN_AGE25_64)  0.0295428  0.0001051   280.998  &lt; 2e-16 ***\nlog(dist)            -1.7024691  0.0004625 -3681.042  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 36117615  on 14273  degrees of freedom\nResidual deviance: 12983718  on 13993  degrees of freedom\nAIC: 13068835\n\nNumber of Fisher Scoring iterations: 6\n\n\nWe can examine how the constraints hold for destinations this time.\n\nCalcRSquared(orcSIM$data$TRIPS, orcSIM$fitted.values)\n\n[1] 0.4029115\n\n\n\n\n8.7 Destination constrained\nIn this section, we will fit a destination constrained SIM by using the code chunk below.\nThe general formula of Destination Constrained Spatial Interaction Model\n\ndecSIM &lt;- glm(formula = TRIPS ~ \n                DESTIN_SZ + \n                log(ORIGIN_AGE25_64) + \n                log(dist),\n              family = poisson(link = \"log\"),\n              data = SIM_data,\n              na.action = na.exclude)\nsummary(decSIM)\n\n\nCall:\nglm(formula = TRIPS ~ DESTIN_SZ + log(ORIGIN_AGE25_64) + log(dist), \n    family = poisson(link = \"log\"), data = SIM_data, na.action = na.exclude)\n\nCoefficients:\n                       Estimate Std. Error   z value Pr(&gt;|z|)    \n(Intercept)          19.4822997  0.0050784  3836.298  &lt; 2e-16 ***\nDESTIN_SZAMSZ02       0.1263056  0.0049743    25.392  &lt; 2e-16 ***\nDESTIN_SZAMSZ03       0.0421788  0.0049859     8.460  &lt; 2e-16 ***\nDESTIN_SZAMSZ04      -1.1668479  0.0074254  -157.143  &lt; 2e-16 ***\nDESTIN_SZAMSZ05      -1.2586639  0.0075854  -165.931  &lt; 2e-16 ***\nDESTIN_SZAMSZ06      -1.1414791  0.0073474  -155.359  &lt; 2e-16 ***\nDESTIN_SZAMSZ07      -1.5565804  0.0109476  -142.185  &lt; 2e-16 ***\nDESTIN_SZAMSZ08      -0.3990754  0.0074159   -53.813  &lt; 2e-16 ***\nDESTIN_SZAMSZ09      -1.0109118  0.0076802  -131.626  &lt; 2e-16 ***\nDESTIN_SZAMSZ10       0.0159285  0.0051765     3.077  0.00209 ** \nDESTIN_SZAMSZ11      -0.3653273  0.0094866   -38.510  &lt; 2e-16 ***\nDESTIN_SZAMSZ12       0.5297606  0.0053243    99.500  &lt; 2e-16 ***\nDESTIN_SZBDSZ01       1.0394822  0.0044226   235.037  &lt; 2e-16 ***\nDESTIN_SZBDSZ02       0.1956964  0.0059564    32.855  &lt; 2e-16 ***\nDESTIN_SZBDSZ03       0.3209267  0.0053718    59.742  &lt; 2e-16 ***\nDESTIN_SZBDSZ04       1.2429874  0.0043104   288.370  &lt; 2e-16 ***\nDESTIN_SZBDSZ05       0.8535842  0.0046360   184.122  &lt; 2e-16 ***\nDESTIN_SZBDSZ06       0.5181443  0.0053736    96.423  &lt; 2e-16 ***\nDESTIN_SZBDSZ07      -0.5849371  0.0110468   -52.951  &lt; 2e-16 ***\nDESTIN_SZBDSZ08      -1.2871050  0.0128623  -100.068  &lt; 2e-16 ***\nDESTIN_SZBKSZ01      -1.0633560  0.0077771  -136.730  &lt; 2e-16 ***\nDESTIN_SZBKSZ02      -0.4065316  0.0066712   -60.938  &lt; 2e-16 ***\nDESTIN_SZBKSZ03      -0.6815674  0.0066509  -102.477  &lt; 2e-16 ***\nDESTIN_SZBKSZ04      -0.4185485  0.0058306   -71.785  &lt; 2e-16 ***\nDESTIN_SZBKSZ05      -0.8887654  0.0073867  -120.319  &lt; 2e-16 ***\nDESTIN_SZBKSZ06      -0.9436078  0.0068625  -137.501  &lt; 2e-16 ***\nDESTIN_SZBKSZ07      -0.0067325  0.0048408    -1.391  0.16430    \nDESTIN_SZBKSZ08      -1.2680903  0.0079177  -160.160  &lt; 2e-16 ***\nDESTIN_SZBKSZ09      -0.0350151  0.0054287    -6.450 1.12e-10 ***\nDESTIN_SZBLSZ01      -0.3045203  0.0081978   -37.146  &lt; 2e-16 ***\nDESTIN_SZBLSZ02       0.6432424  0.0074449    86.400  &lt; 2e-16 ***\nDESTIN_SZBLSZ03       1.9595113  0.0084705   231.333  &lt; 2e-16 ***\nDESTIN_SZBLSZ04       0.0149756  0.0172081     0.870  0.38415    \nDESTIN_SZBMSZ01      -0.0378127  0.0055294    -6.838 8.00e-12 ***\nDESTIN_SZBMSZ02      -0.8458055  0.0054043  -156.505  &lt; 2e-16 ***\nDESTIN_SZBMSZ03      -1.1334399  0.0063720  -177.878  &lt; 2e-16 ***\nDESTIN_SZBMSZ04      -1.1164759  0.0057743  -193.353  &lt; 2e-16 ***\nDESTIN_SZBMSZ05      -1.1078742  0.0078703  -140.766  &lt; 2e-16 ***\nDESTIN_SZBMSZ06      -2.2787234  0.0155126  -146.895  &lt; 2e-16 ***\nDESTIN_SZBMSZ07      -0.2739089  0.0051924   -52.752  &lt; 2e-16 ***\nDESTIN_SZBMSZ08      -1.6825978  0.0071842  -234.209  &lt; 2e-16 ***\nDESTIN_SZBMSZ09      -3.0047801  0.0159980  -187.823  &lt; 2e-16 ***\nDESTIN_SZBMSZ10      -2.2232689  0.0096907  -229.423  &lt; 2e-16 ***\nDESTIN_SZBMSZ11      -1.9657136  0.0086445  -227.394  &lt; 2e-16 ***\nDESTIN_SZBMSZ12      -1.5359286  0.0089658  -171.310  &lt; 2e-16 ***\nDESTIN_SZBMSZ13      -0.5657561  0.0059960   -94.355  &lt; 2e-16 ***\nDESTIN_SZBMSZ14      -1.6904858  0.0084858  -199.214  &lt; 2e-16 ***\nDESTIN_SZBMSZ15      -1.5268383  0.0079959  -190.953  &lt; 2e-16 ***\nDESTIN_SZBMSZ16      -2.2045600  0.0130872  -168.452  &lt; 2e-16 ***\nDESTIN_SZBMSZ17      -2.2992381  0.0184895  -124.353  &lt; 2e-16 ***\nDESTIN_SZBPSZ01      -0.8549497  0.0065168  -131.191  &lt; 2e-16 ***\nDESTIN_SZBPSZ02      -1.7470549  0.0095751  -182.457  &lt; 2e-16 ***\nDESTIN_SZBPSZ03      -1.4015145  0.0090888  -154.203  &lt; 2e-16 ***\nDESTIN_SZBPSZ04      -0.5250632  0.0066496   -78.962  &lt; 2e-16 ***\nDESTIN_SZBPSZ05       0.3413171  0.0046404    73.553  &lt; 2e-16 ***\nDESTIN_SZBPSZ06      -0.8569188  0.0090795   -94.380  &lt; 2e-16 ***\nDESTIN_SZBPSZ07      -0.0751284  0.0089704    -8.375  &lt; 2e-16 ***\nDESTIN_SZBSSZ01       0.1015228  0.0055735    18.215  &lt; 2e-16 ***\nDESTIN_SZBSSZ02      -0.7066412  0.0063845  -110.682  &lt; 2e-16 ***\nDESTIN_SZBSSZ03       0.1622730  0.0046689    34.756  &lt; 2e-16 ***\nDESTIN_SZBTSZ01       0.5470615  0.0047984   114.009  &lt; 2e-16 ***\nDESTIN_SZBTSZ02      -0.1393371  0.0078266   -17.803  &lt; 2e-16 ***\nDESTIN_SZBTSZ03       0.1474771  0.0059428    24.816  &lt; 2e-16 ***\nDESTIN_SZBTSZ04      -1.2857827  0.0122000  -105.392  &lt; 2e-16 ***\nDESTIN_SZBTSZ05      -0.2629188  0.0081769   -32.154  &lt; 2e-16 ***\nDESTIN_SZBTSZ06      -0.8319920  0.0081401  -102.209  &lt; 2e-16 ***\nDESTIN_SZBTSZ07      -1.8829448  0.0121227  -155.324  &lt; 2e-16 ***\nDESTIN_SZBTSZ08      -1.5732123  0.0116752  -134.748  &lt; 2e-16 ***\nDESTIN_SZCBSZ01      -3.5334327  0.3333510   -10.600  &lt; 2e-16 ***\nDESTIN_SZCCSZ01      -0.2129306  0.0093782   -22.705  &lt; 2e-16 ***\nDESTIN_SZCHSZ01      -0.1494972  0.0113078   -13.221  &lt; 2e-16 ***\nDESTIN_SZCHSZ02       0.0041774  0.0063195     0.661  0.50860    \nDESTIN_SZCHSZ03       2.5565450  0.0046495   549.857  &lt; 2e-16 ***\nDESTIN_SZCKSZ01       0.0489719  0.0053801     9.102  &lt; 2e-16 ***\nDESTIN_SZCKSZ02      -0.3548993  0.0060671   -58.496  &lt; 2e-16 ***\nDESTIN_SZCKSZ03       0.5386351  0.0044913   119.928  &lt; 2e-16 ***\nDESTIN_SZCKSZ04      -0.4425512  0.0073837   -59.936  &lt; 2e-16 ***\nDESTIN_SZCKSZ05      -0.4092591  0.0077267   -52.967  &lt; 2e-16 ***\nDESTIN_SZCKSZ06       0.2207041  0.0074252    29.724  &lt; 2e-16 ***\nDESTIN_SZCLSZ01       0.2851460  0.0052362    54.457  &lt; 2e-16 ***\nDESTIN_SZCLSZ02      -1.9270528  0.0147688  -130.482  &lt; 2e-16 ***\nDESTIN_SZCLSZ03      -0.6266521  0.0086780   -72.212  &lt; 2e-16 ***\nDESTIN_SZCLSZ04      -0.1335581  0.0054216   -24.634  &lt; 2e-16 ***\nDESTIN_SZCLSZ05      -0.8912963  0.0096015   -92.829  &lt; 2e-16 ***\nDESTIN_SZCLSZ06       0.1781234  0.0048150    36.993  &lt; 2e-16 ***\nDESTIN_SZCLSZ07      -0.5609619  0.0062277   -90.075  &lt; 2e-16 ***\nDESTIN_SZCLSZ08      -0.3875308  0.0068390   -56.665  &lt; 2e-16 ***\nDESTIN_SZCLSZ09       0.2539453  0.0072623    34.968  &lt; 2e-16 ***\nDESTIN_SZDTSZ02      -2.5036295  0.0373421   -67.046  &lt; 2e-16 ***\nDESTIN_SZDTSZ03      -0.8956407  0.0149971   -59.721  &lt; 2e-16 ***\nDESTIN_SZDTSZ13      -1.6562176  0.0175441   -94.403  &lt; 2e-16 ***\nDESTIN_SZGLSZ01      -0.2716152  0.0056553   -48.029  &lt; 2e-16 ***\nDESTIN_SZGLSZ02      -0.1735665  0.0055548   -31.246  &lt; 2e-16 ***\nDESTIN_SZGLSZ03       0.7029507  0.0044934   156.441  &lt; 2e-16 ***\nDESTIN_SZGLSZ04       0.5788027  0.0045449   127.351  &lt; 2e-16 ***\nDESTIN_SZGLSZ05       0.6865291  0.0045131   152.118  &lt; 2e-16 ***\nDESTIN_SZHGSZ01       0.3275950  0.0043866    74.681  &lt; 2e-16 ***\nDESTIN_SZHGSZ02      -0.6326974  0.0063517   -99.610  &lt; 2e-16 ***\nDESTIN_SZHGSZ03      -1.0597982  0.0073914  -143.382  &lt; 2e-16 ***\nDESTIN_SZHGSZ04      -0.2267013  0.0052178   -43.448  &lt; 2e-16 ***\nDESTIN_SZHGSZ05      -0.3063050  0.0055452   -55.238  &lt; 2e-16 ***\nDESTIN_SZHGSZ06      -0.7483961  0.0065544  -114.182  &lt; 2e-16 ***\nDESTIN_SZHGSZ07       0.1096958  0.0051309    21.379  &lt; 2e-16 ***\nDESTIN_SZHGSZ08      -0.1374201  0.0056692   -24.240  &lt; 2e-16 ***\nDESTIN_SZHGSZ09       0.0775400  0.0060230    12.874  &lt; 2e-16 ***\nDESTIN_SZHGSZ10      -3.3017475  0.0289292  -114.132  &lt; 2e-16 ***\nDESTIN_SZJESZ01      -0.0489065  0.0057246    -8.543  &lt; 2e-16 ***\nDESTIN_SZJESZ02      -0.5101614  0.0060074   -84.921  &lt; 2e-16 ***\nDESTIN_SZJESZ03      -0.5328921  0.0064129   -83.097  &lt; 2e-16 ***\nDESTIN_SZJESZ04      -0.7348953  0.0082249   -89.351  &lt; 2e-16 ***\nDESTIN_SZJESZ05      -1.0864570  0.0111740   -97.231  &lt; 2e-16 ***\nDESTIN_SZJESZ06       0.2407920  0.0046801    51.451  &lt; 2e-16 ***\nDESTIN_SZJESZ07      -1.1523093  0.0090103  -127.888  &lt; 2e-16 ***\nDESTIN_SZJESZ08      -0.4627356  0.0094529   -48.952  &lt; 2e-16 ***\nDESTIN_SZJESZ09       0.0528616  0.0068126     7.759 8.53e-15 ***\nDESTIN_SZJESZ10       1.0240660  0.0084045   121.848  &lt; 2e-16 ***\nDESTIN_SZJESZ11       0.7875517  0.0076251   103.284  &lt; 2e-16 ***\nDESTIN_SZJWSZ01      -0.1533418  0.0076198   -20.124  &lt; 2e-16 ***\nDESTIN_SZJWSZ02      -0.0011019  0.0059389    -0.186  0.85280    \nDESTIN_SZJWSZ03       0.9063789  0.0046747   193.892  &lt; 2e-16 ***\nDESTIN_SZJWSZ04       0.7019286  0.0049743   141.112  &lt; 2e-16 ***\nDESTIN_SZJWSZ05      -0.5197057  0.0072971   -71.220  &lt; 2e-16 ***\nDESTIN_SZJWSZ06       0.3350986  0.0061171    54.780  &lt; 2e-16 ***\nDESTIN_SZJWSZ07      -0.5961960  0.0328336   -18.158  &lt; 2e-16 ***\nDESTIN_SZJWSZ08       0.8054662  0.0056006   143.819  &lt; 2e-16 ***\nDESTIN_SZJWSZ09       1.5860146  0.0040282   393.723  &lt; 2e-16 ***\nDESTIN_SZKLSZ01      -0.6500838  0.0063560  -102.279  &lt; 2e-16 ***\nDESTIN_SZKLSZ02      -0.7039434  0.0064465  -109.197  &lt; 2e-16 ***\nDESTIN_SZKLSZ03      -1.1972384  0.0075577  -158.413  &lt; 2e-16 ***\nDESTIN_SZKLSZ04      -1.7172228  0.0097573  -175.993  &lt; 2e-16 ***\nDESTIN_SZKLSZ05      -0.6042386  0.0093730   -64.466  &lt; 2e-16 ***\nDESTIN_SZKLSZ06      -3.0201496  0.0389503   -77.539  &lt; 2e-16 ***\nDESTIN_SZKLSZ07      -1.1522413  0.0076607  -150.409  &lt; 2e-16 ***\nDESTIN_SZKLSZ08      -0.6977825  0.0057610  -121.122  &lt; 2e-16 ***\nDESTIN_SZLKSZ01      -0.6895952  0.0268661   -25.668  &lt; 2e-16 ***\nDESTIN_SZMDSZ01      -0.7155951  0.0228203   -31.358  &lt; 2e-16 ***\nDESTIN_SZMDSZ02      -0.8153643  0.0123003   -66.288  &lt; 2e-16 ***\nDESTIN_SZMDSZ03      -2.7745226  0.0301326   -92.077  &lt; 2e-16 ***\nDESTIN_SZMPSZ01      -0.5492095  0.0087198   -62.984  &lt; 2e-16 ***\nDESTIN_SZMPSZ02      -0.6104744  0.0069346   -88.033  &lt; 2e-16 ***\nDESTIN_SZMPSZ03       0.2775047  0.0054964    50.489  &lt; 2e-16 ***\nDESTIN_SZMUSZ02      -2.6322870  0.0214943  -122.464  &lt; 2e-16 ***\nDESTIN_SZNTSZ01      -4.0762008  0.0531046   -76.758  &lt; 2e-16 ***\nDESTIN_SZNTSZ02      -1.9765545  0.0125659  -157.296  &lt; 2e-16 ***\nDESTIN_SZNTSZ03      -1.4563069  0.0085433  -170.462  &lt; 2e-16 ***\nDESTIN_SZNTSZ05      -2.0125598  0.0270737   -74.336  &lt; 2e-16 ***\nDESTIN_SZNTSZ06      -3.0145357  0.0504986   -59.695  &lt; 2e-16 ***\nDESTIN_SZNVSZ01      -0.4693625  0.0053866   -87.135  &lt; 2e-16 ***\nDESTIN_SZNVSZ02      -0.4525631  0.0060428   -74.894  &lt; 2e-16 ***\nDESTIN_SZNVSZ03      -0.4821492  0.0064725   -74.492  &lt; 2e-16 ***\nDESTIN_SZNVSZ04      -1.8929756  0.0128397  -147.432  &lt; 2e-16 ***\nDESTIN_SZNVSZ05      -1.4501752  0.0099737  -145.400  &lt; 2e-16 ***\nDESTIN_SZPGSZ01      -1.2305867  0.0174321   -70.593  &lt; 2e-16 ***\nDESTIN_SZPGSZ02      -0.8232919  0.0080153  -102.715  &lt; 2e-16 ***\nDESTIN_SZPGSZ03       0.2138480  0.0050850    42.054  &lt; 2e-16 ***\nDESTIN_SZPGSZ04       0.1045757  0.0053579    19.518  &lt; 2e-16 ***\nDESTIN_SZPGSZ05      -0.7542450  0.0088883   -84.858  &lt; 2e-16 ***\nDESTIN_SZPLSZ01      -0.0098642  0.0080428    -1.226  0.22003    \nDESTIN_SZPLSZ02      -1.2630412  0.0152594   -82.771  &lt; 2e-16 ***\nDESTIN_SZPLSZ03      -0.1554479  0.0108611   -14.312  &lt; 2e-16 ***\nDESTIN_SZPLSZ04      -1.5505819  0.0114768  -135.105  &lt; 2e-16 ***\nDESTIN_SZPLSZ05      -0.2417805  0.0130391   -18.543  &lt; 2e-16 ***\nDESTIN_SZPNSZ01       0.7926715  0.0073628   107.659  &lt; 2e-16 ***\nDESTIN_SZPNSZ02       2.1914920  0.0073537   298.013  &lt; 2e-16 ***\nDESTIN_SZPNSZ03       1.0246845  0.0086874   117.951  &lt; 2e-16 ***\nDESTIN_SZPNSZ04       2.5522612  0.0091789   278.057  &lt; 2e-16 ***\nDESTIN_SZPNSZ05       1.7995301  0.0138562   129.872  &lt; 2e-16 ***\nDESTIN_SZPRSZ01      -0.6576686  0.0096037   -68.481  &lt; 2e-16 ***\nDESTIN_SZPRSZ02       0.3113532  0.0059851    52.021  &lt; 2e-16 ***\nDESTIN_SZPRSZ03       0.9255296  0.0044779   206.687  &lt; 2e-16 ***\nDESTIN_SZPRSZ04      -0.0028578  0.0093218    -0.307  0.75917    \nDESTIN_SZPRSZ05       0.2457863  0.0058261    42.187  &lt; 2e-16 ***\nDESTIN_SZPRSZ06       0.3692137  0.0064542    57.205  &lt; 2e-16 ***\nDESTIN_SZPRSZ07      -1.6733306  0.0138440  -120.871  &lt; 2e-16 ***\nDESTIN_SZPRSZ08      -0.2221048  0.0074846   -29.675  &lt; 2e-16 ***\nDESTIN_SZQTSZ01      -1.0185488  0.0093179  -109.311  &lt; 2e-16 ***\nDESTIN_SZQTSZ02      -1.2802688  0.0081670  -156.761  &lt; 2e-16 ***\nDESTIN_SZQTSZ03      -1.3322708  0.0079106  -168.415  &lt; 2e-16 ***\nDESTIN_SZQTSZ04      -1.1803631  0.0077366  -152.568  &lt; 2e-16 ***\nDESTIN_SZQTSZ05      -1.2215818  0.0072829  -167.734  &lt; 2e-16 ***\nDESTIN_SZQTSZ06      -1.3213145  0.0074858  -176.509  &lt; 2e-16 ***\nDESTIN_SZQTSZ07      -1.6426306  0.0123347  -133.171  &lt; 2e-16 ***\nDESTIN_SZQTSZ08      -0.2224169  0.0058405   -38.082  &lt; 2e-16 ***\nDESTIN_SZQTSZ09      -0.8142678  0.0069796  -116.665  &lt; 2e-16 ***\nDESTIN_SZQTSZ10      -0.1090496  0.0062573   -17.428  &lt; 2e-16 ***\nDESTIN_SZQTSZ11      -0.0108951  0.0061145    -1.782  0.07477 .  \nDESTIN_SZQTSZ12      -0.8582515  0.0090243   -95.105  &lt; 2e-16 ***\nDESTIN_SZQTSZ13       0.1834409  0.0065231    28.122  &lt; 2e-16 ***\nDESTIN_SZQTSZ14       0.1994454  0.0073615    27.093  &lt; 2e-16 ***\nDESTIN_SZQTSZ15       0.6740197  0.0088699    75.990  &lt; 2e-16 ***\nDESTIN_SZRCSZ01      -0.7746427  0.0079375   -97.593  &lt; 2e-16 ***\nDESTIN_SZRCSZ06      -1.4394098  0.0209931   -68.566  &lt; 2e-16 ***\nDESTIN_SZRVSZ01      -2.6060495  0.0175759  -148.274  &lt; 2e-16 ***\nDESTIN_SZRVSZ02      -2.5823769  0.0354706   -72.803  &lt; 2e-16 ***\nDESTIN_SZRVSZ03      -2.5890601  0.0152644  -169.614  &lt; 2e-16 ***\nDESTIN_SZRVSZ04      -2.2277482  0.0165661  -134.477  &lt; 2e-16 ***\nDESTIN_SZRVSZ05      -3.8610445  0.0298251  -129.456  &lt; 2e-16 ***\nDESTIN_SZSBSZ01      -1.2035529  0.0103954  -115.777  &lt; 2e-16 ***\nDESTIN_SZSBSZ02      -1.0267199  0.0085239  -120.452  &lt; 2e-16 ***\nDESTIN_SZSBSZ03       0.5977382  0.0050336   118.750  &lt; 2e-16 ***\nDESTIN_SZSBSZ04       0.5362769  0.0060573    88.534  &lt; 2e-16 ***\nDESTIN_SZSBSZ05      -1.0440525  0.0089622  -116.495  &lt; 2e-16 ***\nDESTIN_SZSBSZ06      -1.3939595  0.0246679   -56.509  &lt; 2e-16 ***\nDESTIN_SZSBSZ07       0.1029116  0.0235414     4.372 1.23e-05 ***\nDESTIN_SZSBSZ08       1.3564902  0.0060529   224.105  &lt; 2e-16 ***\nDESTIN_SZSBSZ09       0.4573712  0.0056585    80.829  &lt; 2e-16 ***\nDESTIN_SZSESZ02      -0.1553609  0.0056716   -27.393  &lt; 2e-16 ***\nDESTIN_SZSESZ03       0.5412776  0.0043801   123.576  &lt; 2e-16 ***\nDESTIN_SZSESZ04      -0.6382091  0.0065411   -97.568  &lt; 2e-16 ***\nDESTIN_SZSESZ05      -0.3332093  0.0055002   -60.581  &lt; 2e-16 ***\nDESTIN_SZSESZ06      -0.3085951  0.0072340   -42.659  &lt; 2e-16 ***\nDESTIN_SZSESZ07      -2.6237684  0.0245753  -106.764  &lt; 2e-16 ***\nDESTIN_SZSGSZ01      -0.1062372  0.0066634   -15.943  &lt; 2e-16 ***\nDESTIN_SZSGSZ02      -0.0475568  0.0058908    -8.073 6.85e-16 ***\nDESTIN_SZSGSZ03      -0.2118402  0.0055056   -38.477  &lt; 2e-16 ***\nDESTIN_SZSGSZ04      -0.1099618  0.0054841   -20.051  &lt; 2e-16 ***\nDESTIN_SZSGSZ05      -2.1556963  0.0113821  -189.394  &lt; 2e-16 ***\nDESTIN_SZSGSZ06       0.4416352  0.0043842   100.734  &lt; 2e-16 ***\nDESTIN_SZSGSZ07      -0.3949335  0.0059250   -66.655  &lt; 2e-16 ***\nDESTIN_SZSISZ01      -1.2847094  0.0288610   -44.514  &lt; 2e-16 ***\nDESTIN_SZSKSZ01       0.3089834  0.0082924    37.261  &lt; 2e-16 ***\nDESTIN_SZSKSZ02       1.4139309  0.0059981   235.729  &lt; 2e-16 ***\nDESTIN_SZSKSZ03       0.2427688  0.0067373    36.034  &lt; 2e-16 ***\nDESTIN_SZSKSZ04      -0.2527488  0.0161286   -15.671  &lt; 2e-16 ***\nDESTIN_SZSKSZ05       0.6046051  0.0122766    49.249  &lt; 2e-16 ***\nDESTIN_SZSLSZ01      -0.3927387  0.0099790   -39.356  &lt; 2e-16 ***\nDESTIN_SZSLSZ04      -0.5942110  0.0086225   -68.914  &lt; 2e-16 ***\nDESTIN_SZSRSZ01      -2.6855766  0.0138707  -193.615  &lt; 2e-16 ***\nDESTIN_SZTHSZ01      -3.2750084  0.0402668   -81.333  &lt; 2e-16 ***\nDESTIN_SZTHSZ03      -1.7964408  0.0261810   -68.616  &lt; 2e-16 ***\nDESTIN_SZTHSZ04      -2.6323994  0.0241831  -108.853  &lt; 2e-16 ***\nDESTIN_SZTHSZ06      -1.9444390  0.0166052  -117.098  &lt; 2e-16 ***\nDESTIN_SZTMSZ01       0.3856054  0.0063086    61.123  &lt; 2e-16 ***\nDESTIN_SZTMSZ02       1.8586526  0.0039229   473.790  &lt; 2e-16 ***\nDESTIN_SZTMSZ03       1.2601385  0.0044018   286.278  &lt; 2e-16 ***\nDESTIN_SZTMSZ04       1.5884327  0.0043362   366.316  &lt; 2e-16 ***\nDESTIN_SZTMSZ05       1.0377553  0.0063271   164.018  &lt; 2e-16 ***\nDESTIN_SZTNSZ01      -0.9954275  0.0080345  -123.895  &lt; 2e-16 ***\nDESTIN_SZTNSZ02      -2.1032696  0.0109228  -192.557  &lt; 2e-16 ***\nDESTIN_SZTNSZ03      -2.0044892  0.0129215  -155.128  &lt; 2e-16 ***\nDESTIN_SZTNSZ04      -0.9750326  0.0081677  -119.377  &lt; 2e-16 ***\nDESTIN_SZTPSZ01      -0.7788383  0.0068769  -113.254  &lt; 2e-16 ***\nDESTIN_SZTPSZ02       0.2866080  0.0042843    66.898  &lt; 2e-16 ***\nDESTIN_SZTPSZ03      -0.8749841  0.0065470  -133.646  &lt; 2e-16 ***\nDESTIN_SZTPSZ04      -1.6852792  0.0081488  -206.812  &lt; 2e-16 ***\nDESTIN_SZTPSZ05      -1.3721346  0.0068230  -201.104  &lt; 2e-16 ***\nDESTIN_SZTPSZ06      -0.7832133  0.0069164  -113.239  &lt; 2e-16 ***\nDESTIN_SZTPSZ07      -2.3109126  0.0130830  -176.635  &lt; 2e-16 ***\nDESTIN_SZTPSZ08      -1.6406531  0.0104897  -156.406  &lt; 2e-16 ***\nDESTIN_SZTPSZ09      -0.5636273  0.0076848   -73.343  &lt; 2e-16 ***\nDESTIN_SZTPSZ10      -1.5640843  0.0099984  -156.433  &lt; 2e-16 ***\nDESTIN_SZTPSZ11      -0.3700482  0.0059834   -61.846  &lt; 2e-16 ***\nDESTIN_SZTPSZ12      -0.8828228  0.0072302  -122.102  &lt; 2e-16 ***\nDESTIN_SZTSSZ01       0.3529526  0.0221887    15.907  &lt; 2e-16 ***\nDESTIN_SZTSSZ02       1.0265792  0.0153515    66.871  &lt; 2e-16 ***\nDESTIN_SZTSSZ03       1.9647347  0.0092388   212.662  &lt; 2e-16 ***\nDESTIN_SZTSSZ04       1.8649836  0.0089976   207.275  &lt; 2e-16 ***\nDESTIN_SZTSSZ05       2.8437058  0.0085738   331.673  &lt; 2e-16 ***\nDESTIN_SZTSSZ06       3.4238870  0.0161304   212.263  &lt; 2e-16 ***\nDESTIN_SZWCSZ01       2.9550693  0.0051690   571.689  &lt; 2e-16 ***\nDESTIN_SZWCSZ02      -0.8214103  0.0129213   -63.570  &lt; 2e-16 ***\nDESTIN_SZWCSZ03      -1.7393427  0.0347472   -50.057  &lt; 2e-16 ***\nDESTIN_SZWDSZ01       1.3424417  0.0039957   335.972  &lt; 2e-16 ***\nDESTIN_SZWDSZ02      -0.2103694  0.0068601   -30.666  &lt; 2e-16 ***\nDESTIN_SZWDSZ03       0.8268551  0.0051363   160.983  &lt; 2e-16 ***\nDESTIN_SZWDSZ04      -0.0643997  0.0079076    -8.144 3.82e-16 ***\nDESTIN_SZWDSZ05       0.0451985  0.0075732     5.968 2.40e-09 ***\nDESTIN_SZWDSZ06       0.6981330  0.0051936   134.423  &lt; 2e-16 ***\nDESTIN_SZWDSZ07      -0.0403233  0.0067749    -5.952 2.65e-09 ***\nDESTIN_SZWDSZ08       0.2850631  0.0069225    41.179  &lt; 2e-16 ***\nDESTIN_SZWDSZ09       1.3016106  0.0050365   258.433  &lt; 2e-16 ***\nDESTIN_SZYSSZ01       0.7598564  0.0044144   172.133  &lt; 2e-16 ***\nDESTIN_SZYSSZ02       0.2648061  0.0058239    45.469  &lt; 2e-16 ***\nDESTIN_SZYSSZ03      -0.0412163  0.0068337    -6.031 1.63e-09 ***\nDESTIN_SZYSSZ04      -0.0561054  0.0060829    -9.223  &lt; 2e-16 ***\nDESTIN_SZYSSZ05      -0.9970159  0.0121827   -81.839  &lt; 2e-16 ***\nDESTIN_SZYSSZ06      -1.3808376  0.0125738  -109.819  &lt; 2e-16 ***\nDESTIN_SZYSSZ07      -0.7128364  0.0165296   -43.125  &lt; 2e-16 ***\nDESTIN_SZYSSZ08       0.9409510  0.0045886   205.064  &lt; 2e-16 ***\nDESTIN_SZYSSZ09       0.3738436  0.0047971    77.930  &lt; 2e-16 ***\nlog(ORIGIN_AGE25_64)  0.1928847  0.0001667  1157.214  &lt; 2e-16 ***\nlog(dist)            -1.7828141  0.0004794 -3718.501  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 36117615  on 14273  degrees of freedom\nResidual deviance: 12319763  on 13992  degrees of freedom\nAIC: 12404881\n\nNumber of Fisher Scoring iterations: 7\n\n\nWe can examine how the constraints hold for destinations this time.\n\nCalcRSquared(decSIM$data$TRIPS, decSIM$fitted.values)\n\n[1] 0.496166\n\n\n\n\n8.8 Doubly constrained\nIn this section, we will fit a doubly constrained SIM by using the code chunk below.\nThe general formula of Doubly Constrained Spatial Interaction Model\n\ndbcSIM &lt;- glm(formula = TRIPS ~ \n                ORIGIN_SZ + \n                DESTIN_SZ + \n                log(dist),\n              family = poisson(link = \"log\"),\n              data = SIM_data,\n              na.action = na.exclude)\nsummary(dbcSIM)\n\n\nCall:\nglm(formula = TRIPS ~ ORIGIN_SZ + DESTIN_SZ + log(dist), family = poisson(link = \"log\"), \n    data = SIM_data, na.action = na.exclude)\n\nCoefficients:\n                  Estimate Std. Error   z value Pr(&gt;|z|)    \n(Intercept)     21.9587595  0.0066831  3285.715  &lt; 2e-16 ***\nORIGIN_SZAMSZ02  0.4778050  0.0054127    88.275  &lt; 2e-16 ***\nORIGIN_SZAMSZ03  0.2895973  0.0055517    52.163  &lt; 2e-16 ***\nORIGIN_SZAMSZ04 -0.2628080  0.0060720   -43.282  &lt; 2e-16 ***\nORIGIN_SZAMSZ05 -0.2631404  0.0069008   -38.132  &lt; 2e-16 ***\nORIGIN_SZAMSZ06  0.1722337  0.0062028    27.767  &lt; 2e-16 ***\nORIGIN_SZAMSZ07 -0.9883200  0.0111224   -88.859  &lt; 2e-16 ***\nORIGIN_SZAMSZ08 -0.4052821  0.0104095   -38.934  &lt; 2e-16 ***\nORIGIN_SZAMSZ09  0.0356290  0.0064816     5.497 3.86e-08 ***\nORIGIN_SZAMSZ10  0.4815569  0.0055521    86.735  &lt; 2e-16 ***\nORIGIN_SZAMSZ11 -1.4440079  0.0146079   -98.851  &lt; 2e-16 ***\nORIGIN_SZAMSZ12 -1.7862677  0.0128071  -139.475  &lt; 2e-16 ***\nORIGIN_SZBDSZ01  0.8653749  0.0054381   159.132  &lt; 2e-16 ***\nORIGIN_SZBDSZ02  0.0841000  0.0062834    13.385  &lt; 2e-16 ***\nORIGIN_SZBDSZ03  0.3158343  0.0057510    54.918  &lt; 2e-16 ***\nORIGIN_SZBDSZ04  1.4556701  0.0049986   291.215  &lt; 2e-16 ***\nORIGIN_SZBDSZ05  0.6363125  0.0057193   111.257  &lt; 2e-16 ***\nORIGIN_SZBDSZ06  0.6749341  0.0058650   115.078  &lt; 2e-16 ***\nORIGIN_SZBDSZ07 -1.2176407  0.0113698  -107.095  &lt; 2e-16 ***\nORIGIN_SZBDSZ08 -0.9803580  0.0105604   -92.833  &lt; 2e-16 ***\nORIGIN_SZBKSZ01 -0.2919642  0.0080763   -36.151  &lt; 2e-16 ***\nORIGIN_SZBKSZ02  0.4609570  0.0067997    67.791  &lt; 2e-16 ***\nORIGIN_SZBKSZ03  0.6273448  0.0065989    95.068  &lt; 2e-16 ***\nORIGIN_SZBKSZ04 -0.2499063  0.0076555   -32.644  &lt; 2e-16 ***\nORIGIN_SZBKSZ05 -0.2628428  0.0078905   -33.311  &lt; 2e-16 ***\nORIGIN_SZBKSZ06 -0.2174034  0.0075134   -28.936  &lt; 2e-16 ***\nORIGIN_SZBKSZ07  0.7094093  0.0058574   121.114  &lt; 2e-16 ***\nORIGIN_SZBKSZ08 -0.1614362  0.0067626   -23.872  &lt; 2e-16 ***\nORIGIN_SZBKSZ09 -0.2739085  0.0072969   -37.537  &lt; 2e-16 ***\nORIGIN_SZBLSZ01 -2.4281074  0.0181172  -134.022  &lt; 2e-16 ***\nORIGIN_SZBLSZ02 -2.7305447  0.0219341  -124.489  &lt; 2e-16 ***\nORIGIN_SZBLSZ03 -3.3071431  0.0540398   -61.198  &lt; 2e-16 ***\nORIGIN_SZBLSZ04 -2.4550671  0.0263946   -93.014  &lt; 2e-16 ***\nORIGIN_SZBMSZ01  0.1198976  0.0065964    18.176  &lt; 2e-16 ***\nORIGIN_SZBMSZ02 -1.3908667  0.0083230  -167.112  &lt; 2e-16 ***\nORIGIN_SZBMSZ03 -0.6999122  0.0069754  -100.339  &lt; 2e-16 ***\nORIGIN_SZBMSZ04 -0.2691159  0.0066184   -40.662  &lt; 2e-16 ***\nORIGIN_SZBMSZ05 -2.6163780  0.0190989  -136.991  &lt; 2e-16 ***\nORIGIN_SZBMSZ06 -2.9729956  0.0197182  -150.774  &lt; 2e-16 ***\nORIGIN_SZBMSZ07 -0.7309916  0.0072407  -100.956  &lt; 2e-16 ***\nORIGIN_SZBMSZ08 -1.0019514  0.0073169  -136.936  &lt; 2e-16 ***\nORIGIN_SZBMSZ09 -1.3667460  0.0105325  -129.764  &lt; 2e-16 ***\nORIGIN_SZBMSZ10 -1.6907268  0.0106687  -158.476  &lt; 2e-16 ***\nORIGIN_SZBMSZ11 -1.2288802  0.0082919  -148.202  &lt; 2e-16 ***\nORIGIN_SZBMSZ12 -1.6517767  0.0115101  -143.507  &lt; 2e-16 ***\nORIGIN_SZBMSZ13 -0.7251351  0.0075289   -96.314  &lt; 2e-16 ***\nORIGIN_SZBMSZ14 -1.1534912  0.0082629  -139.599  &lt; 2e-16 ***\nORIGIN_SZBMSZ15 -0.5476774  0.0075710   -72.339  &lt; 2e-16 ***\nORIGIN_SZBMSZ16 -1.5195034  0.0111459  -136.329  &lt; 2e-16 ***\nORIGIN_SZBMSZ17 -1.6026767  0.0184419   -86.904  &lt; 2e-16 ***\nORIGIN_SZBPSZ01  0.5571291  0.0071866    77.523  &lt; 2e-16 ***\nORIGIN_SZBPSZ02  0.0523197  0.0082259     6.360 2.01e-10 ***\nORIGIN_SZBPSZ03  0.2942047  0.0080482    36.555  &lt; 2e-16 ***\nORIGIN_SZBPSZ04  0.6246296  0.0065878    94.816  &lt; 2e-16 ***\nORIGIN_SZBPSZ05  0.8663708  0.0060852   142.372  &lt; 2e-16 ***\nORIGIN_SZBPSZ06 -0.9896182  0.0109551   -90.334  &lt; 2e-16 ***\nORIGIN_SZBPSZ07 -0.5219250  0.0101830   -51.255  &lt; 2e-16 ***\nORIGIN_SZBSSZ01  0.3299588  0.0066440    49.663  &lt; 2e-16 ***\nORIGIN_SZBSSZ02  0.2851357  0.0057077    49.956  &lt; 2e-16 ***\nORIGIN_SZBSSZ03 -0.2084740  0.0063364   -32.901  &lt; 2e-16 ***\nORIGIN_SZBTSZ01  0.1425664  0.0071103    20.051  &lt; 2e-16 ***\nORIGIN_SZBTSZ02 -0.5591999  0.0093616   -59.733  &lt; 2e-16 ***\nORIGIN_SZBTSZ03 -0.3648190  0.0081677   -44.666  &lt; 2e-16 ***\nORIGIN_SZBTSZ04 -1.4555078  0.0120138  -121.152  &lt; 2e-16 ***\nORIGIN_SZBTSZ05 -0.8635510  0.0133848   -64.517  &lt; 2e-16 ***\nORIGIN_SZBTSZ06 -1.1383111  0.0106421  -106.963  &lt; 2e-16 ***\nORIGIN_SZBTSZ07 -2.3477669  0.0160858  -145.953  &lt; 2e-16 ***\nORIGIN_SZBTSZ08 -1.2918779  0.0124862  -103.464  &lt; 2e-16 ***\nORIGIN_SZCBSZ01 -3.3713588  0.0578683   -58.259  &lt; 2e-16 ***\nORIGIN_SZCCSZ01 -0.6029242  0.0153385   -39.308  &lt; 2e-16 ***\nORIGIN_SZCHSZ01 -0.7641380  0.0135100   -56.561  &lt; 2e-16 ***\nORIGIN_SZCHSZ02 -0.8400736  0.0101951   -82.400  &lt; 2e-16 ***\nORIGIN_SZCHSZ03  1.2753127  0.0072576   175.720  &lt; 2e-16 ***\nORIGIN_SZCKSZ01  0.2470943  0.0067135    36.806  &lt; 2e-16 ***\nORIGIN_SZCKSZ02  0.5793581  0.0070498    82.181  &lt; 2e-16 ***\nORIGIN_SZCKSZ03  1.0795767  0.0060642   178.025  &lt; 2e-16 ***\nORIGIN_SZCKSZ04  1.4947920  0.0063122   236.808  &lt; 2e-16 ***\nORIGIN_SZCKSZ05  0.7457580  0.0074071   100.681  &lt; 2e-16 ***\nORIGIN_SZCKSZ06  0.5760952  0.0094861    60.730  &lt; 2e-16 ***\nORIGIN_SZCLSZ01 -0.9061335  0.0098617   -91.884  &lt; 2e-16 ***\nORIGIN_SZCLSZ02 -1.7609479  0.0156124  -112.791  &lt; 2e-16 ***\nORIGIN_SZCLSZ03 -1.0081325  0.0095171  -105.929  &lt; 2e-16 ***\nORIGIN_SZCLSZ04  0.6181200  0.0057953   106.659  &lt; 2e-16 ***\nORIGIN_SZCLSZ05 -2.0462335  0.0168934  -121.127  &lt; 2e-16 ***\nORIGIN_SZCLSZ06  0.7902389  0.0055680   141.924  &lt; 2e-16 ***\nORIGIN_SZCLSZ07 -0.5472929  0.0071001   -77.082  &lt; 2e-16 ***\nORIGIN_SZCLSZ08 -0.2197650  0.0077460   -28.372  &lt; 2e-16 ***\nORIGIN_SZCLSZ09 -1.8175782  0.0195989   -92.739  &lt; 2e-16 ***\nORIGIN_SZDTSZ02 -3.7618796  0.0872098   -43.136  &lt; 2e-16 ***\nORIGIN_SZDTSZ03 -3.4514766  0.0840812   -41.049  &lt; 2e-16 ***\nORIGIN_SZDTSZ13 -3.0627578  0.0352485   -86.891  &lt; 2e-16 ***\nORIGIN_SZGLSZ01 -1.8055929  0.0111938  -161.303  &lt; 2e-16 ***\nORIGIN_SZGLSZ02 -0.1588829  0.0061413   -25.871  &lt; 2e-16 ***\nORIGIN_SZGLSZ03 -0.2508524  0.0064276   -39.027  &lt; 2e-16 ***\nORIGIN_SZGLSZ04  0.8819358  0.0051993   169.627  &lt; 2e-16 ***\nORIGIN_SZGLSZ05  0.6062778  0.0053735   112.828  &lt; 2e-16 ***\nORIGIN_SZHGSZ01  0.3841503  0.0056776    67.660  &lt; 2e-16 ***\nORIGIN_SZHGSZ02  0.3962330  0.0057579    68.815  &lt; 2e-16 ***\nORIGIN_SZHGSZ03  0.2159531  0.0061671    35.017  &lt; 2e-16 ***\nORIGIN_SZHGSZ04  0.7831941  0.0052216   149.992  &lt; 2e-16 ***\nORIGIN_SZHGSZ05  1.1741558  0.0051799   226.677  &lt; 2e-16 ***\nORIGIN_SZHGSZ06 -0.1891403  0.0065556   -28.852  &lt; 2e-16 ***\nORIGIN_SZHGSZ07  0.3105421  0.0057186    54.304  &lt; 2e-16 ***\nORIGIN_SZHGSZ08 -0.0766364  0.0063474   -12.074  &lt; 2e-16 ***\nORIGIN_SZHGSZ09 -1.2211107  0.0101434  -120.384  &lt; 2e-16 ***\nORIGIN_SZHGSZ10 -3.4844709  0.0504793   -69.028  &lt; 2e-16 ***\nORIGIN_SZJESZ01  0.4916496  0.0063444    77.493  &lt; 2e-16 ***\nORIGIN_SZJESZ02  0.1343893  0.0063762    21.077  &lt; 2e-16 ***\nORIGIN_SZJESZ03 -0.2761723  0.0068085   -40.563  &lt; 2e-16 ***\nORIGIN_SZJESZ04 -1.5932744  0.0121402  -131.240  &lt; 2e-16 ***\nORIGIN_SZJESZ05 -2.3041311  0.0160245  -143.788  &lt; 2e-16 ***\nORIGIN_SZJESZ06  0.2811076  0.0062495    44.981  &lt; 2e-16 ***\nORIGIN_SZJESZ07 -1.9413956  0.0136276  -142.461  &lt; 2e-16 ***\nORIGIN_SZJESZ08 -1.3315645  0.0143168   -93.007  &lt; 2e-16 ***\nORIGIN_SZJESZ09  0.4418314  0.0069208    63.841  &lt; 2e-16 ***\nORIGIN_SZJESZ10 -1.5551555  0.0236523   -65.751  &lt; 2e-16 ***\nORIGIN_SZJESZ11 -1.8888230  0.0224630   -84.086  &lt; 2e-16 ***\nORIGIN_SZJWSZ01  0.2564586  0.0084699    30.279  &lt; 2e-16 ***\nORIGIN_SZJWSZ02  0.6899398  0.0061751   111.729  &lt; 2e-16 ***\nORIGIN_SZJWSZ03  1.4761229  0.0057392   257.198  &lt; 2e-16 ***\nORIGIN_SZJWSZ04  0.5701272  0.0065749    86.713  &lt; 2e-16 ***\nORIGIN_SZJWSZ05 -2.1253657  0.0150769  -140.968  &lt; 2e-16 ***\nORIGIN_SZJWSZ06 -1.5307265  0.0131906  -116.047  &lt; 2e-16 ***\nORIGIN_SZJWSZ07 -2.8801618  0.0360772   -79.833  &lt; 2e-16 ***\nORIGIN_SZJWSZ08  1.4428820  0.0059638   241.938  &lt; 2e-16 ***\nORIGIN_SZJWSZ09  1.8968475  0.0055649   340.860  &lt; 2e-16 ***\nORIGIN_SZKLSZ01  0.1116580  0.0059844    18.658  &lt; 2e-16 ***\nORIGIN_SZKLSZ02 -0.9618787  0.0077344  -124.364  &lt; 2e-16 ***\nORIGIN_SZKLSZ03 -0.7070626  0.0070275  -100.613  &lt; 2e-16 ***\nORIGIN_SZKLSZ04 -2.2742765  0.0139991  -162.459  &lt; 2e-16 ***\nORIGIN_SZKLSZ05 -1.1907262  0.0123719   -96.244  &lt; 2e-16 ***\nORIGIN_SZKLSZ06 -5.9774897  0.1857994   -32.172  &lt; 2e-16 ***\nORIGIN_SZKLSZ07 -1.4258369  0.0103083  -138.320  &lt; 2e-16 ***\nORIGIN_SZKLSZ08 -1.7625888  0.0116107  -151.808  &lt; 2e-16 ***\nORIGIN_SZLKSZ01 -2.0541388  0.0448216   -45.829  &lt; 2e-16 ***\nORIGIN_SZMDSZ01 -0.8571117  0.0321054   -26.697  &lt; 2e-16 ***\nORIGIN_SZMDSZ02 -0.6034597  0.0120724   -49.987  &lt; 2e-16 ***\nORIGIN_SZMDSZ03 -2.1681163  0.0201078  -107.825  &lt; 2e-16 ***\nORIGIN_SZMPSZ01 -0.9331562  0.0096218   -96.984  &lt; 2e-16 ***\nORIGIN_SZMPSZ02 -1.0268229  0.0081379  -126.178  &lt; 2e-16 ***\nORIGIN_SZMPSZ03  0.0054001  0.0066875     0.807 0.419385    \nORIGIN_SZMUSZ02 -3.6269863  0.1105492   -32.809  &lt; 2e-16 ***\nORIGIN_SZNTSZ01 -3.0593717  0.0399843   -76.514  &lt; 2e-16 ***\nORIGIN_SZNTSZ02 -3.3331415  0.0251754  -132.397  &lt; 2e-16 ***\nORIGIN_SZNTSZ03 -0.8351522  0.0090372   -92.413  &lt; 2e-16 ***\nORIGIN_SZNTSZ05 -4.2082472  0.0583343   -72.140  &lt; 2e-16 ***\nORIGIN_SZNTSZ06 -3.8549296  0.0593793   -64.920  &lt; 2e-16 ***\nORIGIN_SZNVSZ01  0.2789069  0.0056024    49.784  &lt; 2e-16 ***\nORIGIN_SZNVSZ02 -0.6036857  0.0077126   -78.273  &lt; 2e-16 ***\nORIGIN_SZNVSZ03 -1.0072683  0.0092678  -108.685  &lt; 2e-16 ***\nORIGIN_SZNVSZ04 -0.8723996  0.0101399   -86.037  &lt; 2e-16 ***\nORIGIN_SZNVSZ05 -2.1552928  0.0183064  -117.734  &lt; 2e-16 ***\nORIGIN_SZPGSZ01  0.0520607  0.0157846     3.298 0.000973 ***\nORIGIN_SZPGSZ02 -0.3481687  0.0089328   -38.976  &lt; 2e-16 ***\nORIGIN_SZPGSZ03  0.9095292  0.0058835   154.590  &lt; 2e-16 ***\nORIGIN_SZPGSZ04  1.3653717  0.0054727   249.489  &lt; 2e-16 ***\nORIGIN_SZPGSZ05  0.3762720  0.0073841    50.957  &lt; 2e-16 ***\nORIGIN_SZPLSZ01 -0.9142754  0.0136552   -66.954  &lt; 2e-16 ***\nORIGIN_SZPLSZ02 -1.0987582  0.0175891   -62.468  &lt; 2e-16 ***\nORIGIN_SZPLSZ03 -2.3427113  0.0474176   -49.406  &lt; 2e-16 ***\nORIGIN_SZPLSZ04 -2.9140779  0.0374458   -77.821  &lt; 2e-16 ***\nORIGIN_SZPLSZ05 -2.2381965  0.0261572   -85.567  &lt; 2e-16 ***\nORIGIN_SZPNSZ01  0.9659006  0.0075177   128.484  &lt; 2e-16 ***\nORIGIN_SZPNSZ02 -0.0158348  0.0143869    -1.101 0.271053    \nORIGIN_SZPNSZ03 -2.1837321  0.0224396   -97.316  &lt; 2e-16 ***\nORIGIN_SZPNSZ04 -3.2481509  0.0370762   -87.608  &lt; 2e-16 ***\nORIGIN_SZPNSZ05 -2.0450679  0.0328165   -62.318  &lt; 2e-16 ***\nORIGIN_SZPRSZ01 -0.6701245  0.0141567   -47.336  &lt; 2e-16 ***\nORIGIN_SZPRSZ02  0.7931907  0.0058079   136.570  &lt; 2e-16 ***\nORIGIN_SZPRSZ03  0.4249094  0.0058610    72.498  &lt; 2e-16 ***\nORIGIN_SZPRSZ04 -0.8529967  0.0090997   -93.739  &lt; 2e-16 ***\nORIGIN_SZPRSZ05  0.7865479  0.0055282   142.278  &lt; 2e-16 ***\nORIGIN_SZPRSZ06 -1.3303664  0.0134512   -98.903  &lt; 2e-16 ***\nORIGIN_SZPRSZ07 -3.0458370  0.0181514  -167.802  &lt; 2e-16 ***\nORIGIN_SZPRSZ08 -0.5342399  0.0075966   -70.327  &lt; 2e-16 ***\nORIGIN_SZQTSZ01 -0.2548930  0.0086485   -29.473  &lt; 2e-16 ***\nORIGIN_SZQTSZ02 -0.8662439  0.0076549  -113.162  &lt; 2e-16 ***\nORIGIN_SZQTSZ03 -0.0890168  0.0072455   -12.286  &lt; 2e-16 ***\nORIGIN_SZQTSZ04 -1.4634370  0.0089384  -163.724  &lt; 2e-16 ***\nORIGIN_SZQTSZ05 -0.6535669  0.0077612   -84.210  &lt; 2e-16 ***\nORIGIN_SZQTSZ06 -0.8275765  0.0081835  -101.128  &lt; 2e-16 ***\nORIGIN_SZQTSZ07 -1.5369800  0.0112808  -136.248  &lt; 2e-16 ***\nORIGIN_SZQTSZ08 -0.4437979  0.0075302   -58.936  &lt; 2e-16 ***\nORIGIN_SZQTSZ09 -0.8184934  0.0083589   -97.918  &lt; 2e-16 ***\nORIGIN_SZQTSZ10 -0.6906597  0.0080980   -85.288  &lt; 2e-16 ***\nORIGIN_SZQTSZ11 -2.3251162  0.0154191  -150.795  &lt; 2e-16 ***\nORIGIN_SZQTSZ12 -3.0442790  0.0208985  -145.670  &lt; 2e-16 ***\nORIGIN_SZQTSZ13 -0.7241013  0.0093441   -77.493  &lt; 2e-16 ***\nORIGIN_SZQTSZ14 -1.8225351  0.0138207  -131.870  &lt; 2e-16 ***\nORIGIN_SZQTSZ15 -0.8720806  0.0138589   -62.926  &lt; 2e-16 ***\nORIGIN_SZRCSZ01 -1.8063415  0.0144295  -125.184  &lt; 2e-16 ***\nORIGIN_SZRCSZ06 -0.5370905  0.0101573   -52.877  &lt; 2e-16 ***\nORIGIN_SZRVSZ01 -2.7426167  0.0341386   -80.338  &lt; 2e-16 ***\nORIGIN_SZRVSZ02 -3.0827269  0.0302299  -101.976  &lt; 2e-16 ***\nORIGIN_SZRVSZ03 -2.9133853  0.0262543  -110.968  &lt; 2e-16 ***\nORIGIN_SZRVSZ04 -3.4220022  0.0582209   -58.776  &lt; 2e-16 ***\nORIGIN_SZRVSZ05 -2.6206257  0.0197470  -132.710  &lt; 2e-16 ***\nORIGIN_SZSBSZ01  0.1010337  0.0085117    11.870  &lt; 2e-16 ***\nORIGIN_SZSBSZ02 -0.8810456  0.0098244   -89.680  &lt; 2e-16 ***\nORIGIN_SZSBSZ03  0.8303668  0.0063009   131.785  &lt; 2e-16 ***\nORIGIN_SZSBSZ04  0.3489128  0.0071456    48.829  &lt; 2e-16 ***\nORIGIN_SZSBSZ05 -0.3182914  0.0085560   -37.201  &lt; 2e-16 ***\nORIGIN_SZSBSZ06 -0.9074308  0.0200035   -45.364  &lt; 2e-16 ***\nORIGIN_SZSBSZ07 -0.2217124  0.0167188   -13.261  &lt; 2e-16 ***\nORIGIN_SZSBSZ08 -1.3007367  0.0178771   -72.760  &lt; 2e-16 ***\nORIGIN_SZSBSZ09 -0.9813703  0.0107885   -90.965  &lt; 2e-16 ***\nORIGIN_SZSESZ02  1.1283424  0.0054209   208.146  &lt; 2e-16 ***\nORIGIN_SZSESZ03  1.2389996  0.0051926   238.610  &lt; 2e-16 ***\nORIGIN_SZSESZ04  0.7535119  0.0060371   124.814  &lt; 2e-16 ***\nORIGIN_SZSESZ05 -0.2347978  0.0071482   -32.847  &lt; 2e-16 ***\nORIGIN_SZSESZ06  0.9520620  0.0057572   165.368  &lt; 2e-16 ***\nORIGIN_SZSESZ07 -2.4296685  0.0231677  -104.873  &lt; 2e-16 ***\nORIGIN_SZSGSZ01 -0.6995899  0.0099969   -69.980  &lt; 2e-16 ***\nORIGIN_SZSGSZ02 -1.2602157  0.0111471  -113.053  &lt; 2e-16 ***\nORIGIN_SZSGSZ03  0.0725860  0.0061970    11.713  &lt; 2e-16 ***\nORIGIN_SZSGSZ04  0.2738315  0.0057524    47.603  &lt; 2e-16 ***\nORIGIN_SZSGSZ05 -2.0207710  0.0119838  -168.625  &lt; 2e-16 ***\nORIGIN_SZSGSZ06  0.4885608  0.0054646    89.404  &lt; 2e-16 ***\nORIGIN_SZSGSZ07 -0.8892155  0.0075074  -118.445  &lt; 2e-16 ***\nORIGIN_SZSKSZ01 -0.3682754  0.0108025   -34.092  &lt; 2e-16 ***\nORIGIN_SZSKSZ02  1.1826086  0.0071388   165.659  &lt; 2e-16 ***\nORIGIN_SZSKSZ03 -0.3230177  0.0101683   -31.767  &lt; 2e-16 ***\nORIGIN_SZSKSZ04 -1.8504236  0.0362400   -51.060  &lt; 2e-16 ***\nORIGIN_SZSKSZ05 -0.2759035  0.0185157   -14.901  &lt; 2e-16 ***\nORIGIN_SZSLSZ01 -2.2757902  0.0348766   -65.253  &lt; 2e-16 ***\nORIGIN_SZSLSZ04 -0.0899820  0.0090356    -9.959  &lt; 2e-16 ***\nORIGIN_SZSRSZ01 -2.1460151  0.0187871  -114.228  &lt; 2e-16 ***\nORIGIN_SZTHSZ01 -2.6851549  0.0571841   -46.956  &lt; 2e-16 ***\nORIGIN_SZTHSZ03 -1.0121495  0.0275551   -36.732  &lt; 2e-16 ***\nORIGIN_SZTHSZ04 -2.6129645  0.0345167   -75.701  &lt; 2e-16 ***\nORIGIN_SZTHSZ06 -1.7229100  0.0208134   -82.779  &lt; 2e-16 ***\nORIGIN_SZTMSZ01 -0.2254986  0.0070312   -32.071  &lt; 2e-16 ***\nORIGIN_SZTMSZ02  1.7271575  0.0049219   350.914  &lt; 2e-16 ***\nORIGIN_SZTMSZ03  0.9891319  0.0052266   189.250  &lt; 2e-16 ***\nORIGIN_SZTMSZ04  0.2018090  0.0062114    32.490  &lt; 2e-16 ***\nORIGIN_SZTMSZ05 -1.1882870  0.0125842   -94.427  &lt; 2e-16 ***\nORIGIN_SZTNSZ01 -1.6122620  0.0141911  -113.611  &lt; 2e-16 ***\nORIGIN_SZTNSZ02 -1.5630967  0.0112227  -139.280  &lt; 2e-16 ***\nORIGIN_SZTNSZ03 -2.0739538  0.0149298  -138.914  &lt; 2e-16 ***\nORIGIN_SZTNSZ04 -0.2816960  0.0085295   -33.026  &lt; 2e-16 ***\nORIGIN_SZTPSZ01 -0.7822239  0.0077901  -100.412  &lt; 2e-16 ***\nORIGIN_SZTPSZ02  0.5735478  0.0053042   108.131  &lt; 2e-16 ***\nORIGIN_SZTPSZ03 -0.8748650  0.0074202  -117.903  &lt; 2e-16 ***\nORIGIN_SZTPSZ04 -0.8537831  0.0069792  -122.332  &lt; 2e-16 ***\nORIGIN_SZTPSZ05 -0.5581114  0.0077012   -72.471  &lt; 2e-16 ***\nORIGIN_SZTPSZ06  0.0262001  0.0075241     3.482 0.000497 ***\nORIGIN_SZTPSZ07 -0.5969952  0.0074272   -80.380  &lt; 2e-16 ***\nORIGIN_SZTPSZ08 -1.0537959  0.0111297   -94.683  &lt; 2e-16 ***\nORIGIN_SZTPSZ09 -0.9588508  0.0081314  -117.920  &lt; 2e-16 ***\nORIGIN_SZTPSZ10 -1.1177249  0.0089403  -125.021  &lt; 2e-16 ***\nORIGIN_SZTPSZ11 -0.2799677  0.0067135   -41.702  &lt; 2e-16 ***\nORIGIN_SZTPSZ12 -0.8898871  0.0080215  -110.938  &lt; 2e-16 ***\nORIGIN_SZTSSZ01 -2.6146463  0.0521606   -50.127  &lt; 2e-16 ***\nORIGIN_SZTSSZ02  0.1682588  0.0119965    14.026  &lt; 2e-16 ***\nORIGIN_SZTSSZ03  0.2587653  0.0123809    20.900  &lt; 2e-16 ***\nORIGIN_SZTSSZ04 -0.5473825  0.0135215   -40.482  &lt; 2e-16 ***\nORIGIN_SZTSSZ05 -0.9967379  0.0206068   -48.369  &lt; 2e-16 ***\nORIGIN_SZTSSZ06  0.4933147  0.0229597    21.486  &lt; 2e-16 ***\nORIGIN_SZWCSZ01  1.2524706  0.0111133   112.700  &lt; 2e-16 ***\nORIGIN_SZWCSZ02 -2.8544820  0.0347805   -82.071  &lt; 2e-16 ***\nORIGIN_SZWCSZ03 -5.1277334  0.1475585   -34.751  &lt; 2e-16 ***\nORIGIN_SZWDSZ01  1.4725308  0.0056496   260.645  &lt; 2e-16 ***\nORIGIN_SZWDSZ02  0.1571680  0.0064909    24.214  &lt; 2e-16 ***\nORIGIN_SZWDSZ03  1.2584097  0.0061471   204.717  &lt; 2e-16 ***\nORIGIN_SZWDSZ04  0.8578765  0.0069277   123.833  &lt; 2e-16 ***\nORIGIN_SZWDSZ05  0.1702728  0.0069687    24.434  &lt; 2e-16 ***\nORIGIN_SZWDSZ06  0.1736910  0.0069507    24.989  &lt; 2e-16 ***\nORIGIN_SZWDSZ07 -1.5610176  0.0100803  -154.859  &lt; 2e-16 ***\nORIGIN_SZWDSZ08 -0.9490906  0.0102047   -93.005  &lt; 2e-16 ***\nORIGIN_SZWDSZ09  1.2107011  0.0062294   194.354  &lt; 2e-16 ***\nORIGIN_SZYSSZ01 -0.3324158  0.0074537   -44.598  &lt; 2e-16 ***\nORIGIN_SZYSSZ02  0.8177113  0.0066108   123.693  &lt; 2e-16 ***\nORIGIN_SZYSSZ03  1.6751777  0.0058470   286.503  &lt; 2e-16 ***\nORIGIN_SZYSSZ04  0.8130044  0.0059025   137.738  &lt; 2e-16 ***\nORIGIN_SZYSSZ05  0.3678420  0.0072431    50.785  &lt; 2e-16 ***\nORIGIN_SZYSSZ06 -0.6024384  0.0126722   -47.540  &lt; 2e-16 ***\nORIGIN_SZYSSZ07 -0.7631918  0.0158478   -48.157  &lt; 2e-16 ***\nORIGIN_SZYSSZ08  0.2141930  0.0076154    28.126  &lt; 2e-16 ***\nORIGIN_SZYSSZ09  1.0809368  0.0057973   186.457  &lt; 2e-16 ***\nDESTIN_SZAMSZ02  0.0761304  0.0051207    14.867  &lt; 2e-16 ***\nDESTIN_SZAMSZ03  0.0143394  0.0050755     2.825 0.004724 ** \nDESTIN_SZAMSZ04 -1.2516780  0.0074947  -167.008  &lt; 2e-16 ***\nDESTIN_SZAMSZ05 -1.2312375  0.0076598  -160.741  &lt; 2e-16 ***\nDESTIN_SZAMSZ06 -1.0333412  0.0075283  -137.261  &lt; 2e-16 ***\nDESTIN_SZAMSZ07 -1.5338249  0.0110036  -139.392  &lt; 2e-16 ***\nDESTIN_SZAMSZ08 -0.3751665  0.0075358   -49.784  &lt; 2e-16 ***\nDESTIN_SZAMSZ09 -1.1633493  0.0077556  -150.001  &lt; 2e-16 ***\nDESTIN_SZAMSZ10  0.1017717  0.0053151    19.148  &lt; 2e-16 ***\nDESTIN_SZAMSZ11 -0.8840362  0.0097007   -91.131  &lt; 2e-16 ***\nDESTIN_SZAMSZ12  0.1628123  0.0055220    29.484  &lt; 2e-16 ***\nDESTIN_SZBDSZ01  1.0040794  0.0047922   209.523  &lt; 2e-16 ***\nDESTIN_SZBDSZ02 -0.2478149  0.0063085   -39.283  &lt; 2e-16 ***\nDESTIN_SZBDSZ03  0.1016088  0.0057420    17.696  &lt; 2e-16 ***\nDESTIN_SZBDSZ04  1.1082928  0.0047747   232.116  &lt; 2e-16 ***\nDESTIN_SZBDSZ05  0.8737933  0.0050593   172.712  &lt; 2e-16 ***\nDESTIN_SZBDSZ06  0.2897032  0.0058244    49.740  &lt; 2e-16 ***\nDESTIN_SZBDSZ07 -0.9026193  0.0113656   -79.416  &lt; 2e-16 ***\nDESTIN_SZBDSZ08 -1.7063577  0.0131234  -130.024  &lt; 2e-16 ***\nDESTIN_SZBKSZ01 -1.3892839  0.0083307  -166.767  &lt; 2e-16 ***\nDESTIN_SZBKSZ02 -0.6661120  0.0073464   -90.672  &lt; 2e-16 ***\nDESTIN_SZBKSZ03 -0.9536826  0.0073196  -130.292  &lt; 2e-16 ***\nDESTIN_SZBKSZ04 -0.6655610  0.0065868  -101.044  &lt; 2e-16 ***\nDESTIN_SZBKSZ05 -0.9053119  0.0079264  -114.215  &lt; 2e-16 ***\nDESTIN_SZBKSZ06 -1.2622159  0.0075079  -168.119  &lt; 2e-16 ***\nDESTIN_SZBKSZ07 -0.0423370  0.0056686    -7.469 8.10e-14 ***\nDESTIN_SZBKSZ08 -1.3811240  0.0084985  -162.515  &lt; 2e-16 ***\nDESTIN_SZBKSZ09 -0.0797012  0.0061428   -12.975  &lt; 2e-16 ***\nDESTIN_SZBLSZ01 -0.8859670  0.0088108  -100.555  &lt; 2e-16 ***\nDESTIN_SZBLSZ02  0.1362723  0.0082167    16.585  &lt; 2e-16 ***\nDESTIN_SZBLSZ03  1.2037396  0.0093508   128.732  &lt; 2e-16 ***\nDESTIN_SZBLSZ04 -0.9316219  0.0178080   -52.315  &lt; 2e-16 ***\nDESTIN_SZBMSZ01  0.7188470  0.0061160   117.536  &lt; 2e-16 ***\nDESTIN_SZBMSZ02 -0.0597895  0.0061206    -9.769  &lt; 2e-16 ***\nDESTIN_SZBMSZ03 -0.2427075  0.0069937   -34.704  &lt; 2e-16 ***\nDESTIN_SZBMSZ04 -0.0622494  0.0065569    -9.494  &lt; 2e-16 ***\nDESTIN_SZBMSZ05 -0.2857019  0.0086450   -33.048  &lt; 2e-16 ***\nDESTIN_SZBMSZ06 -1.3486558  0.0158904   -84.872  &lt; 2e-16 ***\nDESTIN_SZBMSZ07  0.4549687  0.0058315    78.020  &lt; 2e-16 ***\nDESTIN_SZBMSZ08 -0.8730268  0.0077814  -112.195  &lt; 2e-16 ***\nDESTIN_SZBMSZ09 -2.0319890  0.0163038  -124.633  &lt; 2e-16 ***\nDESTIN_SZBMSZ10 -1.4319101  0.0102616  -139.541  &lt; 2e-16 ***\nDESTIN_SZBMSZ11 -1.2429176  0.0092250  -134.733  &lt; 2e-16 ***\nDESTIN_SZBMSZ12 -0.8526549  0.0096009   -88.810  &lt; 2e-16 ***\nDESTIN_SZBMSZ13  0.1399907  0.0066885    20.930  &lt; 2e-16 ***\nDESTIN_SZBMSZ14 -1.0103155  0.0091377  -110.566  &lt; 2e-16 ***\nDESTIN_SZBMSZ15 -0.6819769  0.0086179   -79.135  &lt; 2e-16 ***\nDESTIN_SZBMSZ16 -1.4468308  0.0134051  -107.931  &lt; 2e-16 ***\nDESTIN_SZBMSZ17 -1.5312175  0.0186843   -81.952  &lt; 2e-16 ***\nDESTIN_SZBPSZ01 -1.1726725  0.0073257  -160.077  &lt; 2e-16 ***\nDESTIN_SZBPSZ02 -2.1072012  0.0103320  -203.949  &lt; 2e-16 ***\nDESTIN_SZBPSZ03 -1.6944911  0.0098520  -171.995  &lt; 2e-16 ***\nDESTIN_SZBPSZ04 -0.7664610  0.0074458  -102.939  &lt; 2e-16 ***\nDESTIN_SZBPSZ05  0.1358370  0.0056258    24.145  &lt; 2e-16 ***\nDESTIN_SZBPSZ06 -1.2425471  0.0096942  -128.175  &lt; 2e-16 ***\nDESTIN_SZBPSZ07 -0.1666192  0.0094969   -17.545  &lt; 2e-16 ***\nDESTIN_SZBSSZ01  0.3857894  0.0057261    67.374  &lt; 2e-16 ***\nDESTIN_SZBSSZ02 -0.5293265  0.0064886   -81.578  &lt; 2e-16 ***\nDESTIN_SZBSSZ03  0.3909966  0.0048540    80.551  &lt; 2e-16 ***\nDESTIN_SZBTSZ01  0.7114965  0.0054528   130.482  &lt; 2e-16 ***\nDESTIN_SZBTSZ02 -0.0487084  0.0082474    -5.906 3.51e-09 ***\nDESTIN_SZBTSZ03  0.5539032  0.0064423    85.979  &lt; 2e-16 ***\nDESTIN_SZBTSZ04 -0.7120734  0.0128676   -55.339  &lt; 2e-16 ***\nDESTIN_SZBTSZ05  0.2176097  0.0086791    25.073  &lt; 2e-16 ***\nDESTIN_SZBTSZ06 -0.2167084  0.0084925   -25.518  &lt; 2e-16 ***\nDESTIN_SZBTSZ07 -1.4045618  0.0124363  -112.940  &lt; 2e-16 ***\nDESTIN_SZBTSZ08 -0.8213918  0.0120793   -68.000  &lt; 2e-16 ***\nDESTIN_SZCBSZ01 -5.7340877  0.3333916   -17.199  &lt; 2e-16 ***\nDESTIN_SZCCSZ01 -0.0304192  0.0095920    -3.171 0.001518 ** \nDESTIN_SZCHSZ01 -0.2598507  0.0115311   -22.535  &lt; 2e-16 ***\nDESTIN_SZCHSZ02  0.3497750  0.0068334    51.186  &lt; 2e-16 ***\nDESTIN_SZCHSZ03  2.4550172  0.0050883   482.481  &lt; 2e-16 ***\nDESTIN_SZCKSZ01 -0.4691744  0.0063130   -74.319  &lt; 2e-16 ***\nDESTIN_SZCKSZ02 -0.9557084  0.0069331  -137.847  &lt; 2e-16 ***\nDESTIN_SZCKSZ03  0.0442112  0.0057117     7.740 9.91e-15 ***\nDESTIN_SZCKSZ04 -0.8592063  0.0081238  -105.764  &lt; 2e-16 ***\nDESTIN_SZCKSZ05 -1.1745333  0.0087305  -134.532  &lt; 2e-16 ***\nDESTIN_SZCKSZ06 -0.4982877  0.0085514   -58.269  &lt; 2e-16 ***\nDESTIN_SZCLSZ01  0.2665065  0.0059712    44.632  &lt; 2e-16 ***\nDESTIN_SZCLSZ02 -1.9758876  0.0150823  -131.007  &lt; 2e-16 ***\nDESTIN_SZCLSZ03 -0.9051310  0.0091479   -98.944  &lt; 2e-16 ***\nDESTIN_SZCLSZ04 -0.0828732  0.0061559   -13.462  &lt; 2e-16 ***\nDESTIN_SZCLSZ05 -1.1414780  0.0100760  -113.287  &lt; 2e-16 ***\nDESTIN_SZCLSZ06  0.3229402  0.0056269    57.392  &lt; 2e-16 ***\nDESTIN_SZCLSZ07 -0.4833612  0.0069777   -69.272  &lt; 2e-16 ***\nDESTIN_SZCLSZ08 -0.3219670  0.0075615   -42.580  &lt; 2e-16 ***\nDESTIN_SZCLSZ09  0.0564166  0.0080703     6.991 2.74e-12 ***\nDESTIN_SZDTSZ02 -1.6384236  0.0374725   -43.723  &lt; 2e-16 ***\nDESTIN_SZDTSZ03 -0.4021571  0.0152716   -26.334  &lt; 2e-16 ***\nDESTIN_SZDTSZ13 -1.2799441  0.0177095   -72.274  &lt; 2e-16 ***\nDESTIN_SZGLSZ01 -0.0190303  0.0060665    -3.137 0.001707 ** \nDESTIN_SZGLSZ02 -0.0308469  0.0058724    -5.253 1.50e-07 ***\nDESTIN_SZGLSZ03  0.6927638  0.0048456   142.969  &lt; 2e-16 ***\nDESTIN_SZGLSZ04  0.9325848  0.0049183   189.616  &lt; 2e-16 ***\nDESTIN_SZGLSZ05  0.8480056  0.0048801   173.768  &lt; 2e-16 ***\nDESTIN_SZHGSZ01  0.0652969  0.0047795    13.662  &lt; 2e-16 ***\nDESTIN_SZHGSZ02 -0.9498251  0.0066577  -142.667  &lt; 2e-16 ***\nDESTIN_SZHGSZ03 -1.4372499  0.0076387  -188.154  &lt; 2e-16 ***\nDESTIN_SZHGSZ04 -0.5236292  0.0055353   -94.599  &lt; 2e-16 ***\nDESTIN_SZHGSZ05 -0.5420295  0.0058099   -93.295  &lt; 2e-16 ***\nDESTIN_SZHGSZ06 -0.9054730  0.0067581  -133.983  &lt; 2e-16 ***\nDESTIN_SZHGSZ07  0.0215109  0.0054019     3.982 6.83e-05 ***\nDESTIN_SZHGSZ08 -0.0490979  0.0059206    -8.293  &lt; 2e-16 ***\nDESTIN_SZHGSZ09 -0.0711560  0.0062875   -11.317  &lt; 2e-16 ***\nDESTIN_SZHGSZ10 -3.5807154  0.0290642  -123.200  &lt; 2e-16 ***\nDESTIN_SZJESZ01 -0.4023638  0.0065057   -61.848  &lt; 2e-16 ***\nDESTIN_SZJESZ02 -0.7654353  0.0067096  -114.081  &lt; 2e-16 ***\nDESTIN_SZJESZ03 -0.8778812  0.0071238  -123.232  &lt; 2e-16 ***\nDESTIN_SZJESZ04 -1.1998075  0.0088733  -135.215  &lt; 2e-16 ***\nDESTIN_SZJESZ05 -1.5623652  0.0116898  -133.652  &lt; 2e-16 ***\nDESTIN_SZJESZ06  0.2311474  0.0055595    41.577  &lt; 2e-16 ***\nDESTIN_SZJESZ07 -1.2753348  0.0094838  -134.475  &lt; 2e-16 ***\nDESTIN_SZJESZ08 -0.7654533  0.0099306   -77.081  &lt; 2e-16 ***\nDESTIN_SZJESZ09  0.1637628  0.0074164    22.081  &lt; 2e-16 ***\nDESTIN_SZJESZ10  0.7394958  0.0091249    81.041  &lt; 2e-16 ***\nDESTIN_SZJESZ11  0.5157364  0.0086546    59.591  &lt; 2e-16 ***\nDESTIN_SZJWSZ01 -1.0165204  0.0083025  -122.435  &lt; 2e-16 ***\nDESTIN_SZJWSZ02 -0.8530646  0.0067851  -125.727  &lt; 2e-16 ***\nDESTIN_SZJWSZ03  0.5176135  0.0056449    91.695  &lt; 2e-16 ***\nDESTIN_SZJWSZ04  0.3427105  0.0058499    58.584  &lt; 2e-16 ***\nDESTIN_SZJWSZ05 -1.1695940  0.0080069  -146.073  &lt; 2e-16 ***\nDESTIN_SZJWSZ06 -0.7466462  0.0070240  -106.299  &lt; 2e-16 ***\nDESTIN_SZJWSZ07 -3.0124535  0.0333481   -90.334  &lt; 2e-16 ***\nDESTIN_SZJWSZ08 -0.4253502  0.0066584   -63.881  &lt; 2e-16 ***\nDESTIN_SZJWSZ09  0.9428005  0.0053190   177.251  &lt; 2e-16 ***\nDESTIN_SZKLSZ01 -0.2965013  0.0066422   -44.639  &lt; 2e-16 ***\nDESTIN_SZKLSZ02 -0.4921137  0.0067689   -72.702  &lt; 2e-16 ***\nDESTIN_SZKLSZ03 -0.8489213  0.0078294  -108.427  &lt; 2e-16 ***\nDESTIN_SZKLSZ04 -1.2656342  0.0099918  -126.667  &lt; 2e-16 ***\nDESTIN_SZKLSZ05 -0.3570126  0.0096300   -37.073  &lt; 2e-16 ***\nDESTIN_SZKLSZ06 -2.4764906  0.0390868   -63.359  &lt; 2e-16 ***\nDESTIN_SZKLSZ07 -0.7316189  0.0080994   -90.330  &lt; 2e-16 ***\nDESTIN_SZKLSZ08 -0.1115398  0.0061168   -18.235  &lt; 2e-16 ***\nDESTIN_SZLKSZ01 -1.4940710  0.0271518   -55.027  &lt; 2e-16 ***\nDESTIN_SZMDSZ01 -1.6101440  0.0231238   -69.631  &lt; 2e-16 ***\nDESTIN_SZMDSZ02 -0.9339318  0.0126277   -73.959  &lt; 2e-16 ***\nDESTIN_SZMDSZ03 -3.4868547  0.0303657  -114.829  &lt; 2e-16 ***\nDESTIN_SZMPSZ01 -0.4518483  0.0089869   -50.279  &lt; 2e-16 ***\nDESTIN_SZMPSZ02 -0.5868264  0.0073193   -80.176  &lt; 2e-16 ***\nDESTIN_SZMPSZ03  0.4805365  0.0059041    81.391  &lt; 2e-16 ***\nDESTIN_SZMUSZ02 -1.3837581  0.0218713   -63.268  &lt; 2e-16 ***\nDESTIN_SZNTSZ01 -3.0694691  0.0533346   -57.551  &lt; 2e-16 ***\nDESTIN_SZNTSZ02 -1.4992973  0.0130358  -115.014  &lt; 2e-16 ***\nDESTIN_SZNTSZ03 -0.5221236  0.0089923   -58.064  &lt; 2e-16 ***\nDESTIN_SZNTSZ05 -1.9751162  0.0282369   -69.948  &lt; 2e-16 ***\nDESTIN_SZNTSZ06 -3.9959411  0.0511214   -78.166  &lt; 2e-16 ***\nDESTIN_SZNVSZ01 -0.1126966  0.0057077   -19.745  &lt; 2e-16 ***\nDESTIN_SZNVSZ02 -0.0259250  0.0064427    -4.024 5.72e-05 ***\nDESTIN_SZNVSZ03 -0.0123214  0.0067692    -1.820 0.068725 .  \nDESTIN_SZNVSZ04 -1.3371298  0.0130261  -102.650  &lt; 2e-16 ***\nDESTIN_SZNVSZ05 -0.9686333  0.0101539   -95.395  &lt; 2e-16 ***\nDESTIN_SZPGSZ01 -1.1798309  0.0180543   -65.349  &lt; 2e-16 ***\nDESTIN_SZPGSZ02 -1.3289737  0.0085335  -155.736  &lt; 2e-16 ***\nDESTIN_SZPGSZ03 -0.1661373  0.0055166   -30.116  &lt; 2e-16 ***\nDESTIN_SZPGSZ04 -0.3046408  0.0058469   -52.103  &lt; 2e-16 ***\nDESTIN_SZPGSZ05 -1.5412612  0.0093261  -165.264  &lt; 2e-16 ***\nDESTIN_SZPLSZ01 -0.3439667  0.0083504   -41.192  &lt; 2e-16 ***\nDESTIN_SZPLSZ02 -1.7574919  0.0154244  -113.942  &lt; 2e-16 ***\nDESTIN_SZPLSZ03 -0.3455776  0.0112089   -30.831  &lt; 2e-16 ***\nDESTIN_SZPLSZ04 -2.0749385  0.0141153  -146.999  &lt; 2e-16 ***\nDESTIN_SZPLSZ05 -0.4855216  0.0134069   -36.214  &lt; 2e-16 ***\nDESTIN_SZPNSZ01  0.0117816  0.0083558     1.410 0.158543    \nDESTIN_SZPNSZ02  0.7389858  0.0089823    82.272  &lt; 2e-16 ***\nDESTIN_SZPNSZ03 -0.4708719  0.0098588   -47.761  &lt; 2e-16 ***\nDESTIN_SZPNSZ04  1.3156771  0.0111200   118.316  &lt; 2e-16 ***\nDESTIN_SZPNSZ05  0.9881886  0.0153169    64.516  &lt; 2e-16 ***\nDESTIN_SZPRSZ01 -1.0678999  0.0098295  -108.642  &lt; 2e-16 ***\nDESTIN_SZPRSZ02  0.0650279  0.0063927    10.172  &lt; 2e-16 ***\nDESTIN_SZPRSZ03  0.6348138  0.0050147   126.592  &lt; 2e-16 ***\nDESTIN_SZPRSZ04 -0.3640286  0.0097572   -37.309  &lt; 2e-16 ***\nDESTIN_SZPRSZ05  0.0380410  0.0062577     6.079 1.21e-09 ***\nDESTIN_SZPRSZ06  0.3153712  0.0068742    45.877  &lt; 2e-16 ***\nDESTIN_SZPRSZ07 -1.6669973  0.0145573  -114.513  &lt; 2e-16 ***\nDESTIN_SZPRSZ08 -0.6170648  0.0078424   -78.683  &lt; 2e-16 ***\nDESTIN_SZQTSZ01 -0.5496582  0.0098285   -55.925  &lt; 2e-16 ***\nDESTIN_SZQTSZ02 -0.7318114  0.0086807   -84.303  &lt; 2e-16 ***\nDESTIN_SZQTSZ03 -0.5893064  0.0084789   -69.503  &lt; 2e-16 ***\nDESTIN_SZQTSZ04 -0.7103906  0.0085341   -83.242  &lt; 2e-16 ***\nDESTIN_SZQTSZ05 -0.4721472  0.0078164   -60.405  &lt; 2e-16 ***\nDESTIN_SZQTSZ06 -0.6591466  0.0080069   -82.322  &lt; 2e-16 ***\nDESTIN_SZQTSZ07 -0.9540454  0.0126807   -75.236  &lt; 2e-16 ***\nDESTIN_SZQTSZ08  0.4508867  0.0064870    69.507  &lt; 2e-16 ***\nDESTIN_SZQTSZ09 -0.4061810  0.0075485   -53.810  &lt; 2e-16 ***\nDESTIN_SZQTSZ10  0.1351889  0.0068202    19.822  &lt; 2e-16 ***\nDESTIN_SZQTSZ11  0.3181553  0.0067958    46.816  &lt; 2e-16 ***\nDESTIN_SZQTSZ12 -0.1055766  0.0095576   -11.046  &lt; 2e-16 ***\nDESTIN_SZQTSZ13  0.5199663  0.0071928    72.290  &lt; 2e-16 ***\nDESTIN_SZQTSZ14  0.6086332  0.0078537    77.496  &lt; 2e-16 ***\nDESTIN_SZQTSZ15  1.3906866  0.0092250   150.753  &lt; 2e-16 ***\nDESTIN_SZRCSZ01 -0.0862091  0.0085363   -10.099  &lt; 2e-16 ***\nDESTIN_SZRCSZ06 -1.0186282  0.0211113   -48.250  &lt; 2e-16 ***\nDESTIN_SZRVSZ01 -1.5294454  0.0179337   -85.283  &lt; 2e-16 ***\nDESTIN_SZRVSZ02 -2.3607754  0.0355628   -66.383  &lt; 2e-16 ***\nDESTIN_SZRVSZ03 -1.5266254  0.0156276   -97.688  &lt; 2e-16 ***\nDESTIN_SZRVSZ04 -1.0986565  0.0168695   -65.127  &lt; 2e-16 ***\nDESTIN_SZRVSZ05 -2.4004418  0.0320917   -74.799  &lt; 2e-16 ***\nDESTIN_SZSBSZ01 -1.4023966  0.0109496  -128.078  &lt; 2e-16 ***\nDESTIN_SZSBSZ02 -1.3899893  0.0090891  -152.929  &lt; 2e-16 ***\nDESTIN_SZSBSZ03  0.4509008  0.0059864    75.321  &lt; 2e-16 ***\nDESTIN_SZSBSZ04  0.1796309  0.0070142    25.610  &lt; 2e-16 ***\nDESTIN_SZSBSZ05 -1.3159699  0.0096485  -136.391  &lt; 2e-16 ***\nDESTIN_SZSBSZ06 -1.7705263  0.0253064   -69.964  &lt; 2e-16 ***\nDESTIN_SZSBSZ07 -0.7471529  0.0238628   -31.310  &lt; 2e-16 ***\nDESTIN_SZSBSZ08  0.7884520  0.0069638   113.221  &lt; 2e-16 ***\nDESTIN_SZSBSZ09  0.0131702  0.0066350     1.985 0.047150 *  \nDESTIN_SZSESZ02 -0.7247347  0.0060626  -119.541  &lt; 2e-16 ***\nDESTIN_SZSESZ03  0.1032728  0.0048330    21.368  &lt; 2e-16 ***\nDESTIN_SZSESZ04 -1.0992420  0.0068328  -160.878  &lt; 2e-16 ***\nDESTIN_SZSESZ05 -0.8374712  0.0058155  -144.006  &lt; 2e-16 ***\nDESTIN_SZSESZ06 -0.5531619  0.0074766   -73.985  &lt; 2e-16 ***\nDESTIN_SZSESZ07 -3.0328672  0.0246371  -123.101  &lt; 2e-16 ***\nDESTIN_SZSGSZ01 -0.1933777  0.0068235   -28.340  &lt; 2e-16 ***\nDESTIN_SZSGSZ02 -0.3000845  0.0060284   -49.779  &lt; 2e-16 ***\nDESTIN_SZSGSZ03 -0.4322879  0.0057308   -75.433  &lt; 2e-16 ***\nDESTIN_SZSGSZ04 -0.1214792  0.0056548   -21.482  &lt; 2e-16 ***\nDESTIN_SZSGSZ05 -2.0309074  0.0114993  -176.611  &lt; 2e-16 ***\nDESTIN_SZSGSZ06  0.6592095  0.0046364   142.182  &lt; 2e-16 ***\nDESTIN_SZSGSZ07 -0.4618538  0.0062027   -74.460  &lt; 2e-16 ***\nDESTIN_SZSISZ01 -0.5227257  0.0293399   -17.816  &lt; 2e-16 ***\nDESTIN_SZSKSZ01 -0.4797341  0.0091087   -52.668  &lt; 2e-16 ***\nDESTIN_SZSKSZ02  0.8477357  0.0067821   124.996  &lt; 2e-16 ***\nDESTIN_SZSKSZ03 -0.2477566  0.0074817   -33.115  &lt; 2e-16 ***\nDESTIN_SZSKSZ04 -1.3315992  0.0167055   -79.710  &lt; 2e-16 ***\nDESTIN_SZSKSZ05 -0.3519096  0.0131326   -26.797  &lt; 2e-16 ***\nDESTIN_SZSLSZ01 -0.8570431  0.0102100   -83.941  &lt; 2e-16 ***\nDESTIN_SZSLSZ04 -0.9949105  0.0088280  -112.699  &lt; 2e-16 ***\nDESTIN_SZSRSZ01 -1.0260696  0.0154393   -66.458  &lt; 2e-16 ***\nDESTIN_SZTHSZ01 -4.2040410  0.0404795  -103.856  &lt; 2e-16 ***\nDESTIN_SZTHSZ03 -2.4907000  0.0264056   -94.325  &lt; 2e-16 ***\nDESTIN_SZTHSZ04 -3.0701470  0.0244975  -125.325  &lt; 2e-16 ***\nDESTIN_SZTHSZ06 -2.5308161  0.0169699  -149.135  &lt; 2e-16 ***\nDESTIN_SZTMSZ01 -0.2354889  0.0067201   -35.042  &lt; 2e-16 ***\nDESTIN_SZTMSZ02  1.7379292  0.0044573   389.906  &lt; 2e-16 ***\nDESTIN_SZTMSZ03  0.9112458  0.0048718   187.043  &lt; 2e-16 ***\nDESTIN_SZTMSZ04  1.0731075  0.0048626   220.685  &lt; 2e-16 ***\nDESTIN_SZTMSZ05  0.6398583  0.0067321    95.046  &lt; 2e-16 ***\nDESTIN_SZTNSZ01 -0.3500456  0.0083835   -41.754  &lt; 2e-16 ***\nDESTIN_SZTNSZ02 -1.0573515  0.0112412   -94.060  &lt; 2e-16 ***\nDESTIN_SZTNSZ03 -1.4069979  0.0132733  -106.002  &lt; 2e-16 ***\nDESTIN_SZTNSZ04 -0.3616604  0.0085207   -42.445  &lt; 2e-16 ***\nDESTIN_SZTPSZ01 -0.5919243  0.0071153   -83.190  &lt; 2e-16 ***\nDESTIN_SZTPSZ02  0.7083350  0.0046540   152.198  &lt; 2e-16 ***\nDESTIN_SZTPSZ03 -0.5746433  0.0069625   -82.534  &lt; 2e-16 ***\nDESTIN_SZTPSZ04 -1.5821259  0.0084517  -187.196  &lt; 2e-16 ***\nDESTIN_SZTPSZ05 -1.1796256  0.0073039  -161.505  &lt; 2e-16 ***\nDESTIN_SZTPSZ06 -0.3968272  0.0077295   -51.339  &lt; 2e-16 ***\nDESTIN_SZTPSZ07 -2.1796617  0.0135199  -161.219  &lt; 2e-16 ***\nDESTIN_SZTPSZ08 -1.2568483  0.0107267  -117.170  &lt; 2e-16 ***\nDESTIN_SZTPSZ09 -0.2446623  0.0080840   -30.265  &lt; 2e-16 ***\nDESTIN_SZTPSZ10 -1.2542191  0.0102049  -122.904  &lt; 2e-16 ***\nDESTIN_SZTPSZ11 -0.0886883  0.0062888   -14.102  &lt; 2e-16 ***\nDESTIN_SZTPSZ12 -0.7211823  0.0075086   -96.048  &lt; 2e-16 ***\nDESTIN_SZTSSZ01 -1.6271921  0.0238498   -68.227  &lt; 2e-16 ***\nDESTIN_SZTSSZ02 -0.3340439  0.0169137   -19.750  &lt; 2e-16 ***\nDESTIN_SZTSSZ03  0.3924580  0.0111060    35.338  &lt; 2e-16 ***\nDESTIN_SZTSSZ04  0.4169932  0.0114926    36.283  &lt; 2e-16 ***\nDESTIN_SZTSSZ05  1.3206287  0.0120381   109.704  &lt; 2e-16 ***\nDESTIN_SZTSSZ06  2.4023725  0.0192840   124.579  &lt; 2e-16 ***\nDESTIN_SZWCSZ01  2.0697378  0.0061379   337.206  &lt; 2e-16 ***\nDESTIN_SZWCSZ02 -2.0934025  0.0134782  -155.318  &lt; 2e-16 ***\nDESTIN_SZWCSZ03 -3.0670149  0.0349748   -87.692  &lt; 2e-16 ***\nDESTIN_SZWDSZ01  1.0113215  0.0051461   196.522  &lt; 2e-16 ***\nDESTIN_SZWDSZ02 -1.3383793  0.0076482  -174.993  &lt; 2e-16 ***\nDESTIN_SZWDSZ03  0.3394361  0.0060396    56.202  &lt; 2e-16 ***\nDESTIN_SZWDSZ04 -0.8324928  0.0086019   -96.780  &lt; 2e-16 ***\nDESTIN_SZWDSZ05 -0.8279090  0.0083251   -99.447  &lt; 2e-16 ***\nDESTIN_SZWDSZ06 -0.2252899  0.0061074   -36.888  &lt; 2e-16 ***\nDESTIN_SZWDSZ07 -1.3638599  0.0077990  -174.877  &lt; 2e-16 ***\nDESTIN_SZWDSZ08 -0.4350176  0.0077566   -56.083  &lt; 2e-16 ***\nDESTIN_SZWDSZ09  0.5461048  0.0060745    89.901  &lt; 2e-16 ***\nDESTIN_SZYSSZ01  0.0243093  0.0053476     4.546 5.47e-06 ***\nDESTIN_SZYSSZ02 -0.3398962  0.0065947   -51.540  &lt; 2e-16 ***\nDESTIN_SZYSSZ03 -0.3694187  0.0074032   -49.900  &lt; 2e-16 ***\nDESTIN_SZYSSZ04 -0.5222848  0.0067396   -77.495  &lt; 2e-16 ***\nDESTIN_SZYSSZ05 -1.5460539  0.0124899  -123.784  &lt; 2e-16 ***\nDESTIN_SZYSSZ06 -1.5556892  0.0127640  -121.881  &lt; 2e-16 ***\nDESTIN_SZYSSZ07 -0.8673403  0.0167723   -51.713  &lt; 2e-16 ***\nDESTIN_SZYSSZ08  0.5389364  0.0052540   102.577  &lt; 2e-16 ***\nDESTIN_SZYSSZ09  0.1199483  0.0055235    21.716  &lt; 2e-16 ***\nlog(dist)       -1.8906989  0.0005319 -3554.786  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 36117615  on 14273  degrees of freedom\nResidual deviance:  8091747  on 13715  degrees of freedom\nAIC: 8177420\n\nNumber of Fisher Scoring iterations: 7\n\n\nWe can examine how the constraints hold for destinations this time.\n\nCalcRSquared(dbcSIM$data$TRIPS, dbcSIM$fitted.values)\n\n[1] 0.6883675\n\n\nNotice that there is a relatively greater improvement in the R^2 value.\n\n\n8.9 Model comparison\nAnother useful model performance measure for continuous dependent variable is Root Mean Squared Error. In this sub-section, you will learn how to use compare_performance() of performance package\nFirst of all, let us create a list called model_list by using the code chun below.\n\nmodel_list &lt;- list(unconstrained=uncSIM,\n                   originConstrained=orcSIM,\n                   destinationConstrained=decSIM,\n                   doublyConstrained=dbcSIM)\n\nNext, we will compute the RMSE of all the models in model_list file by using the code chunk below.\n\ncompare_performance(model_list,\n                    metrics = \"RMSE\")\n\n# Comparison of Model Performance Indices\n\nName                   | Model |     RMSE\n-----------------------------------------\nunconstrained          |   glm | 2429.978\noriginConstrained      |   glm | 2057.579\ndestinationConstrained |   glm | 1891.724\ndoublyConstrained      |   glm | 1487.111\n\n\nThe print above reveals that doubly constrained SIM is the best model among all the four SIMs because it has the smallest RMSE value of 1487.111.\n\n\n8.10 Visualising fitted\nIn this section, you will learn how to visualise the observed values and the fitted values.\nFirstly we will extract the fitted values from each model by using the code chunk below.\n\ndf &lt;- as.data.frame(uncSIM$fitted.values) %&gt;%\n  round(digits = 0)\n\nNext, we will join the values to SIM_data data frame.\n\nSIM_data &lt;- SIM_data %&gt;%\n  cbind(df) %&gt;%\n  rename(uncTRIPS = \"uncSIM$fitted.values\")\n\nRepeat the same step by for Origin Constrained SIM (i.e. orcSIM)\n\ndf &lt;- as.data.frame(orcSIM$fitted.values) %&gt;%\n  round(digits = 0)\n\n\nSIM_data &lt;- SIM_data %&gt;%\n  cbind(df) %&gt;%\n  rename(orcTRIPS = \"orcSIM$fitted.values\")\n\nRepeat the same step by for Destination Constrained SIM (i.e. decSIM)\n\ndf &lt;- as.data.frame(decSIM$fitted.values) %&gt;%\n  round(digits = 0)\n\n\nSIM_data &lt;- SIM_data %&gt;%\n  cbind(df) %&gt;%\n  rename(decTRIPS = \"decSIM$fitted.values\")\n\nRepeat the same step by for Doubly Constrained SIM (i.e. dbcSIM)\n\ndf &lt;- as.data.frame(dbcSIM$fitted.values) %&gt;%\n  round(digits = 0)\n\n\nSIM_data &lt;- SIM_data %&gt;%\n  cbind(df) %&gt;%\n  rename(dbcTRIPS = \"dbcSIM$fitted.values\")\n\n\nunc_p &lt;- ggplot(data = SIM_data,\n                aes(x = uncTRIPS,\n                    y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\norc_p &lt;- ggplot(data = SIM_data,\n                aes(x = orcTRIPS,\n                    y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\ndec_p &lt;- ggplot(data = SIM_data,\n                aes(x = decTRIPS,\n                    y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\ndbc_p &lt;- ggplot(data = SIM_data,\n                aes(x = dbcTRIPS,\n                    y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\nggarrange(unc_p, orc_p, dec_p, dbc_p,\n          ncol = 2,\n          nrow = 2)"
  },
  {
    "objectID": "In-class_Exercise/In-class_Ex3/data/geospatial/MPSZ-2019.html",
    "href": "In-class_Exercise/In-class_Ex3/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624 - Applied Geospatial Analytics",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#the-study-area-and-data",
    "href": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#the-study-area-and-data",
    "title": "Take-home_Ex1: Public Bus Passengers in Singapore",
    "section": "2 The Study Area and Data",
    "text": "2 The Study Area and Data\n\n2.1 Aspatial Data\nThe aspatial data used in this take-home exercise is extracted from LTA DataMall (Passenger Volume by Origin Destination Bus Stops) for the month of October 2023.\n\n\n2.2 Geospatial Data\nThe geospatial data used in this take-home exercise are as follows.\n\nBus Stop Location from LTA DataMall, which provides information about all the bus stops currently being serviced by buses, including the bus stop code (identifier) and location coordinates.\nhexagon, a hexagon layer of 250m perpendicular distance between the centre of the hexagon and its edges is used to replace the relative coarse and irregular Master Plan 2019 Planning Sub-zone GIS data set of URA."
  },
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#setting-the-analytical-tools",
    "href": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#setting-the-analytical-tools",
    "title": "Take-home_Ex1: Public Bus Passengers in Singapore",
    "section": "3 Setting the Analytical Tools",
    "text": "3 Setting the Analytical Tools\nBefore I get started, I need to ensure that sf, spdep, tmap, tidyverse, and knitr packages of R are currently installed in my R.\n\nsf : for importing and handling geospatial data in R,\nspdep : for computing spatial weights, global and local spatial autocorrelation statistics, and\ntmap : for preparing cartographic quality chropleth map\ntidyverse : for wrangling attribute data in R ; tidyverse has already included collection of packages such as readr, ggplot2, dplyr, tiblle, purr, etc.\nknitr: for facilitating dynamic report generation in R Markdown documents.\n\nThe code chunk below is used to ensure that the necessary R packages have been installed , if its iyet to be installed, it will then be installed and ready to be used in the R environment.\n\n\nShow the code\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)"
  },
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#creating-honeycomb_grid",
    "href": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#creating-honeycomb_grid",
    "title": "Take-home_Ex1: Public Bus Passengers in Singapore",
    "section": "6 Creating honeycomb_grid",
    "text": "6 Creating honeycomb_grid\nHoneycomb grid are preferred to replace coarse and irregular Master Plan 2019 Sub-zone GIS data set of URA because hexagon reduce sampling bias due to its grid shape of low perimeter to are ratio and its ability to form evenly spaced grid. Honeycomb grids are well-suited for approximating circular areas, making them suitable for mapping Singapore edges with is irregular shape.\nThe code chunk below uses st_make_grid of sf package to create a hexagonal or honeycomb grid with a 250m (perpendicular distance between the center of hexagon and its edges). According the the R documentation, the cellsize is the distance between opposite edges, which is 2 times the perpendicular distance between the center of hexagon and its edges. Thus, for the purpose of this exercise, I will use the cellsize of 500m and indicate the square=FALSE for hexagonal grid. After doing do, I will create a grid_id for each hexagonal grid.\n\narea_honeycomb_grid = st_make_grid(busstop3414, c(500, 500), what = \"polygons\", square = FALSE)    \n# To sf and add grid ID  \nhoneycomb_grid_sf = st_sf(area_honeycomb_grid) %&gt;%    \n  # add grid ID      \n  mutate(grid_id = 1:length(lengths(area_honeycomb_grid)))"
  },
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#extracting-the-study-data",
    "href": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#extracting-the-study-data",
    "title": "Take-home_Ex1: Public Bus Passengers in Singapore",
    "section": "7 Extracting the study data",
    "text": "7 Extracting the study data\nIn this exercise, I will extract the commuting flows during peak hours as follows.\n\n\n\n\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\n\n7.1 Weekday Morning Peak\nThe code chunk below will be used to extract the weekday morning peak (Weekday: 6-9am) and calculate the passenger trips in each origin bus stop by using the group_by() from dplyr package and aggregate the values using summarise() and sum up the “Total_Trips”. The mutate() in the code below is to ensure that after the group_by, the ORIGIN_PT_CODE remains in the factor data type.\n\n\nShow the code\nodbus_weekday_6_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))%&gt;%\n  mutate(ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE))\n\n\nI will repeat the processes above for the other peak hours as shown below.\n\n\n7.2 Weekday Afternoon Peak\nThe code chunk below will be used to extract the weekday afternoon peak (Weekday: 5-8pm) and calculate the passenger trips in each origin bus stop.\n\n\nShow the code\nodbus_weekday_17_20 &lt;- odbus %&gt;%   \n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%   \n  filter(TIME_PER_HOUR &gt;= 17 &            \n           TIME_PER_HOUR &lt;= 20) %&gt;%   \n  group_by(ORIGIN_PT_CODE) %&gt;%   \n  summarise(TRIPS = sum(TOTAL_TRIPS)) %&gt;%\n  mutate(ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE))\n\n\n\n\n7.3 Weekends/Holiday Morning Peak\nThe code chunk below will be used to extract the weekend/holiday morning peak (Weekend/holiday: 11am-2pm) and calculate the passenger trips in each origin bus stop.\n\n\nShow the code\nodbus_weekend_11_14 &lt;- odbus %&gt;% \n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%   \n  filter(TIME_PER_HOUR &gt;= 11 &            \n           TIME_PER_HOUR &lt;= 14) %&gt;%   \n  group_by(ORIGIN_PT_CODE) %&gt;%   \n  summarise(TRIPS = sum(TOTAL_TRIPS)) %&gt;%\n  mutate(ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE))\n\n\n\n\n7.4 Weekends/Holiday Evening Peak\nThe code chunk below will be used to extract the weekend/holiday evening peak (Weekend/holiday: 4-7pm) and calculate the total trips in each origin and destination pair.\n\n\nShow the code\nodbus_weekend_16_19 &lt;- odbus %&gt;%   \n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%   \n  filter(TIME_PER_HOUR &gt;= 16 &            \n           TIME_PER_HOUR &lt;= 19) %&gt;%   \n  group_by(ORIGIN_PT_CODE) %&gt;%   \n  summarise(TRIPS = sum(TOTAL_TRIPS)) %&gt;%\n  mutate(ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE))"
  },
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#further-data-wrangling-for-weekday-morning-peak-hour-a-step-by-step-guide",
    "href": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#further-data-wrangling-for-weekday-morning-peak-hour-a-step-by-step-guide",
    "title": "Take-home_Ex1: Public Bus Passengers in Singapore",
    "section": "8 Further Data Wrangling for Weekday Morning Peak Hour: A Step-by-Step Guide",
    "text": "8 Further Data Wrangling for Weekday Morning Peak Hour: A Step-by-Step Guide\nThis section provides a comprehensive step-by step walkthrough to calculate the number of trips within each hexagonal grid during Weekday Morning Peak Hour with a subsequent plan to replicate the same process for Weekday Afternoon Peak Hour, Weekends/Holiday Morning Peak, and Weekends/Holiday Evening Peak in the subsequent section\n\n8.1 Performing Relational Join\nThe code chunk below will be used to join the busstop3414 SpatialPolygonsDataframe and odbus_weekday_6_9_data by BUS_STOP_N for busstop3414 and BUS_STOP_ID for original_destination_bus. This is performed by using left_join() of dplyr package. In this take-home exercise, I will focus on passenger trips generated by origin bus stop, I will remove the rows with bus stops solely serve as destinations which are indicated by NA values on the corresponding “Total_Trips” using the filter() from dplyr package.\n\n\nShow the code\ntotal_trips_per_busstop_wdmp &lt;- left_join(busstop3414, odbus_weekday_6_9, by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\")) %&gt;%\n  filter(!is.na(TRIPS))\n\n\n\n\n8.2 Spatial Join with Hexagonal Honeycomb Grid and Calculating Total Trips in a Hexagonal Grid\nThe code chunk below will be used to join the total_trips_per_busstop and honeycomb grid spatially using st_join() from sf package and remove the hexagon grid without any bus stop which is indicated by NA value on the “BUS_STOP_N”. Next, I will calculate the total trips in a hexagonal grid using the group_by() from dplyr package.\n\n\nShow the code\ntotal_trips_per_grid_wdmp &lt;- st_join(honeycomb_grid_sf,total_trips_per_busstop_wdmp) %&gt;%\n  filter(!is.na(BUS_STOP_N))%&gt;%\n  group_by(grid_id) %&gt;%\n  summarise(total_trips = sum(TRIPS))\n\n\n\n\n8.3 Replicating the Steps for Other Peak Hours\n\n8.3.1 Weekday Afternoon Peak\n\n\nShow the code\ntotal_trips_per_busstop_wdap &lt;- left_join(busstop3414, odbus_weekday_17_20, by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\"))%&gt;%\n  filter(!is.na(TRIPS))\n\ntotal_trips_per_grid_wdap &lt;- st_join(honeycomb_grid_sf,total_trips_per_busstop_wdap) %&gt;%\n  filter(!is.na(BUS_STOP_N))%&gt;%\n  group_by(grid_id) %&gt;%\n  summarise(total_trips = sum(TRIPS))\n\n\n\n\n8.3.2 Weekends/Holiday Morning Peak\n\n\nShow the code\ntotal_trips_per_busstop_wemp &lt;- left_join(busstop3414, odbus_weekend_11_14, by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\"))%&gt;%\n  filter(!is.na(TRIPS))\n\ntotal_trips_per_grid_wemp &lt;- st_join(honeycomb_grid_sf,total_trips_per_busstop_wemp) %&gt;%\n  filter(!is.na(BUS_STOP_N))%&gt;%\n  group_by(grid_id) %&gt;%\n  summarise(total_trips = sum(TRIPS))\n\n\n\n\n8.3.3 Weekends/Holiday Evening Peak\n\n\nShow the code\ntotal_trips_per_busstop_weep &lt;- left_join(busstop3414, odbus_weekend_16_19, by = c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\"))%&gt;%\n  filter(!is.na(TRIPS))\n\ntotal_trips_per_grid_weep &lt;- st_join(honeycomb_grid_sf,total_trips_per_busstop_weep) %&gt;%\n  filter(!is.na(BUS_STOP_N))%&gt;%\n  group_by(grid_id) %&gt;%\n  summarise(total_trips = sum(TRIPS))"
  },
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#exploratory-spatial-data-analysis",
    "href": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#exploratory-spatial-data-analysis",
    "title": "Take-home_Ex1: Public Bus Passengers in Singapore",
    "section": "10 Exploratory Spatial Data Analysis",
    "text": "10 Exploratory Spatial Data Analysis\nExploratory Spatial Data Analysis or ESDA consists of descriptive techniques to discover spatial distribution of data and identify outliers. In this ESDA section, I will cover global spatial autocorrelation which focuses on overall trend and local spatial autocorrelation which focuses on hot and cold spots in the data.\n\n10.1 Global Spatial Autocorrelation\nIn this section, I will include the computation of global spatial autocorrelation statistics and spatial complete randomness test for global spatial autocorrelation. The goal here is to understand whether the passenger trips generated by origin are evenly distributed across Singapore.\n\n10.1.1 Spatial Weights Matrix\nBefore computing global spatial autocorrelation, we need to define spatial neighbourhood by using spatial weight. There are two common methods to compute spatial weight which are contiguity-based and distanced-based.\nIn contiguity-based method, neighbour share common boundary and there are 2 methods in defining the boundary, ROOK by common edge while QUEEN by common edge and vertices as shown below for square shape.\n\n\n\nHook Neighbors\n\n\n\n\n\nQueen Neighbors\n\n\nIn hexagonal grid, finding neighbours are straighforward. Both ROOK and QUEEN yield the same results as shown below.\n\n\n\nHexagon Neighbors\n\n\nIn distance-based method, there are 2 method fixed weighting where the grid are considered neighbours if they are within specified distance from one another and adaptive weighting where each grid has same specified number of neighbours.\nIf the hexagonal grid are isolated from each other, contiguity-based method may not be appropriate as it may yield many grids with no neighbours.\n\n\n10.1.2 Contiguity Weight Matrix (QUEEN)\nIn the code chunk below, I will use poly2nb() of spdep package to compute contiguity weight matrices for weekday morning peak hour. This function builds a list of neighbours based on grids with contiguous boundaries. By default, Queen contiguity is applied.\n\n\nShow the code\nwm_q &lt;- poly2nb(total_trips_per_grid_wdmp, queen=TRUE)\nsummary(wm_q)\n\n\nNeighbour list object:\nNumber of regions: 1492 \nNumber of nonzero links: 6714 \nPercentage nonzero weights: 0.3016086 \nAverage number of links: 4.5 \n12 regions with no links:\n276 296 454 550 713 964 1030 1387 1477 1480 1484 1492\nLink number distribution:\n\n  0   1   2   3   4   5   6 \n 12  40 105 206 285 358 486 \n40 least connected regions:\n1 7 22 38 98 166 183 184 185 191 207 214 253 257 260 551 595 629 683 695 719 738 755 771 855 990 1004 1005 1029 1069 1194 1436 1443 1454 1472 1473 1475 1478 1482 1491 with 1 link\n486 most connected regions:\n10 13 16 17 24 25 31 35 42 43 48 53 55 60 63 67 73 77 80 81 84 85 87 88 91 92 97 102 107 111 117 121 127 132 138 139 141 145 146 147 151 152 153 154 160 161 162 170 171 172 179 180 181 187 188 189 190 196 197 198 201 202 203 204 212 225 235 239 240 242 252 268 272 280 287 289 290 291 293 299 300 302 303 306 311 314 315 317 327 328 329 330 333 342 345 353 358 380 381 390 392 393 397 404 408 413 415 421 426 427 428 430 433 440 441 442 450 456 457 458 459 463 467 470 471 478 482 483 485 491 492 496 502 503 506 507 512 518 523 528 532 537 538 539 541 545 547 553 557 562 563 565 566 570 579 580 583 587 588 592 593 597 603 607 611 613 620 623 624 625 635 636 637 641 642 644 645 646 656 657 658 664 667 668 669 674 675 677 678 687 688 691 692 693 700 703 704 711 714 715 716 727 728 742 744 745 747 758 761 762 763 764 769 770 774 775 776 779 780 781 782 786 787 792 793 796 797 798 799 805 806 809 810 811 816 817 818 827 829 830 832 833 834 836 837 838 839 840 846 849 851 852 853 857 858 862 863 864 866 867 868 870 871 874 877 879 882 885 888 890 891 895 899 904 906 911 912 913 915 916 920 928 929 930 931 932 933 938 942 943 946 947 952 953 955 956 957 961 967 968 969 970 971 973 979 980 981 982 987 994 995 996 997 1007 1008 1009 1011 1012 1019 1020 1021 1025 1033 1034 1037 1039 1040 1045 1046 1047 1049 1050 1051 1052 1059 1061 1062 1063 1066 1072 1076 1083 1084 1085 1088 1089 1093 1094 1100 1103 1104 1105 1111 1116 1117 1118 1119 1124 1125 1127 1128 1129 1130 1131 1133 1139 1140 1141 1145 1146 1147 1149 1151 1152 1153 1154 1158 1159 1160 1161 1162 1166 1168 1171 1172 1173 1174 1175 1181 1182 1183 1184 1185 1186 1187 1190 1191 1197 1198 1199 1200 1201 1207 1213 1214 1215 1219 1224 1225 1231 1233 1234 1235 1240 1244 1245 1250 1251 1252 1256 1259 1261 1267 1276 1279 1280 1293 1298 1299 1300 1301 1303 1304 1305 1308 1309 1311 1317 1326 1328 1329 1337 1338 1340 1343 1344 1349 1352 1353 1354 1356 1360 1363 1365 1367 1370 1378 1380 1384 1389 1390 1392 1396 1397 1398 1399 1400 1405 1406 1407 1408 1412 1413 1414 1418 1419 1421 1423 1425 1428 1429 1431 1432 1433 1434 1441 with 6 links\n\n\nThe summary report above shows that there are 1492 hexagonal grids. There are 12 hexagonal grid with no neighbour, 40 hexagonal grids with 1 neighbour and the most connected grids have 6 links. The average number of links is 4.5.\n\n\n10.1.3 Contiguity Weight Matrix (ROOK)\nIn the code chunk below, I will use poly2nb() of spdep package to compute contiguity weight matrices for weekday morning peak hour by specifying queen = FALSE to compute Rook contiguity.\n\n\nShow the code\nwm_r &lt;- poly2nb(total_trips_per_grid_wdmp, queen=FALSE)\nsummary(wm_r)\n\n\nNeighbour list object:\nNumber of regions: 1492 \nNumber of nonzero links: 6714 \nPercentage nonzero weights: 0.3016086 \nAverage number of links: 4.5 \n12 regions with no links:\n276 296 454 550 713 964 1030 1387 1477 1480 1484 1492\nLink number distribution:\n\n  0   1   2   3   4   5   6 \n 12  40 105 206 285 358 486 \n40 least connected regions:\n1 7 22 38 98 166 183 184 185 191 207 214 253 257 260 551 595 629 683 695 719 738 755 771 855 990 1004 1005 1029 1069 1194 1436 1443 1454 1472 1473 1475 1478 1482 1491 with 1 link\n486 most connected regions:\n10 13 16 17 24 25 31 35 42 43 48 53 55 60 63 67 73 77 80 81 84 85 87 88 91 92 97 102 107 111 117 121 127 132 138 139 141 145 146 147 151 152 153 154 160 161 162 170 171 172 179 180 181 187 188 189 190 196 197 198 201 202 203 204 212 225 235 239 240 242 252 268 272 280 287 289 290 291 293 299 300 302 303 306 311 314 315 317 327 328 329 330 333 342 345 353 358 380 381 390 392 393 397 404 408 413 415 421 426 427 428 430 433 440 441 442 450 456 457 458 459 463 467 470 471 478 482 483 485 491 492 496 502 503 506 507 512 518 523 528 532 537 538 539 541 545 547 553 557 562 563 565 566 570 579 580 583 587 588 592 593 597 603 607 611 613 620 623 624 625 635 636 637 641 642 644 645 646 656 657 658 664 667 668 669 674 675 677 678 687 688 691 692 693 700 703 704 711 714 715 716 727 728 742 744 745 747 758 761 762 763 764 769 770 774 775 776 779 780 781 782 786 787 792 793 796 797 798 799 805 806 809 810 811 816 817 818 827 829 830 832 833 834 836 837 838 839 840 846 849 851 852 853 857 858 862 863 864 866 867 868 870 871 874 877 879 882 885 888 890 891 895 899 904 906 911 912 913 915 916 920 928 929 930 931 932 933 938 942 943 946 947 952 953 955 956 957 961 967 968 969 970 971 973 979 980 981 982 987 994 995 996 997 1007 1008 1009 1011 1012 1019 1020 1021 1025 1033 1034 1037 1039 1040 1045 1046 1047 1049 1050 1051 1052 1059 1061 1062 1063 1066 1072 1076 1083 1084 1085 1088 1089 1093 1094 1100 1103 1104 1105 1111 1116 1117 1118 1119 1124 1125 1127 1128 1129 1130 1131 1133 1139 1140 1141 1145 1146 1147 1149 1151 1152 1153 1154 1158 1159 1160 1161 1162 1166 1168 1171 1172 1173 1174 1175 1181 1182 1183 1184 1185 1186 1187 1190 1191 1197 1198 1199 1200 1201 1207 1213 1214 1215 1219 1224 1225 1231 1233 1234 1235 1240 1244 1245 1250 1251 1252 1256 1259 1261 1267 1276 1279 1280 1293 1298 1299 1300 1301 1303 1304 1305 1308 1309 1311 1317 1326 1328 1329 1337 1338 1340 1343 1344 1349 1352 1353 1354 1356 1360 1363 1365 1367 1370 1378 1380 1384 1389 1390 1392 1396 1397 1398 1399 1400 1405 1406 1407 1408 1412 1413 1414 1418 1419 1421 1423 1425 1428 1429 1431 1432 1433 1434 1441 with 6 links\n\n\nThe summary report above shows that there are 1492 hexagonal grids. There are 12 hexagonal grid with no neighbour, 40 hexagonal grids with 1 neighbour and the most connected grids have 6 links. The average number of links is 4.5.\nNote: The results for both rook and queen method are the same as shown from the computation above.\n\n\n10.1.4 Visualising contiguity weights\nBefore visualising the weights, I need to reproject coordingate to WGS84 for longitude-latitude projection using st_transform of sf package.\n\n\nShow the code\ntotal_trips_per_grid_wdmp$area_honeycomb_grid &lt;- st_transform(total_trips_per_grid_wdmp$area_honeycomb_grid, \"+proj=longlat +datum=WGS84\")\n\n\nNext, I will get the coordinates of the hexagonal grid centroid in longitude and latitude using the st_coordinates and st_centroid of sf package.\n\n\nShow the code\ncoords &lt;- st_coordinates(st_centroid(total_trips_per_grid_wdmp$area_honeycomb_grid))\nhead(coords)\n\n\n            X        Y\n[1,] 103.6174 1.271424\n[2,] 103.6196 1.275340\n[3,] 103.6219 1.294920\n[4,] 103.6241 1.275341\n[5,] 103.6241 1.291005\n[6,] 103.6241 1.298837\n\n\nI will only plot queen contiguity as rook contiguity yields the same results.\n\n\nShow the code\nplot(total_trips_per_grid_wdmp$area_honeycomb_grid, border=\"lightgrey\", main=\"Queen and Rook Contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\n\n\n\n\n\n\n\n10.1.5 Fixed Distance Weight Matrix\nThe dnearneigh() of spdep package will be used to derive the distance-based weight matrices by . This function identifies neighbours of hexagonal grid centroid points by Euclidean distance with a lower and upper bounds distance controlled by the bounds argument or by Great Circle distance in kilometres if longlat argument is set to TRUE.\n\n10.1.5.1 Determine the cut-off distance\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n\nShow the code\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.5000  0.5000  0.5000  0.5072  0.5000  4.5825 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 4,582.5 metres, so using a number slightly larger than this (i.e. 4.6) as the upper threshold gives certainty that all regions will have at least one neighbour.\n\n\nShow the code\ntotal_trips_per_grid_wdmp$grid_id[match(max(k1dists), k1dists)]\n\n\n[1] 1767\n\n\nUsing the code chunk above, we discover that the grid_id with the maximum distance to its nearest neighbour is 1767 which is the grid in Johor.\n\n\n10.1.5.2 Computing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown below.\n\n\nShow the code\nwm_d4.6 &lt;- dnearneigh(coords, 0, 4.6, longlat = TRUE)\nwm_d4.6\n\n\nNeighbour list object:\nNumber of regions: 1492 \nNumber of nonzero links: 236430 \nPercentage nonzero weights: 10.62099 \nAverage number of links: 158.4651 \n\n\nFrom the output, we see that the average number of links is 158.4651. The number is quite high and may skew the analysis.\nNext, we will use str() to display the content of wm_d4.6 weight matrix.\n\n\nShow the code\nstr(wm_d4.6)\n\n\nList of 1492\n $ : int [1:30] 2 3 4 5 6 7 8 9 10 11 ...\n $ : int [1:33] 1 3 4 5 6 7 8 9 10 11 ...\n $ : int [1:63] 1 2 4 5 6 7 8 9 10 11 ...\n $ : int [1:35] 1 2 3 5 6 7 8 9 10 11 ...\n $ : int [1:61] 1 2 3 4 6 7 8 9 10 11 ...\n $ : int [1:74] 1 2 3 4 5 7 8 9 10 11 ...\n $ : int [1:86] 1 2 3 4 5 6 8 9 10 11 ...\n $ : int [1:43] 1 2 3 4 5 6 7 9 10 11 ...\n $ : int [1:57] 1 2 3 4 5 6 7 8 10 11 ...\n $ : int [1:72] 1 2 3 4 5 6 7 8 9 11 ...\n $ : int [1:91] 1 2 3 4 5 6 7 8 9 10 ...\n $ : int [1:52] 1 2 3 4 5 6 7 8 9 10 ...\n $ : int [1:69] 1 2 3 4 5 6 7 8 9 10 ...\n $ : int [1:84] 1 2 3 4 5 6 7 8 9 10 ...\n $ : int [1:94] 2 3 4 5 6 7 8 9 10 11 ...\n $ : int [1:66] 1 2 3 4 5 6 7 8 9 10 ...\n $ : int [1:82] 1 2 3 4 5 6 7 8 9 10 ...\n $ : int [1:94] 1 2 3 4 5 6 7 8 9 10 ...\n $ : int [1:98] 1 2 3 4 5 6 7 8 9 10 ...\n $ : int [1:93] 3 5 6 7 8 9 10 11 12 13 ...\n $ : int [1:85] 3 5 6 7 9 10 11 13 14 15 ...\n $ : int [1:43] 1 2 3 4 5 6 7 8 9 10 ...\n $ : int [1:60] 1 2 3 4 5 6 7 8 9 10 ...\n $ : int [1:77] 1 2 3 4 5 6 7 8 9 10 ...\n $ : int [1:91] 1 2 3 4 5 6 7 8 9 10 ...\n $ : int [1:99] 1 2 3 4 5 6 7 8 9 10 ...\n $ : int [1:99] 2 3 4 5 6 7 8 9 10 11 ...\n $ : int [1:94] 3 5 6 7 9 10 11 12 13 14 ...\n $ : int [1:84] 3 5 6 7 10 11 13 14 15 17 ...\n $ : int [1:71] 1 2 3 4 5 6 7 8 9 10 ...\n $ : int [1:89] 1 2 3 4 5 6 7 8 9 10 ...\n $ : int [1:99] 1 2 3 4 5 6 7 8 9 10 ...\n $ : int [1:103] 2 3 4 5 6 7 8 9 10 11 ...\n $ : int [1:99] 3 5 6 7 8 9 10 11 12 13 ...\n $ : int [1:93] 3 5 6 7 9 10 11 13 14 15 ...\n $ : int [1:81] 3 6 7 10 11 14 15 17 18 19 ...\n $ : int [1:63] 7 11 15 18 19 20 21 26 27 28 ...\n $ : int [1:48] 1 2 3 4 5 6 7 8 9 10 ...\n $ : int [1:65] 1 2 3 4 5 6 7 8 9 10 ...\n $ : int [1:84] 1 2 3 4 5 6 7 8 9 10 ...\n $ : int [1:97] 1 2 3 4 5 6 7 8 9 10 ...\n $ : int [1:101] 3 5 6 7 9 10 11 12 13 14 ...\n $ : int [1:92] 3 5 6 7 10 11 13 14 15 17 ...\n $ : int [1:78] 6 7 11 14 15 18 19 20 21 25 ...\n $ : int [1:60] 7 11 15 19 20 21 26 27 28 29 ...\n $ : int [1:96] 1 2 3 4 5 6 7 8 9 10 ...\n $ : int [1:108] 3 5 6 7 8 9 10 11 12 13 ...\n $ : int [1:100] 3 5 6 7 9 10 11 13 14 15 ...\n $ : int [1:90] 6 7 10 11 14 15 17 18 19 20 ...\n $ : int [1:74] 7 11 15 18 19 20 21 26 27 28 ...\n $ : int [1:113] 3 5 6 7 8 9 10 11 12 13 ...\n $ : int [1:109] 3 5 6 7 9 10 11 12 13 14 ...\n $ : int [1:101] 3 6 7 10 11 13 14 15 17 18 ...\n $ : int [1:69] 11 15 19 20 21 26 27 28 29 33 ...\n $ : int [1:110] 3 5 6 7 10 11 13 14 15 16 ...\n $ : int [1:98] 6 7 11 14 15 17 18 19 20 21 ...\n $ : int [1:84] 7 11 15 18 19 20 21 26 27 28 ...\n $ : int [1:120] 3 5 6 7 8 9 10 11 12 13 ...\n $ : int [1:119] 3 5 6 7 9 10 11 13 14 15 ...\n $ : int [1:109] 6 7 10 11 14 15 17 18 19 20 ...\n $ : int [1:97] 7 11 15 18 19 20 21 25 26 27 ...\n $ : int [1:123] 3 5 6 7 9 10 11 12 13 14 ...\n $ : int [1:120] 3 6 7 10 11 13 14 15 17 18 ...\n $ : int [1:110] 7 11 14 15 18 19 20 21 25 26 ...\n $ : int [1:125] 3 4 5 6 7 8 9 10 11 12 ...\n $ : int [1:127] 3 5 6 7 10 11 13 14 15 16 ...\n $ : int [1:122] 6 7 11 14 15 17 18 19 20 21 ...\n $ : int [1:122] 3 4 5 6 7 8 9 10 11 12 ...\n $ : int [1:129] 3 5 6 7 8 9 10 11 12 13 ...\n $ : int [1:133] 3 5 6 7 9 10 11 13 14 15 ...\n $ : int [1:129] 6 7 10 11 14 15 17 18 19 20 ...\n $ : int [1:122] 7 11 15 18 19 20 21 25 26 27 ...\n $ : int [1:129] 3 5 6 7 8 9 10 11 12 13 ...\n $ : int [1:136] 3 5 6 7 9 10 11 12 13 14 ...\n $ : int [1:131] 7 11 14 15 18 19 20 21 25 26 ...\n $ : int [1:128] 3 5 6 7 8 9 10 11 12 13 ...\n $ : int [1:139] 3 5 6 7 9 10 11 12 13 14 ...\n $ : int [1:141] 6 7 10 11 13 14 15 16 17 18 ...\n $ : int [1:139] 7 11 14 15 17 18 19 20 21 25 ...\n $ : int [1:137] 3 5 6 7 9 10 11 12 13 14 ...\n $ : int [1:145] 6 7 10 11 13 14 15 16 17 18 ...\n $ : int [1:144] 7 11 14 15 17 18 19 20 21 24 ...\n $ : int [1:133] 3 5 6 7 9 10 11 12 13 14 ...\n $ : int [1:143] 6 7 10 11 13 14 15 16 17 18 ...\n $ : int [1:147] 7 11 14 15 17 18 19 20 21 24 ...\n $ : int [1:126] 5 6 7 9 10 11 12 13 14 15 ...\n $ : int [1:140] 6 7 10 11 13 14 15 16 17 18 ...\n $ : int [1:150] 7 11 14 15 17 18 19 20 21 24 ...\n $ : int [1:152] 11 15 18 19 20 21 25 26 27 28 ...\n $ : int [1:130] 10 11 13 14 15 16 17 18 19 20 ...\n $ : int [1:144] 11 14 15 17 18 19 20 21 24 25 ...\n $ : int [1:152] 11 15 18 19 20 21 25 26 27 28 ...\n $ : int [1:155] 15 19 20 21 26 27 28 29 32 33 ...\n $ : int [1:117] 13 14 16 17 18 19 23 24 25 26 ...\n $ : int [1:134] 14 15 17 18 19 20 24 25 26 27 ...\n $ : int [1:150] 15 18 19 20 21 25 26 27 28 29 ...\n $ : int [1:158] 15 19 20 21 26 27 28 29 32 33 ...\n $ : int [1:104] 16 17 18 23 24 25 26 30 31 32 ...\n $ : int [1:143] 18 19 20 25 26 27 28 31 32 33 ...\n  [list output truncated]\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:1492] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language dnearneigh(x = coords, d1 = 0, d2 = 4.6, longlat = TRUE)\n - attr(*, \"dnn\")= num [1:2] 0 4.6\n - attr(*, \"bounds\")= chr [1:2] \"GE\" \"LE\"\n - attr(*, \"nbtype\")= chr \"distance\"\n - attr(*, \"sym\")= logi TRUE\n\n\n\n\nShow the code\npar(mfrow = c(1,2))\nplot(total_trips_per_grid_wdmp$area_honeycomb_grid, border = \"lightgrey\",main=\"1st nearest neighbours\" )\nplot(k1, coords, add = TRUE, col = \"red\", length = 0.88, )\n\nplot(total_trips_per_grid_wdmp$area_honeycomb_grid, border = \"lightgrey\", main = \"Distance Link\")\nplot(wm_d4.6, coords, add = TRUE, pch = 19, cex = 0.6)\n\n\n\n\n\nDue to a high number of links, we have very dense graphs which make it difficult to interpret. However, we can still make some observations:\n\nThe above charts actually illustrates a characteristic of fixed distance weight matrix whereby the hexagonal grid of busttop origin in the centre of Singapore tend to have more neighbours and the edges of Singapore with lesser neighbours like Johor, Tanah Merah Coast.\nBased on the above charts, we can tell that the geographical areas of the regions in Singapore are highly connected by the bus.\n\n\n\n\n10.1.6 Adaptive Distance Weight Matrix\nTo overcome the issue of fixed distance weight matrix where there is uneven distribution of neighbours, we can use directly control the numbers of neighbours using k-nearest neighbours, as shown in the code chunk below.\nI will set k = 6 i.e., all hexagonal grids will have 6 neighbours for hexagonal grids.\n\n\nShow the code\nknn6 &lt;- knn2nb(knearneigh(coords, k=6))\nknn6\n\n\nNeighbour list object:\nNumber of regions: 1492 \nNumber of nonzero links: 8952 \nPercentage nonzero weights: 0.4021448 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\n\n\n10.1.7 Plotting distance based neighbours\n\n\nShow the code\npar(mfrow = c(1,2))\nplot(total_trips_per_grid_wdmp$area_honeycomb_grid, border = \"lightgrey\",main=\"6 nearest neighbours\" )\nplot(knn6, coords, add = TRUE, col = \"red\", length = 0.88, )\n\nplot(total_trips_per_grid_wdmp$area_honeycomb_grid, border = \"lightgrey\", main = \"Distance Link w KNN\")\nplot(knn6, coords, add = TRUE, pch = 19, cex = 0.6)\n\n\n\n\n\n\n\n10.1.8 Determining Which Weights Matrix to Use\nSelecting a spatial weight matrix is use is dependent on the geographical area of interest and the focus of the study. Contiguity-based is preferred for hexagonal grid with uniform sizes because contiguity matrices are well-suited for regular grids where neighboring units share common boundaries. However, in my case, there are some hexagonal grid with no neighbour making contiguity-based not preferable. Therefore, I will use distance-based methods with adaptive distance spatial weight matrix because fixed distance has disadvantage where some regions only have 1 neighbour, while others may have 158 neighbours.\n\n\n10.1.9 Row-Standardised Weights Matrix\nAfter selecting the weight matrix to use, I will now assign weights to each neighboring polygon. Each neighboring polygon will be assigned equal weight (style=“W”) by assigning the fraction 1/(#of neighbors) to each neighbouring area. This is also known as a row-standardised matrix where each row in the matrix sums to 1.\n\n\nShow the code\nrswm_knn6 &lt;- nb2listw(knn6,\n                   style = \"W\",\n                   zero.policy = TRUE)\nrswm_knn6\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 1492 \nNumber of nonzero links: 8952 \nPercentage nonzero weights: 0.4021448 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\nWeights style: W \nWeights constants summary:\n     n      nn   S0    S1       S2\nW 1492 2226064 1492 462.5 6048.333\n\n\nI will be using the row-standardised weight matrix for the next part of the analysis.\nNotes:\nThe input of nb2listw() must be an object of class nb. The syntax of the function has two major arguments, namely style and zero.poly.\n\nstyle can take values “W”, “B”, “C”, “U”, “minmax” and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nIf zero policy is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice.\n\nThe above processes are previously implemented on Weekday Morning Peak Hour. Next, I will implemented the same process as above for Weekday Afternoon Peak Hour, Weekends/Holiday Morning Peak Hour and Weekends/Holiday Evening Peak Hour\n\n\nShow the code\ntotal_trips_per_grid_wdap$area_honeycomb_grid &lt;- st_transform(total_trips_per_grid_wdap$area_honeycomb_grid, \"+proj=longlat +datum=WGS84\") \n\ncoords_wdap &lt;- st_coordinates(st_centroid(total_trips_per_grid_wdap$area_honeycomb_grid)) \nknn6_wdap &lt;- knn2nb(knearneigh(coords_wdap, k=6)) \nrswm_knn6_wdap &lt;- nb2listw(knn6_wdap,\n                   style = \"W\",\n                   zero.policy = TRUE)\n\n\n\n\nShow the code\ntotal_trips_per_grid_wdap$area_honeycomb_grid &lt;- st_transform(total_trips_per_grid_wdap$area_honeycomb_grid, \"+proj=longlat +datum=WGS84\")\ncoords_wdap &lt;- st_coordinates(st_centroid(total_trips_per_grid_wdap$area_honeycomb_grid)) \nknn6_wdap &lt;- knn2nb(knearneigh(coords_wdap, k=6)) \nrswm_knn6_wdap &lt;- nb2listw(knn6_wdap,\n                   style = \"W\",\n                   zero.policy = TRUE)\n\n\n\n\nShow the code\ntotal_trips_per_grid_wemp$area_honeycomb_grid &lt;- st_transform(total_trips_per_grid_wemp$area_honeycomb_grid, \"+proj=longlat +datum=WGS84\")\ncoords_wemp &lt;- st_coordinates(st_centroid(total_trips_per_grid_wemp$area_honeycomb_grid)) \nknn6_wemp &lt;- knn2nb(knearneigh(coords_wemp, k=6)) \nrswm_knn6_wemp &lt;- nb2listw(knn6_wemp,\n                   style = \"W\",\n                   zero.policy = TRUE)\n\n\n\n\nShow the code\ntotal_trips_per_grid_weep$area_honeycomb_grid &lt;- st_transform(total_trips_per_grid_weep$area_honeycomb_grid, \"+proj=longlat +datum=WGS84\") \ncoords_weep &lt;- st_coordinates(st_centroid(total_trips_per_grid_weep$area_honeycomb_grid)) \nknn6_weep &lt;- knn2nb(knearneigh(coords_weep, k=6)) \nrswm_knn6_weep &lt;- nb2listw(knn6_weep,\n                   style = \"W\",\n                   zero.policy = TRUE)\n\n\n\n\n10.1.10 Computing Global Spatial Autocorrelation Statistics\nThis in sub-section, I will use two methods: Moran’s I and Geary’s C to test the hypothesis the following hypothesis:\n\nH0: Observed spatial patterns of values is equally likely as any other spatial pattern i.e. data is randomly disbursed, no spatial pattern\nH1: Data is more spatially clustered than expected by chance alone.\n\nMoran’s I\nI will perform Moran’s I statistical testing by using moran.test() of spdep pacakage. Moran’s I describe how features differ from the values in the study area as a whole. The Moran I statistic ranges from -1 to 1. If the Moran I is:\n\npositive (I&gt;0): Clustered, observations tend to be similar\nnegative (I&lt;0): Disperse, observations tend to be dissimilar\napproximately zero: observations arranged randomly over space\n\nThe below code chunk will perform the Moran’s I test on the passenger trips generate by origin.\n\n\nShow the code\nmoran.test(total_trips_per_grid_wdmp$total_trips,\n           listw = rswm_knn6,\n           zero.policy = TRUE,\n           na.action = na.omit)\n\n\n\n    Moran I test under randomisation\n\ndata:  total_trips_per_grid_wdmp$total_trips  \nweights: rswm_knn6    \n\nMoran I statistic standard deviate = 14.627, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.2066719242     -0.0006706908      0.0002009369 \n\n\n\n\nShow the code\nmoran.test(total_trips_per_grid_wdap$total_trips,\n           listw = rswm_knn6_wdap,\n           zero.policy = TRUE,\n           na.action = na.omit)\n\n\n\n    Moran I test under randomisation\n\ndata:  total_trips_per_grid_wdap$total_trips  \nweights: rswm_knn6_wdap    \n\nMoran I statistic standard deviate = 4.5255, p-value = 3.013e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.0624977956     -0.0006702413      0.0001948330 \n\n\n\n\nShow the code\nmoran.test(total_trips_per_grid_wemp$total_trips,\n           listw = rswm_knn6_wemp,\n           zero.policy = TRUE,\n           na.action = na.omit)\n\n\n\n    Moran I test under randomisation\n\ndata:  total_trips_per_grid_wemp$total_trips  \nweights: rswm_knn6_wemp    \n\nMoran I statistic standard deviate = 11.152, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.1568590667     -0.0006684492      0.0001995208 \n\n\n\n\nShow the code\nmoran.test(total_trips_per_grid_weep$total_trips,\n           listw = rswm_knn6_weep,\n           zero.policy = TRUE,\n           na.action = na.omit)\n\n\n\n    Moran I test under randomisation\n\ndata:  total_trips_per_grid_weep$total_trips  \nweights: rswm_knn6_weep    \n\nMoran I statistic standard deviate = 7.5898, p-value = 1.602e-14\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.1059540543     -0.0006715917      0.0001973622 \n\n\nSince the p-value &lt; 0.05, I have sufficient statistical evidence to reject the null hypothesis at the 95% level of confidence. This means that data is more spatially clustered than expected by chance alone. Since Moran I statistics are larger than 0, the observation are clustered, observations tend to be similar.\nComputing Monte Carlo Moran’s I\nIf there are doubts that the assumptions of Moran’s I are true (normality and randomisation), a Monte Carlo simulation is used to perform a permutation test for Moran’s I.\nThe permutation tests consists of randomly reassigning the attribute values to a cell under the assumption of no spatial pattern. This random assignment is conducted n times. Each time, I will compute the Moran’s I to crerate an empirical distribution of Moran’s I under H0.\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep package with a total of 1000 simulation.\n\n\nShow the code\nset.seed(1234)\nbperm= moran.mc(total_trips_per_grid_wdmp$total_trips, \n                listw=rswm_knn6, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  total_trips_per_grid_wdmp$total_trips \nweights: rswm_knn6  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.20667, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\nShow the code\nset.seed(1234)\nbperm_wdap= moran.mc(total_trips_per_grid_wdap$total_trips, \n                listw=rswm_knn6_wdap, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm_wdap\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  total_trips_per_grid_wdap$total_trips \nweights: rswm_knn6_wdap  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.062498, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\nShow the code\nset.seed(1234)\nbperm_wemp= moran.mc(total_trips_per_grid_wemp$total_trips, \n                listw=rswm_knn6_wemp, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm_wemp\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  total_trips_per_grid_wemp$total_trips \nweights: rswm_knn6_wemp  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.15686, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\nShow the code\nset.seed(1234)\nbperm_weep= moran.mc(total_trips_per_grid_weep$total_trips, \n                listw=rswm_knn6_weep, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm_weep\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  total_trips_per_grid_weep$total_trips \nweights: rswm_knn6_weep  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.10595, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nSince the p-value is &lt; 0.05, we have sufficient statistical evidence to reject the null hypothesis at the 95% level of confidence. This means that data is more spatially clustered than expected by chance alone.\nGeary’s C\nGeary’s C considers the difference between respective observations meaning that it describe how features differ from their immediate neighbours. Geary’s C range from -1 to an undefined number above 1. If the Geary’s C is:\n\nLarge (c&gt;1): Dispersed, observations tend to be dissimilar\nSmall (c&lt;1): Clustered, observations tend to be similar\nc = 1: observations arranged randomly over space\n\nThe code chunk below performs Geary’s C test for spatial autocorrelation for passenger trips generate by origin using geary.test() of spdep pacakage.\n\n\nShow the code\ngeary.test(total_trips_per_grid_wdmp$total_trips, listw = rswm_knn6)\n\n\n\n    Geary C test under randomisation\n\ndata:  total_trips_per_grid_wdmp$total_trips \nweights: rswm_knn6 \n\nGeary C statistic standard deviate = 7.4283, p-value = 5.502e-14\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n     0.8176559658      1.0000000000      0.0006025712 \n\n\n\n\nShow the code\ngeary.test(total_trips_per_grid_wdap$total_trips, listw = rswm_knn6_wdap)\n\n\n\n    Geary C test under randomisation\n\ndata:  total_trips_per_grid_wdap$total_trips \nweights: rswm_knn6_wdap \n\nGeary C statistic standard deviate = 0.88403, p-value = 0.1883\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n     0.9722463494      1.0000000000      0.0009856147 \n\n\n\n\nShow the code\ngeary.test(total_trips_per_grid_wemp$total_trips, listw = rswm_knn6_wemp)\n\n\n\n    Geary C test under randomisation\n\ndata:  total_trips_per_grid_wemp$total_trips \nweights: rswm_knn6_wemp \n\nGeary C statistic standard deviate = 5.3249, p-value = 5.049e-08\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n      0.864653629       1.000000000       0.000646046 \n\n\n\n\nShow the code\ngeary.test(total_trips_per_grid_weep$total_trips, listw = rswm_knn6_weep)\n\n\n\n    Geary C test under randomisation\n\ndata:  total_trips_per_grid_weep$total_trips \nweights: rswm_knn6_weep \n\nGeary C statistic standard deviate = 2.8704, p-value = 0.00205\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n     0.9167373752      1.0000000000      0.0008414086 \n\n\nFor Weekday Morning Peak Hour, Weekends/Holiday Morning Peak and Weekends/Holiday Evening Peak, the p-value &lt; 0.05, there is sufficient statistical evidence to reject the null hypothesis at the 95% level of confidence. This means that data is more spatially clustered than expected by chance alone. The Geary C statistics are less than 1 suggesting that clusters are present . This finding is consistent with the results of the Global Moran’s I test in the previous section.\nHowever, for Weekday Afternoon Peak Hour, the p-value &gt; 0.05, suggesting that there is no sufficient statistical evidence to reject the null hypothesis at the 95% level of confidence. This mean that the data is randomly disbursed with no spatial pattern which is not consistent with results of the Global Moran’s I test in the previous section.\nComputing Monte Carlo Geary’s C\nSimilar to Moran’s I, Monte Carlo simulation is used to perform a permutation test for Geary’s C. The code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep pacakagepackeage with a total of 1000 simulation.\n\n\nShow the code\nset.seed(1234)\nbperm_func = geary.mc(total_trips_per_grid_wdmp$total_trips,\n                 listw = rswm_knn6,\n                 nsim = 999)\nbperm_func\n\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  total_trips_per_grid_wdmp$total_trips \nweights: rswm_knn6 \nnumber of simulations + 1: 1000 \n\nstatistic = 0.81766, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\nShow the code\nset.seed(1234)\nbperm_func_wdap = geary.mc(total_trips_per_grid_wdap$total_trips,\n                 listw = rswm_knn6_wdap,\n                 nsim = 999)\nbperm_func_wdap\n\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  total_trips_per_grid_wdap$total_trips \nweights: rswm_knn6_wdap \nnumber of simulations + 1: 1000 \n\nstatistic = 0.97225, observed rank = 184, p-value = 0.184\nalternative hypothesis: greater\n\n\n\n\nShow the code\nbperm_func_wemp = geary.mc(total_trips_per_grid_wemp$total_trips,\n                 listw = rswm_knn6_wemp,\n                 nsim = 999)\nbperm_func_wemp\n\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  total_trips_per_grid_wemp$total_trips \nweights: rswm_knn6_wemp \nnumber of simulations + 1: 1000 \n\nstatistic = 0.86465, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\nShow the code\nbperm_func_weep = geary.mc(total_trips_per_grid_weep$total_trips,\n                 listw = rswm_knn6_weep,\n                 nsim = 999)\nbperm_func_weep\n\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  total_trips_per_grid_weep$total_trips \nweights: rswm_knn6_weep \nnumber of simulations + 1: 1000 \n\nstatistic = 0.91674, observed rank = 7, p-value = 0.007\nalternative hypothesis: greater\n\n\nFor Weekday Morning Peak Hour, Weekends/Holiday Morning Peak and Weekends/Holiday Evening Peak, the p-value is less than 0.05, suggesting that there is sufficient statistical evidence to reject the null hypothesis at the 95% level of confidence. This means that data is more spatially clustered than expected by chance alone.\nHowever, for Weekday Afternoon Peak Hour, the p-value &gt; 0.05, suggesting that there is no sufficient statistical evidence to reject the null hypothesis at the 95% level of confidence, suggesting that the data is randomly disbursed with no spatial pattern."
  },
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#local-spatial-autocorrelation-statistics",
    "href": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#local-spatial-autocorrelation-statistics",
    "title": "Take-home_Ex1: Public Bus Passengers in Singapore",
    "section": "11 Local Spatial Autocorrelation Statistics",
    "text": "11 Local Spatial Autocorrelation Statistics\nFrom the results above, I have established the statistical testing that spatial clustering occurs for Weekday Morning Peak Hour, Weekends/Holiday Morning Peak and Weekends/Holiday Evening but not for Weekday Afternoon Peak Hour in Singapore.\nIn this section, I will detect clusters/outliers and discover hot or cold spots using Local Spatial Autocorrelation Statistics with Local Indicators of Spatial Association (LISA).\n\n11.1 Local Indicators of Spatial Association (LISA) Analysis\nLocal Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable.\nIn this section, I will apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran’I to detect cluster and/or outlier from passenger trips generate by origin at hexagon level.\nPositive Local Moran’s I value indicates that a feature has neighboring features with similarly high or low attribute values which refer to a cluster. While Negative Local Moran’s I value indicates that a feature has neighboring features with dissimilar values which refer to an outlier.\nComputing local Moran’s I\nI will use localmoran() function of spdep package to compute local Moran’s I by first computing Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\nThe code chunks below are used to compute local Moran’s I of Total Trips at the hexagonal grid level.\n\n\nShow the code\nfips &lt;- order(total_trips_per_grid_wdmp$grid_id)\nlocalMI &lt;- localmoran(total_trips_per_grid_wdmp$total_trips, rswm_knn6)\nhead(localMI)\n\n\n         Ii          E.Ii     Var.Ii     Z.Ii Pr(z != E(Ii))\n1 0.3922014 -0.0002685351 0.06653377 1.521547      0.1281227\n2 0.3931675 -0.0002701326 0.06692947 1.520783      0.1283143\n3 0.3861718 -0.0002693176 0.06672760 1.495997      0.1346545\n4 0.3915081 -0.0002659764 0.06590000 1.526135      0.1269762\n5 0.3727281 -0.0002377856 0.05891693 1.536558      0.1244015\n6 0.3910946 -0.0002701012 0.06692170 1.512858      0.1303158\n\n\n\n\nShow the code\nfips_wdap &lt;- order(total_trips_per_grid_wdap$grid_id)\nlocalMI_wdap &lt;- localmoran(total_trips_per_grid_wdap$total_trips, rswm_knn6_wdap)\nhead(localMI_wdap)\n\n\n         Ii          E.Ii     Var.Ii     Z.Ii Pr(z != E(Ii))\n1 0.2052174 -0.0001410007 0.03496308 1.098266      0.2720883\n2 0.2081184 -0.0001458668 0.03616954 1.095073      0.2734845\n3 0.1964740 -0.0001427366 0.03539348 1.045103      0.2959755\n4 0.1891799 -0.0001157598 0.02870499 1.117280      0.2638748\n5 0.1745250 -0.0001005505 0.02493391 1.105891      0.2687735\n6 0.2010256 -0.0001436174 0.03561185 1.066017      0.2864158\n\n\n\n\nShow the code\nfips_wemp &lt;- order(total_trips_per_grid_wemp$grid_id)\nlocalMI_wemp &lt;- localmoran(total_trips_per_grid_wemp$total_trips, rswm_knn6_wemp)\nhead(localMI_wemp)\n\n\n         Ii          E.Ii     Var.Ii     Z.Ii Pr(z != E(Ii))\n1 0.3117395 -0.0002177150 0.05412644 1.340881      0.1799590\n2 0.2986896 -0.0002154877 0.05357282 1.291401      0.1965648\n3 0.3035088 -0.0002041068 0.05074398 1.348252      0.1775774\n4 0.2909179 -0.0001824286 0.04535543 1.366873      0.1716652\n5 0.3007774 -0.0002134419 0.05306431 1.306628      0.1913390\n6 0.2969174 -0.0002160862 0.05372159 1.281968      0.1998540\n\n\n\n\nShow the code\nfips_weep &lt;- order(total_trips_per_grid_weep$grid_id)\nlocalMI_weep &lt;- localmoran(total_trips_per_grid_weep$total_trips, rswm_knn6_weep)\nhead(localMI_weep)\n\n\n         Ii          E.Ii     Var.Ii     Z.Ii Pr(z != E(Ii))\n1 0.2257033 -0.0001581269 0.03913004 1.141792      0.2535405\n2 0.2272433 -0.0001607542 0.03978008 1.140159      0.2542201\n3 0.2106050 -0.0001553977 0.03845477 1.074766      0.2824796\n4 0.2156888 -0.0001405767 0.03478769 1.157172      0.2472022\n5 0.2009096 -0.0001241690 0.03072786 1.146840      0.2514479\n6 0.2147767 -0.0001581892 0.03914545 1.086341      0.2773281\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\nThe code chunk below list the content of the local Moran matrix derived by using printCoefmat(). Since the matrix is very long, I only show the head().\n\n\nShow the code\nprintCoefmat(head(data.frame(\n  localMI[fips,], \n  row.names=total_trips_per_grid_wdmp$grid_id[fips]),\n  check.names=FALSE))\n\n\n             Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\n34   0.39220141 -0.00026854  0.06653377  1.52154689         0.1281\n65   0.39316750 -0.00027013  0.06692947  1.52078282         0.1283\n99   0.38617181 -0.00026932  0.06672760  1.49599657         0.1347\n127  0.39150811 -0.00026598  0.06590000  1.52613519         0.1270\n129  0.37272807 -0.00023779  0.05891693  1.53655827         0.1244\n130  0.39109457 -0.00027010  0.06692170  1.51285786         0.1303\n\n\nMapping Local Moran’s I values and p-values\nNext, I will append the local Moran’s I dataframe onto nigeria SpatialPolygonDataFrame in preparation for the next part. This can be done using the code chunks below.\n\n\nShow the code\ntotal_trips_per_grid_wdmp.localMI &lt;- cbind(total_trips_per_grid_wdmp,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\n\n\nShow the code\ntotal_trips_per_grid_wdap.localMI_wdap &lt;- cbind(total_trips_per_grid_wdap,localMI_wdap) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\n\n\nShow the code\ntotal_trips_per_grid_wemp.localMI_wemp &lt;- cbind(total_trips_per_grid_wemp,localMI_wemp) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\n\n\nShow the code\ntotal_trips_per_grid_weep.localMI_weep &lt;- cbind(total_trips_per_grid_weep,localMI_weep) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\nLet’s visualise both the Local Moran’s I values and its p-values using the choropleth mapping functions of tmap package.\n\n\nShow the code\nlocalMI.map &lt;- tm_shape(total_trips_per_grid_wdmp.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(total_trips_per_grid_wdmp.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)\n\n\n\n\n\n\n\nShow the code\nlocalMI_wdap.map &lt;- tm_shape(total_trips_per_grid_wdap.localMI_wdap) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue_wdap.map &lt;- tm_shape(total_trips_per_grid_wdap.localMI_wdap) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI_wdap.map, pvalue_wdap.map, asp=1, ncol=2)\n\n\n\n\n\n\n\nShow the code\nlocalMI_wemp.map &lt;- tm_shape(total_trips_per_grid_wemp.localMI_wemp) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue_wemp.map &lt;- tm_shape(total_trips_per_grid_wemp.localMI_wemp) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI_wemp.map, pvalue_wemp.map, asp=1, ncol=2)\n\n\n\n\n\n\n\nShow the code\nlocalMI_weep.map &lt;- tm_shape(total_trips_per_grid_weep.localMI_weep) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue_weep.map &lt;- tm_shape(total_trips_per_grid_weep.localMI_weep) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI_weep.map, pvalue_weep.map, asp=1, ncol=2)\n\n\n\n\n\nI will further build a choropleth and shade only grids that are statistically significant using the chunk code below. Firstly, I create ane object that consist statistically significant values using filter(). Then, I plot the base map consisting of the polygons features and lastly I overlay the base map with the statically significant Moran I’ value map. The processes are repeated for the other peah hour group.\n\n\nShow the code\n#Wdmp\ntotal_trips_per_grid_wdmp.localMI_sig_wdmp &lt;- cbind(total_trips_per_grid_wdmp,\n                                                 localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..) %&gt;%\n  filter(Pr.Ii &lt; 0.05)\n\nbase_wdmp &lt;- tm_shape(total_trips_per_grid_wdmp) + \n  tm_fill(col = 'gray98') + \n  tm_borders(alpha = 0.3)\n\nlocalMI_sig_wdmp.map &lt;- base_wdmp + \n  tm_shape(total_trips_per_grid_wdmp.localMI_sig_wdmp) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          title = \"Local Moran I Statistics\") +\n  tm_borders(alpha = 0.3) + \n  tm_layout(main.title = \"Local Moran's I (Sig.) Map \\n(Weekday Morning Peak) \",\n            main.title.size = 1,\n            main.title.position = \"center\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE)\n\n#wdap\ntotal_trips_per_grid_wdap.localMI_sig_wdap &lt;- cbind(total_trips_per_grid_wdap,localMI_wdap) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..) %&gt;%\n  filter(Pr.Ii &lt; 0.05)\n\nbase_wdap &lt;- tm_shape(total_trips_per_grid_wdap) + \n  tm_fill(col = 'gray98') + \n  tm_borders(alpha = 0.3)\n\nlocalMI_sig_wdap.map &lt;- base_wdap + \n  tm_shape(total_trips_per_grid_wdap.localMI_sig_wdap) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          title = \"Local Moran I Statistics\") +\n  tm_borders(alpha = 0.3) + \n  tm_layout(main.title = \"Local Moran's I (Sig.) Map \\n(Weekday Afternoon Peak) \",\n            main.title.size = 1,\n            main.title.position = \"center\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE)\n\n#wemp\ntotal_trips_per_grid_wemp.localMI_sig_wemp &lt;- cbind(total_trips_per_grid_wemp,localMI_wemp) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..) %&gt;%\n  filter(Pr.Ii &lt; 0.05)\n\nbase_wemp &lt;- tm_shape(total_trips_per_grid_wemp) + \n  tm_fill(col = 'gray98') + \n  tm_borders(alpha = 0.3)\n\nlocalMI_sig_wemp.map &lt;- base_wemp + \n  tm_shape(total_trips_per_grid_wemp.localMI_sig_wemp) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          title = \"Local Moran I Statistics\") +\n  tm_borders(alpha = 0.3) + \n  tm_layout(main.title = \"Local Moran's I (Sig.) Map \\n(Weekends/Holiday Morning Peak) \",\n            main.title.size = 1,\n            main.title.position = \"center\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE)\n\n#weep\ntotal_trips_per_grid_weep.localMI_sig_weep &lt;- cbind(total_trips_per_grid_weep,localMI_weep) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..) %&gt;%\n  filter(Pr.Ii &lt; 0.05)\n\nbase_weep &lt;- tm_shape(total_trips_per_grid_weep) + \n  tm_fill(col = 'gray98') + \n  tm_borders(alpha = 0.3)\n\nlocalMI_sig_weep.map &lt;- base_weep + \n  tm_shape(total_trips_per_grid_weep.localMI_sig_weep) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          title = \"Local Moran I Statistics\") +\n  tm_borders(alpha = 0.3) + \n  tm_layout(main.title = \"Local Moran's I (Sig.) Map \\n(Weekends/Holiday Afternoon Peak) \",\n            main.title.size = 1,\n            main.title.position = \"center\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE)\n\ntmap_arrange(localMI_sig_wdmp.map,localMI_sig_wdap.map, localMI_sig_wemp.map,localMI_sig_weep.map)\n\n\n\n\n\nA positive Local Moran’s I value indicates that a feature is part of a cluster while a negative Local Moran’s I value indicates that a feature is an outlier. From the plot above, it it notable that north of Singapore which is around Woodlands are has 1 cluster with green shade for weekdays. While for weekends/holiday, there are more yellow referring to outliers except for the south part of Singapore, there is a noticeable cluster with green shade.\n\n\n11.2 Plotting LISA Cluster Map\nThe local Moran’s I value alone is not enough to show spatial clustering because it does not tell us whether the value of the variable being tested is high or low and whether the test result was significant. Therefore, I will assign each observation to a quadrant depending on the value of the variable on the y-axis (centred around the mean) and Moran’s I on the x-axis. Quadrant 1 contains coldspots and quadrant contains hotspot as follows:\n\n\n\nLISA_Quadrant\n\n\nLISA Cluster Maps also categorises each region into one of five groups: (1) High-High, (2) High-Low, (3) Low-High, (4) Low-Low and (5) Insignificant.\nThe following steps are conducted to prepare for LISA cluster map:\n\nCreate a vector of the same length as the number of hexagonal grid in each peak hour datasets.\nDerive a variable, DV by using a by using the spatially lagged version of total_trips and center it around its means. When DV &gt; 0, the spatially lagged variable of the region is higher than the mean.\nDerive a variable, L_MI using the Local Moran’s I.\nSet the significance level for the local Moran.\nDefine the command lines for: high-high, low-low, low-high, high-low\nPlace statistically insignificant Moran I in the category 0.\n\n\n\nShow the code\n#Step 1\nquadrant &lt;- vector(mode = 'numeric', length = nrow(localMI))\n#Step 2\ntotal_trips_per_grid_wdmp$lag_total_trips &lt;- lag.listw(rswm_knn6, total_trips_per_grid_wdmp$total_trips)\nDV &lt;- total_trips_per_grid_wdmp$lag_total_trips - mean(total_trips_per_grid_wdmp$lag_total_trips)     \n#Step 3\nLM_I &lt;- localMI[,1] \n#Step 4\nsignif &lt;- 0.05\n#Step 5\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1 #low-low\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2 #high-low\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3 #low-high\nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4 #high-high\n#Step 6\nquadrant[localMI[,5]&gt;signif] &lt;- 0 \n\n\nThe code chunk below is to plot the LISA cluster map for Weekday Morning Peak Hour.\n\n\nShow the code\n#Assign each region  to its respective quadrant\ntotal_trips_per_grid_wdmp.localMI$quadrant &lt;- quadrant\n#Set the colours--one for each quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\") \n\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap_wdmp &lt;- tm_shape(total_trips_per_grid_wdmp.localMI) + \n  tm_fill(col = \"quadrant\",\n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1],\n          labels = clusters[c(sort(unique(quadrant)))+1])  + \n  tm_borders(alpha = 0.3) + \n  tm_layout(main.title = \"LISA Cluster Map \\n Weekday Morning Peak Hour \",\n            main.title.size = 1,\n            main.title.position = \"center\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE)\n\n\nThe above processes are implemented for the other peak hours group.\n\n\nShow the code\n#Step 1\nquadrant_wdap &lt;- vector(mode = 'numeric', length = nrow(localMI_wdap))\n#Step 2\ntotal_trips_per_grid_wdap$lag_total_trips &lt;- lag.listw(rswm_knn6_wdap, total_trips_per_grid_wdap$total_trips)\nDV_wdap &lt;- total_trips_per_grid_wdap$lag_total_trips - mean(total_trips_per_grid_wdap$lag_total_trips)     \n#Step 3\nLM_I_wdap &lt;- localMI_wdap[,1] \n#Step 4\nsignif &lt;- 0.05\n#Step 5\nquadrant_wdap[DV_wdap &lt;0 & LM_I_wdap&gt;0] &lt;- 1 #low-low\nquadrant_wdap[DV_wdap &gt;0 & LM_I_wdap&lt;0] &lt;- 2 #high-low\nquadrant_wdap[DV_wdap &lt;0 & LM_I_wdap&lt;0] &lt;- 3 #low-high\nquadrant_wdap[DV_wdap &gt;0 & LM_I_wdap&gt;0] &lt;- 4 #high-high\n#Step 6\nquadrant_wdap[localMI_wdap[,5]&gt;signif] &lt;- 0 \n\n#Assign each region  to its respective quadrant\ntotal_trips_per_grid_wdap.localMI_wdap$quadrant_wdap &lt;- quadrant_wdap\n#Set the colours--one for each quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\") \n\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap_wdap &lt;- tm_shape(total_trips_per_grid_wdap.localMI_wdap) + \n  tm_fill(col = \"quadrant_wdap\",\n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant_wdap)))+1],\n          labels = clusters[c(sort(unique(quadrant_wdap)))+1])  + \n  tm_borders(alpha = 0.3) + \n  tm_layout(main.title = \"LISA Cluster Map \\n Weekday Afternoon Peak Hour \",\n            main.title.size = 1,\n            main.title.position = \"center\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE)\n\n\n\n\nShow the code\n#Step 1\nquadrant_wemp &lt;- vector(mode = 'numeric', length = nrow(localMI_wemp))\n#Step 2\ntotal_trips_per_grid_wemp$lag_total_trips &lt;- lag.listw(rswm_knn6_wemp, total_trips_per_grid_wemp$total_trips)\nDV_wemp &lt;- total_trips_per_grid_wemp$lag_total_trips - mean(total_trips_per_grid_wemp$lag_total_trips)     \n#Step 3\nLM_I_wemp &lt;- localMI_wemp[,1] \n#Step 4\nsignif &lt;- 0.05\n#Step 5\nquadrant_wemp[DV_wemp &lt;0 & LM_I_wemp&gt;0] &lt;- 1 #low-low\nquadrant_wemp[DV_wemp &gt;0 & LM_I_wemp&lt;0] &lt;- 2 #high-low\nquadrant_wemp[DV_wemp &lt;0 & LM_I_wemp&lt;0] &lt;- 3 #low-high\nquadrant_wemp[DV_wemp &gt;0 & LM_I_wemp&gt;0] &lt;- 4 #high-high\n#Step 6\nquadrant_wemp[localMI_wemp[,5]&gt;signif] &lt;- 0 \n\n#Assign each region  to its respective quadrant\ntotal_trips_per_grid_wemp.localMI_wemp$quadrant_wemp &lt;- quadrant_wemp\n#Set the colours--one for each quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\") \n\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap_wemp &lt;- tm_shape(total_trips_per_grid_wemp.localMI_wemp) + \n  tm_fill(col = \"quadrant_wemp\",\n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant_wemp)))+1],\n          labels = clusters[c(sort(unique(quadrant_wemp)))+1])  + \n  tm_borders(alpha = 0.3) + \n  tm_layout(main.title = \"LISA Cluster Map \\n Weekends/Holiday Morning Peak Hour \",\n            main.title.size = 1,\n            main.title.position = \"center\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE)\n\n\n\n\nShow the code\n#Step 1\nquadrant_weep &lt;- vector(mode = 'numeric', length = nrow(localMI_weep))\n#Step 2\ntotal_trips_per_grid_weep$lag_total_trips &lt;- lag.listw(rswm_knn6_weep, total_trips_per_grid_weep$total_trips)\nDV_weep &lt;- total_trips_per_grid_weep$lag_total_trips - mean(total_trips_per_grid_weep$lag_total_trips)     \n#Step 3\nLM_I_weep &lt;- localMI_weep[,1] \n#Step 4\nsignif &lt;- 0.05\n#Step 5\nquadrant_weep[DV_weep &lt;0 & LM_I_weep&gt;0] &lt;- 1 #low-low\nquadrant_weep[DV_weep &gt;0 & LM_I_weep&lt;0] &lt;- 2 #high-low\nquadrant_weep[DV_weep &lt;0 & LM_I_weep&lt;0] &lt;- 3 #low-high\nquadrant_weep[DV_weep &gt;0 & LM_I_weep&gt;0] &lt;- 4 #high-high\n#Step 6\nquadrant_weep[localMI_weep[,5]&gt;signif] &lt;- 0 \n\n#Assign each region  to its respective quadrant\ntotal_trips_per_grid_weep.localMI_weep$quadrant_weep &lt;- quadrant_weep\n#Set the colours--one for each quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\") \n\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap_weep &lt;- tm_shape(total_trips_per_grid_weep.localMI_weep) + \n  tm_fill(col = \"quadrant_weep\",\n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant_weep)))+1],\n          labels = clusters[c(sort(unique(quadrant_weep)))+1])  + \n  tm_borders(alpha = 0.3) + \n  tm_layout(main.title = \"LISA Cluster Map \\n Weekends/Holiday Evening Peak Hour \",\n            main.title.size = 1,\n            main.title.position = \"center\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE)\n\n\nNext, I will plot the local Moran’s I map (statistically significant values only) and LISA map together. The shaded hexagonal grids will be the same for each pair of maps.\n\n\nShow the code\ntmap_arrange(localMI_sig_wdmp.map, LISAmap_wdmp,\n  localMI_sig_wdap.map, LISAmap_wdap,\n  ncol =2)\n\n\n\n\n\n\n\nShow the code\ntmap_arrange(localMI_sig_wemp.map, LISAmap_wemp,\n  localMI_sig_weep.map, LISAmap_weep,\n  ncol =2)\n\n\n\n\n\nThe LISA maps provides extra information whether the hexagonal grids have relatively higher or lower passenger trips generate by origin. Overall, Local Moran’s have revealed significant spatial cluster and outliers.\nFrom the results above, it is noticed that there are only 3 type of quadrants which are insignificant, low-high and high-high. For ease of interpreting the result, below are the maps of Singapore regions.\n\n\n\nSingapore Regions\n\n\nWeekday Morning Peak Hour:\nFrom the Local Moran’s I map, there are some hexagonal grids with positive Local Moran’s I values and they have neigbours with another high or low passenger trip grids. This consistent with the LISA maps. The high-high clusters spread across Singapore while the low-high outliers are typically near the high-high clusters. The hexagonal grid near the Singapore Malaysia border (Johor and edges near the border) are high-high cluster indicating a high level of cross country activities during the weekday morning.\nWeekday Afternoon Peak Hour:\nThe cluster with high-high characteristics are distributed throughout Singapore with exception of West Singapore where only low-high outliers are apparent. This is attributed to the presence of Boon Lay Bus Interchange as the sole busy interchange in the western region, leading to comparatively lower passenger trips in the other parts of West Singapore.\nWeekends/Holiday Morning Peak Hour:\nThe cluster with high-high characteristics are distributed throughout Singapore especially in the CBD/Central area where a significant concentration of passenger trips are observed during these times. This elevated activities may be influenced by various factors such as recreational activities, family/friend gathering, etc., contributing to the high level of engagement and movement in the CBD area.\nWeekends/Holiday Evening Peak Hour:\nThe distribution of high-high cluster and low-high outliers are quite similar with Weekends/Holiday Morning Peak Hour except that the high-high cluster are wider around the CBD area during evening and there is no high-high cluster around the Yishun area as compared to weekends/holiday morning peak hour."
  },
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#conclusion",
    "href": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#conclusion",
    "title": "Take-home_Ex1: Public Bus Passengers in Singapore",
    "section": "12 Conclusion",
    "text": "12 Conclusion\nIn this take-home exercise, ESDA was used to generate insights about the distribution of passenger trips in Singapore. Using the Global Spatial Autocorrelation Statistics, it is noted that there is existence of spatial autocorrelation for all peak hour groups as whole analysis using Moran’s I statistical test. However, when employing Geary’s C, which accounts for differences with immediate neighbors, the data on weekday afternoon peak hour passenger trips is determined to be randomly dispersed, showing no discernible spatial pattern.\nThe existence of spatial autocorrelation led me to use local spatial autocorrelation statistics like LISA cluster maps to identify areas of clusters and outliers. Understanding the uneven distribution of passenger trips for different peak hours will be crucial for optimizing public transportation infrastrcuture, managing congestion and enhancing overall urban mobility.\nIn conclusion, the insights gained from both global and local spatial autocorrelation analyses contribute to a comprehensive understanding of passenger trip patterns for targeted interventions and strategic planning which in turn help to improve the efficiency and sustainability of transportation systems in Singapore. To enhance the depth of transportation dynamics analysis, future work could explore destination-specific or individual bus route considerations."
  },
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#references",
    "href": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#references",
    "title": "Take-home_Ex1: Public Bus Passengers in Singapore",
    "section": "13 References",
    "text": "13 References\n[1] https://urbandatapalette.com/post/2021-08-tessellation-sf/"
  },
  {
    "objectID": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#creating-honeycomb_grid1",
    "href": "Take-home_Exercise/Take-home_Ex1/Take-home_Ex1.html#creating-honeycomb_grid1",
    "title": "Take-home_Ex1: Public Bus Passengers in Singapore",
    "section": "6 Creating honeycomb_grid1",
    "text": "6 Creating honeycomb_grid1\nHoneycomb grid are preferred to replace coarse and irregular Master Plan 2019 Sub-zone GIS data set of URA because hexagon reduce sampling bias due to its grid shape of low perimeter to are ratio and its ability to form evenly spaced grid. Honeycomb grids are well-suited for approximating circular areas, making them suitable for mapping Singapore edges with is irregular shape.\nThe code chunk below uses st_make_grid of sf package to create a hexagonal or honeycomb grid with a 250m (perpendicular distance between the center of hexagon and its edges). According the the R documentation, the cellsize is the distance between opposite edges, which is 2 times the perpendicular distance between the center of hexagon and its edges. Thus, for the purpose of this exercise, I will use the cellsize of 500m and indicate the square=FALSE for hexagonal grid. After doing do, I will create a grid_id for each hexagonal grid.\n\n\nShow the code\narea_honeycomb_grid = st_make_grid(busstop3414, c(500, 500), what = \"polygons\", square = FALSE)    \n# To sf and add grid ID  \nhoneycomb_grid_sf = st_sf(area_honeycomb_grid) %&gt;%    \n  # add grid ID      \n  mutate(grid_id = 1:length(lengths(area_honeycomb_grid)))"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex4/Hands-on_Ex4.html",
    "href": "Hands-on_Exercise/Hands-on_Ex4/Hands-on_Ex4.html",
    "title": "Hands-on_Ex4:Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, you will learn how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex4/Hands-on_Ex4.html#overview",
    "href": "Hands-on_Exercise/Hands-on_Ex4/Hands-on_Ex4.html#overview",
    "title": "Hands-on_Ex4:Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, you will learn how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex4/Hands-on_Ex4.html#the-data",
    "href": "Hands-on_Exercise/Hands-on_Ex4/Hands-on_Ex4.html#the-data",
    "title": "Hands-on_Ex4:Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "2 The Data",
    "text": "2 The Data\nTwo data sets will be used in this model building exercise, they are:\n\nURA Master Plan subzone boundary in shapefile format (i.e. MP14_SUBZONE_WEB_PL)\ncondo_resale_2015 in csv format (i.e. condo_resale_2015.csv)"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex4/Hands-on_Ex4.html#getting-started",
    "href": "Hands-on_Exercise/Hands-on_Ex4/Hands-on_Ex4.html#getting-started",
    "title": "Hands-on_Ex4:Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "3 Getting Started",
    "text": "3 Getting Started\nBefore we get started, it is important for us to install the necessary R packages into R and launch these R packages into R environment.\nThe R packages needed for this exercise are as follows:\n\nR package for building OLS and performing diagnostics tests\n\nolsrr\n\nR package for calibrating geographical weighted family of models\n\nGWmodel\n\nR package for multivariate data visualisation and analysis\n\ncorrplot\n\nSpatial data handling\n\nsf\n\nAttribute data handling\n\ntidyverse, especially readr, ggplot2 and dplyr\n\nChoropleth mapping\n\ntmap\n\n\nThe code chunks below installs and launches these R packages into R environment.\n\npacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary)\n\npackage 'GWmodel' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\tikaw\\AppData\\Local\\Temp\\Rtmp46qrqW\\downloaded_packages"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex4/Hands-on_Ex4.html#a-short-note-about-gwmodel",
    "href": "Hands-on_Exercise/Hands-on_Ex4/Hands-on_Ex4.html#a-short-note-about-gwmodel",
    "title": "Hands-on_Ex4:Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "4 A short note about GWmodel",
    "text": "4 A short note about GWmodel\nGWmodel package provides a collection of localised spatial statistical methods, namely: GW summary statistics, GW principal components analysis, GW discriminant analysis and various forms of GW regression; some of which are provided in basic and robust (outlier resistant) forms. Commonly, outputs or parameters of the GWmodel are mapped to provide a useful exploratory tool, which can often precede (and direct) a more traditional or sophisticated statistical analysis."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex4/Hands-on_Ex4.html#geospatial-data-wrangling",
    "href": "Hands-on_Exercise/Hands-on_Ex4/Hands-on_Ex4.html#geospatial-data-wrangling",
    "title": "Hands-on_Ex4:Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "5 Geospatial Data Wrangling",
    "text": "5 Geospatial Data Wrangling\n\n5.1 Importing geospatial data\nThe geospatial data used in this hands-on exercise is called MP14_SUBZONE_WEB_PL. It is in ESRI shapefile format. The shapefile consists of URA Master Plan 2014's planning subzone boundaries. Polygon features are used to represent these geographic boundaries. The GIS data is in svy21 projected coordinates systems.\nThe code chunk below is used to import MP_SUBZONE_WEB_PL shapefile by using st_read() of sf packages.\n\nmpsz = st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `W:\\widyayutika\\ISSS624\\Hands-on_Exercise\\Hands-on_Ex4\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThe report above shows that the R object used to contain the imported MP14_SUBZONE_WEB_PL shapefile is called mpsz and it is a simple feature object. The geometry type is multipolygon. it is also important to note that mpsz simple feature object does not have EPSG information.\n\n\n5.2 Updating CRS information\nThe code chunk below updates the newly imported mpsz with the correct ESPG code (i.e. 3414)\n\nmpsz_svy21 &lt;- st_transform(mpsz, 3414)\n\nAfter transforming the projection metadata, you can varify the projection of the newly transformed mpsz_svy21 by using st_crs() of sf package.\nThe code chunk below will be used to varify the newly transformed mpsz_svy21.\n\nst_crs(mpsz_svy21)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNotice that the EPSG: is indicated as 3414 now.\nNext, you will reveal the extent of mpsz_svy21 by using st_bbox() of sf package.\n\nst_bbox(mpsz_svy21) #view extent\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex4/Hands-on_Ex4.html#aspatial-data-wrangling",
    "href": "Hands-on_Exercise/Hands-on_Ex4/Hands-on_Ex4.html#aspatial-data-wrangling",
    "title": "Hands-on_Ex4:Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "6 Aspatial Data Wrangling",
    "text": "6 Aspatial Data Wrangling\n\n6.1 Importing the aspatial data\nThe condo_resale_2015 is in csv file format. The codes chunk below uses read_csv() function of readr package to import condo_resale_2015 into R as a tibble data frame called condo_resale.\n\ncondo_resale = read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe codes chunks below uses glimpse() to display the data structure of will do the job.\n\nglimpse(condo_resale)\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             &lt;dbl&gt; 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,…\n$ LONGITUDE            &lt;dbl&gt; 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,…\n$ POSTCODE             &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472, 3…\n$ SELLING_PRICE        &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1320…\n$ AREA_SQM             &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,…\n$ AGE                  &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,…\n$ PROX_CBD             &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783402…\n$ PROX_CHILDCARE       &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0…\n$ PROX_ELDERLYCARE     &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121…\n$ PROX_URA_GROWTH_AREA &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,…\n$ PROX_HAWKER_MARKET   &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0…\n$ PROX_KINDERGARTEN    &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0…\n$ PROX_MRT             &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528…\n$ PROX_PARK            &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116…\n$ PROX_PRIMARY_SCH     &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709…\n$ PROX_TOP_PRIMARY_SCH &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709…\n$ PROX_SHOPPING_MALL   &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307…\n$ PROX_SUPERMARKET     &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581…\n$ PROX_BUS_STOP        &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0…\n$ NO_Of_UNITS          &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3…\n$ FAMILY_FRIENDLY      &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\n\nhead(condo_resale$LONGITUDE) #see the data in XCOORD column\n\n[1] 103.7802 103.8123 103.7971 103.8247 103.9505 103.9386\n\n\n\nhead(condo_resale$LATITUDE) #see the data in YCOORD column\n\n[1] 1.287145 1.328698 1.313727 1.308563 1.321437 1.314198\n\n\nNext, summary() of base R is used to display the summary statistics of cond_resale tibble data frame.\n\nsummary(condo_resale)\n\n    LATITUDE       LONGITUDE        POSTCODE      SELLING_PRICE     \n Min.   :1.240   Min.   :103.7   Min.   : 18965   Min.   :  540000  \n 1st Qu.:1.309   1st Qu.:103.8   1st Qu.:259849   1st Qu.: 1100000  \n Median :1.328   Median :103.8   Median :469298   Median : 1383222  \n Mean   :1.334   Mean   :103.8   Mean   :440439   Mean   : 1751211  \n 3rd Qu.:1.357   3rd Qu.:103.9   3rd Qu.:589486   3rd Qu.: 1950000  \n Max.   :1.454   Max.   :104.0   Max.   :828833   Max.   :18000000  \n    AREA_SQM          AGE           PROX_CBD       PROX_CHILDCARE    \n Min.   : 34.0   Min.   : 0.00   Min.   : 0.3869   Min.   :0.004927  \n 1st Qu.:103.0   1st Qu.: 5.00   1st Qu.: 5.5574   1st Qu.:0.174481  \n Median :121.0   Median :11.00   Median : 9.3567   Median :0.258135  \n Mean   :136.5   Mean   :12.14   Mean   : 9.3254   Mean   :0.326313  \n 3rd Qu.:156.0   3rd Qu.:18.00   3rd Qu.:12.6661   3rd Qu.:0.368293  \n Max.   :619.0   Max.   :37.00   Max.   :19.1804   Max.   :3.465726  \n PROX_ELDERLYCARE  PROX_URA_GROWTH_AREA PROX_HAWKER_MARKET PROX_KINDERGARTEN \n Min.   :0.05451   Min.   :0.2145       Min.   :0.05182    Min.   :0.004927  \n 1st Qu.:0.61254   1st Qu.:3.1643       1st Qu.:0.55245    1st Qu.:0.276345  \n Median :0.94179   Median :4.6186       Median :0.90842    Median :0.413385  \n Mean   :1.05351   Mean   :4.5981       Mean   :1.27987    Mean   :0.458903  \n 3rd Qu.:1.35122   3rd Qu.:5.7550       3rd Qu.:1.68578    3rd Qu.:0.578474  \n Max.   :3.94916   Max.   :9.1554       Max.   :5.37435    Max.   :2.229045  \n    PROX_MRT         PROX_PARK       PROX_PRIMARY_SCH  PROX_TOP_PRIMARY_SCH\n Min.   :0.05278   Min.   :0.02906   Min.   :0.07711   Min.   :0.07711     \n 1st Qu.:0.34646   1st Qu.:0.26211   1st Qu.:0.44024   1st Qu.:1.34451     \n Median :0.57430   Median :0.39926   Median :0.63505   Median :1.88213     \n Mean   :0.67316   Mean   :0.49802   Mean   :0.75471   Mean   :2.27347     \n 3rd Qu.:0.84844   3rd Qu.:0.65592   3rd Qu.:0.95104   3rd Qu.:2.90954     \n Max.   :3.48037   Max.   :2.16105   Max.   :3.92899   Max.   :6.74819     \n PROX_SHOPPING_MALL PROX_SUPERMARKET PROX_BUS_STOP       NO_Of_UNITS    \n Min.   :0.0000     Min.   :0.0000   Min.   :0.001595   Min.   :  18.0  \n 1st Qu.:0.5258     1st Qu.:0.3695   1st Qu.:0.098356   1st Qu.: 188.8  \n Median :0.9357     Median :0.5687   Median :0.151710   Median : 360.0  \n Mean   :1.0455     Mean   :0.6141   Mean   :0.193974   Mean   : 409.2  \n 3rd Qu.:1.3994     3rd Qu.:0.7862   3rd Qu.:0.220466   3rd Qu.: 590.0  \n Max.   :3.4774     Max.   :2.2441   Max.   :2.476639   Max.   :1703.0  \n FAMILY_FRIENDLY     FREEHOLD      LEASEHOLD_99YR  \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4868   Mean   :0.4227   Mean   :0.4882  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n\n\n\n\n6.2 Converting aspatial data frame into a sf object\nCurrently, the condo_resale tibble data frame is aspatial. We will convert it to a sf object. The code chunk below converts condo_resale data frame into a simple feature data frame by using st_as_sf() of sf packages.\n\ncondo_resale.sf &lt;- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %&gt;%\n  st_transform(crs=3414)\n\nNotice that st_transform() of sf package is used to convert the coordinates from wgs84 (i.e. crs:4326) to svy21 (i.e. crs=3414).\nNext, head() is used to list the content of condo_resale.sf object.\n\nhead(condo_resale.sf)\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 22\n  POSTCODE SELLING_PRICE AREA_SQM   AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1   118635       3000000      309    30     7.94          0.166            2.52 \n2   288420       3880000      290    32     6.61          0.280            1.93 \n3   267833       3325000      248    33     6.90          0.429            0.502\n4   258380       4250000      127     7     4.04          0.395            1.99 \n5   467169       1400000      145    28    11.8           0.119            1.12 \n6   466472       1320000      139    22    10.3           0.125            0.789\n# ℹ 15 more variables: PROX_URA_GROWTH_AREA &lt;dbl&gt;, PROX_HAWKER_MARKET &lt;dbl&gt;,\n#   PROX_KINDERGARTEN &lt;dbl&gt;, PROX_MRT &lt;dbl&gt;, PROX_PARK &lt;dbl&gt;,\n#   PROX_PRIMARY_SCH &lt;dbl&gt;, PROX_TOP_PRIMARY_SCH &lt;dbl&gt;,\n#   PROX_SHOPPING_MALL &lt;dbl&gt;, PROX_SUPERMARKET &lt;dbl&gt;, PROX_BUS_STOP &lt;dbl&gt;,\n#   NO_Of_UNITS &lt;dbl&gt;, FAMILY_FRIENDLY &lt;dbl&gt;, FREEHOLD &lt;dbl&gt;,\n#   LEASEHOLD_99YR &lt;dbl&gt;, geometry &lt;POINT [m]&gt;\n\n\nNotice that the output is in point feature data frame."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex4/Hands-on_Ex4.html#exploratory-data-analysis-eda",
    "href": "Hands-on_Exercise/Hands-on_Ex4/Hands-on_Ex4.html#exploratory-data-analysis-eda",
    "title": "Hands-on_Ex4:Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "7 Exploratory Data Analysis (EDA)",
    "text": "7 Exploratory Data Analysis (EDA)\nIn the section, you will learn how to use statistical graphics functions of ggplot2 package to perform EDA.\n\n7.1 EDA using statistical graphics\nWe can plot the distribution of SELLING_PRICE by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.\n\nggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\nThe figure above reveals a right skewed distribution. This means that more condominium units were transacted at relative lower prices.\nStatistically, the skewed dsitribution can be normalised by using log transformation. The code chunk below is used to derive a new variable called LOG_SELLING_PRICE by using a log transformation on the variable SELLING_PRICE. It is performed using mutate() of dplyr package.\n\ncondo_resale.sf &lt;- condo_resale.sf %&gt;%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\nNow, you can plot the LOG_SELLING_PRICE using the code chunk below.\n\nggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\nNotice that the distribution is relatively less skewed after the transformation.\n\n\n7.2 Multiple Histogram Plots distribution of variables\nIn this section, you will learn how to draw a small multiple histograms (also known as trellis plot) by using ggarrange() of ggpubr package.\nThe code chunk below is used to create 12 histograms. Then, ggarrange() is used to organised these histogram into a 3 columns by 4 rows small multiple plot.\n\nAREA_SQM &lt;- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nAGE &lt;- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CBD &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CHILDCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_ELDERLYCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_URA_GROWTH_AREA &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_HAWKER_MARKET &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_KINDERGARTEN &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_MRT &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PARK &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_TOP_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)\n\n\n\n\n\n\n7.3 Drawing Statistical Point Map\nLastly, we want to reveal the geospatial distribution condominium resale prices in Singapore. The map will be prepared by using tmap package.\nFirst, we will turn on the interactive mode of tmap by using the code chunk below.\n\ntmap_mode(\"view\")\n\nNext, the code chunks below is used to create an interactive point symbol map.\n\ntmap_options(check.and.fix = TRUE)\ntm_shape(mpsz_svy21)+\n  tm_polygons() +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\nNotice that tm_dots() is used instead of tm_bubbles().\nset.zoom.limits argument of tm_view() sets the minimum and maximum zoom level to 11 and 14 respectively.\nBefore moving on to the next section, the code below will be used to turn R display into plot mode.\n\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex4/Hands-on_Ex4.html#hedonic-pricing-modelling-in-r",
    "href": "Hands-on_Exercise/Hands-on_Ex4/Hands-on_Ex4.html#hedonic-pricing-modelling-in-r",
    "title": "Hands-on_Ex4:Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "8 Hedonic Pricing Modelling in R",
    "text": "8 Hedonic Pricing Modelling in R\nIn this section, you will learn how to building hedonic pricing models for condominium resale units using lm() of R base.\n\n8.1 Simple Linear Regression Method\nFirst, we will build a simple linear regression model by using SELLING_PRICE as the dependent variable and AREA_SQM as the independent variable.\n\ncondo.slr &lt;- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nlm() returns an object of class \"lm\" or for multiple responses of class c(\"mlm\", \"lm\").\nThe functions summary() and anova() can be used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by lm.\n\nsummary(condo.slr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3695815  -391764   -87517   258900 13503875 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -258121.1    63517.2  -4.064 5.09e-05 ***\nAREA_SQM      14719.0      428.1  34.381  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 942700 on 1434 degrees of freedom\nMultiple R-squared:  0.4518,    Adjusted R-squared:  0.4515 \nF-statistic:  1182 on 1 and 1434 DF,  p-value: &lt; 2.2e-16\n\n\nThe output report reveals that the SELLING_PRICE can be explained by using the formula:\n      *y = -258121.1 + 14719x1*\nThe R-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.\nSince p-value is much smaller than 0.0001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.\nThe Coefficients: section of the report reveals that the p-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a results, we will be able to infer that the B0 and B1 are good parameter estimates.\nTo visualise the best fit curve on a scatterplot, we can incorporate lm() as a method function in ggplot's geometry as shown in the code chunk below.\n\nggplot(data=condo_resale.sf,  \n       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n\nFigure above reveals that there are a few statistical outliers with relatively high selling prices.\n\n\n8.2 Multiple Linear Regression Method\n\n8.2.1 Visualising the relationships of the independent variables\nBefore building a multiple regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as multicollinearity in statistics.\nCorrelation matrix is commonly used to visualise the relationships between the independent variables. Beside the pairs() of R, there are many packages support the display of a correlation matrix. In this section, the corrplot package will be used.\nThe code chunk below is used to plot a scatterplot matrix of the relationship between the independent variables in condo_resale data.frame.\n\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         tl.pos = \"td\", tl.cex = 0.5, method = \"number\", type = \"upper\")\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and patter in the matrix. There are four methods in corrplot (parameter order), named \"AOE\", \"FPC\", \"hclust\", \"alphabet\". In the code chunk above, AOE order is used. It orders the variables by using the angular order of the eigenvectors method suggested by Michael Friendly.\nFrom the scatterplot matrix, it is clear that Freehold is highly correlated to LEASE_99YEAR. In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, LEASE_99YEAR is excluded in the subsequent model building.\n\n\n\n8.3 Building a hedonic pricing model using multiple linear regression method\nThe code chunk below using lm() to calibrate the multiple linear regression model.\n\ncondo.mlr &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                data=condo_resale.sf)\nsummary(condo.mlr)\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + \n    PROX_KINDERGARTEN + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + \n    PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3475964  -293923   -23069   241043 12260381 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           481728.40  121441.01   3.967 7.65e-05 ***\nAREA_SQM               12708.32     369.59  34.385  &lt; 2e-16 ***\nAGE                   -24440.82    2763.16  -8.845  &lt; 2e-16 ***\nPROX_CBD              -78669.78    6768.97 -11.622  &lt; 2e-16 ***\nPROX_CHILDCARE       -351617.91  109467.25  -3.212  0.00135 ** \nPROX_ELDERLYCARE      171029.42   42110.51   4.061 5.14e-05 ***\nPROX_URA_GROWTH_AREA   38474.53   12523.57   3.072  0.00217 ** \nPROX_HAWKER_MARKET     23746.10   29299.76   0.810  0.41782    \nPROX_KINDERGARTEN     147468.99   82668.87   1.784  0.07466 .  \nPROX_MRT             -314599.68   57947.44  -5.429 6.66e-08 ***\nPROX_PARK             563280.50   66551.68   8.464  &lt; 2e-16 ***\nPROX_PRIMARY_SCH      180186.08   65237.95   2.762  0.00582 ** \nPROX_TOP_PRIMARY_SCH    2280.04   20410.43   0.112  0.91107    \nPROX_SHOPPING_MALL   -206604.06   42840.60  -4.823 1.57e-06 ***\nPROX_SUPERMARKET      -44991.80   77082.64  -0.584  0.55953    \nPROX_BUS_STOP         683121.35  138353.28   4.938 8.85e-07 ***\nNO_Of_UNITS             -231.18      89.03  -2.597  0.00951 ** \nFAMILY_FRIENDLY       140340.77   47020.55   2.985  0.00289 ** \nFREEHOLD              359913.01   49220.22   7.312 4.38e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 755800 on 1417 degrees of freedom\nMultiple R-squared:  0.6518,    Adjusted R-squared:  0.6474 \nF-statistic: 147.4 on 18 and 1417 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n8.4 Preparing Publication Quality Table: olsrr method\nWith reference to the report above, it is clear that not all the independent variables are statistically significant. We will revised the model by removing those variables which are not statistically significant.\nNow, we are ready to calibrate the revised model by using the code chunk below.\n\ncondo.mlr1 &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + \n                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + \n                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                 data=condo_resale.sf)\nols_regress(condo.mlr1)\n\n                             Model Summary                               \n------------------------------------------------------------------------\nR                       0.807       RMSE                     755957.289 \nR-Squared               0.651       Coef. Var                    43.168 \nAdj. R-Squared          0.647       MSE                571471422208.591 \nPred R-Squared          0.638       MAE                      414819.628 \n------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares          DF         Mean Square       F         Sig. \n--------------------------------------------------------------------------------\nRegression    1.512586e+15          14        1.080418e+14    189.059    0.0000 \nResidual      8.120609e+14        1421    571471422208.591                      \nTotal         2.324647e+15        1435                                          \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n               model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-----------------------------------------------------------------------------------------------------------------\n         (Intercept)     527633.222    108183.223                   4.877    0.000     315417.244     739849.200 \n            AREA_SQM      12777.523       367.479        0.584     34.771    0.000      12056.663      13498.382 \n                 AGE     -24687.739      2754.845       -0.167     -8.962    0.000     -30091.739     -19283.740 \n            PROX_CBD     -77131.323      5763.125       -0.263    -13.384    0.000     -88436.469     -65826.176 \n      PROX_CHILDCARE    -318472.751    107959.512       -0.084     -2.950    0.003    -530249.889    -106695.613 \n    PROX_ELDERLYCARE     185575.623     39901.864        0.090      4.651    0.000     107302.737     263848.510 \nPROX_URA_GROWTH_AREA      39163.254     11754.829        0.060      3.332    0.001      16104.571      62221.936 \n            PROX_MRT    -294745.107     56916.367       -0.112     -5.179    0.000    -406394.234    -183095.980 \n           PROX_PARK     570504.807     65507.029        0.150      8.709    0.000     442003.938     699005.677 \n    PROX_PRIMARY_SCH     159856.136     60234.599        0.062      2.654    0.008      41697.849     278014.424 \n  PROX_SHOPPING_MALL    -220947.251     36561.832       -0.115     -6.043    0.000    -292668.213    -149226.288 \n       PROX_BUS_STOP     682482.221    134513.243        0.134      5.074    0.000     418616.359     946348.082 \n         NO_Of_UNITS       -245.480        87.947       -0.053     -2.791    0.005       -418.000        -72.961 \n     FAMILY_FRIENDLY     146307.576     46893.021        0.057      3.120    0.002      54320.593     238294.560 \n            FREEHOLD     350599.812     48506.485        0.136      7.228    0.000     255447.802     445751.821 \n-----------------------------------------------------------------------------------------------------------------\n\n\n\n\n8.5 Preparing Publication Quality Table: gtsummary method\nThe gtsummary package provides an elegant and flexible way to create publication-ready summary tables in R.\nIn the code chunk below, tbl_regression() is used to create a well formatted regression report.\n\ntbl_regression(condo.mlr1, intercept = TRUE)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n&lt;0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n&lt;0.001\n    AGE\n-24,688\n-30,092, -19,284\n&lt;0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n&lt;0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n&lt;0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n&lt;0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n&lt;0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n&lt;0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n&lt;0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n&lt;0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nWith gtsummary package, model statistics can be included in the report by either appending them to the report table by using add_glance_table() or adding as a table source note by using add_glance_source_note() as shown in the code chunk below.\n\ntbl_regression(condo.mlr1, \n               intercept = TRUE) %&gt;% \n  add_glance_source_note(\n    label = list(sigma ~ \"\\U03C3\"),\n    include = c(r.squared, adj.r.squared, \n                AIC, statistic,\n                p.value, sigma))\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n&lt;0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n&lt;0.001\n    AGE\n-24,688\n-30,092, -19,284\n&lt;0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n&lt;0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n&lt;0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n&lt;0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n&lt;0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n&lt;0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n&lt;0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n&lt;0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n&lt;0.001\n  \n  \n    \n      R² = 0.651; Adjusted R² = 0.647; AIC = 42,967; Statistic = 189; p-value = &lt;0.001; σ = 755,957\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nFor more customisation options, refer to Tutorial: tbl_regression\n\n8.5.1 Checking for multicolinearity\nIn this section, we would like to introduce you a fantastic R package specially programmed for performing OLS regression. It is called olsrr. It provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\nresidual diagnostics\nmeasures of influence\nheteroskedasticity tests\ncollinearity diagnostics\nmodel fit assessment\nvariable contribution assessment\nvariable selection procedures\n\nIn the code chunk below, the ols_vif_tol() of olsrr package is used to test if there are sign of multicollinearity.\n\nols_vif_tol(condo.mlr1)\n\n              Variables Tolerance      VIF\n1              AREA_SQM 0.8728554 1.145665\n2                   AGE 0.7071275 1.414172\n3              PROX_CBD 0.6356147 1.573280\n4        PROX_CHILDCARE 0.3066019 3.261559\n5      PROX_ELDERLYCARE 0.6598479 1.515501\n6  PROX_URA_GROWTH_AREA 0.7510311 1.331503\n7              PROX_MRT 0.5236090 1.909822\n8             PROX_PARK 0.8279261 1.207837\n9      PROX_PRIMARY_SCH 0.4524628 2.210126\n10   PROX_SHOPPING_MALL 0.6738795 1.483945\n11        PROX_BUS_STOP 0.3514118 2.845664\n12          NO_Of_UNITS 0.6901036 1.449058\n13      FAMILY_FRIENDLY 0.7244157 1.380423\n14             FREEHOLD 0.6931163 1.442759\n\n\nSince the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.\n\n\n8.5.2 Test for Non-Linearity\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\nIn the code chunk below, the ols_plot_resid_fit() of olsrr package is used to perform linearity assumption test.\n\nols_plot_resid_fit(condo.mlr1)\n\n\n\n\nThe figure above reveals that most of the data poitns are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.\n\n\n8.5.3 Test for Normality Assumption\nLastly, the code chunk below uses ols_plot_resid_hist() of olsrr package to perform normality assumption test.\n\nols_plot_resid_hist(condo.mlr1)\n\n\n\n\nThe figure reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) is resemble normal distribution.\nIf you prefer formal statistical test methods, the ols_test_normality() of olsrr package can be used as shown in the code chun below.\n\nols_test_normality(condo.mlr1)\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.6856         0.0000 \nKolmogorov-Smirnov        0.1366         0.0000 \nCramer-von Mises         121.0768        0.0000 \nAnderson-Darling         67.9551         0.0000 \n-----------------------------------------------\n\n\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.\n\n\n8.5.4 Testing for Spatial Autocorrelation\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\nIn order to perform spatial autocorrelation test, we need to convert condo_resale.sf from sf data frame into a SpatialPointsDataFrame.\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\n\nmlr.output &lt;- as.data.frame(condo.mlr1$residuals)\n\nNext, we will join the newly created data frame with condo_resale.sf object.\n\ncondo_resale.res.sf &lt;- cbind(condo_resale.sf, \n                        condo.mlr1$residuals) %&gt;%\nrename(`MLR_RES` = `condo.mlr1.residuals`)\n\nNext, we will convert condo_resale.res.sf from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.\nThe code chunk below will be used to perform the data conversion process.\n\ncondo_resale.sp &lt;- as_Spatial(condo_resale.res.sf)\ncondo_resale.sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1436 \nextent      : 14940.85, 43352.45, 24765.67, 48382.81  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 23\nnames       : POSTCODE, SELLING_PRICE, AREA_SQM, AGE,    PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN,    PROX_MRT,   PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH, PROX_SHOPPING_MALL, ... \nmin values  :    18965,        540000,       34,   0, 0.386916393,    0.004927023,      0.054508623,          0.214539508,        0.051817113,       0.004927023, 0.052779424, 0.029064164,      0.077106132,          0.077106132,                  0, ... \nmax values  :   828833,       1.8e+07,      619,  37, 19.18042832,     3.46572633,      3.949157205,           9.15540001,        5.374348075,       2.229045366,  3.48037319,  2.16104919,      3.928989144,          6.748192062,        3.477433767, ... \n\n\nNext, we will use tmap package to display the distribution of the residuals on an interactive map.\nThe code churn below will turn on the interactive mode of tmap.\n\ntmap_mode(\"view\")\n\nThe code chunks below is used to create an interactive point symbol map.\n\ntm_shape(mpsz_svy21)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\nRemember to switch back to \"plot\" mode before continue.\n\ntmap_mode(\"plot\")\n\nThe figure above reveal that there is sign of spatial autocorrelation.\nTo proof that our observation is indeed true, the Moran's I test will be performed\nFirst, we will compute the distance-based weight matrix by using dnearneigh() function of spdep.\n\nnb &lt;- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)\nsummary(nb)\n\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\n\nNext, nb2listw() of spdep packge will be used to convert the output neighbours lists (i.e. nb) into a spatial weights.\n\nnb_lw &lt;- nb2listw(nb, style = 'W')\nsummary(nb_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\nWeights style: W \nWeights constants summary:\n     n      nn   S0       S1       S2\nW 1436 2062096 1436 94.81916 5798.341\n\n\nNext, lm.morantest() of spdep package will be used to perform Moran's I test for residual spatial autocorrelation\n\nlm.morantest(condo.mlr1, nb_lw)\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +\nPROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT +\nPROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\nNO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\nweights: nb_lw\n\nMoran I statistic standard deviate = 24.366, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    1.438876e-01    -5.487594e-03     3.758259e-05 \n\n\n\nlm.morantest(condo.mlr1, nb_lw)\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +\nPROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT +\nPROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\nNO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\nweights: nb_lw\n\nMoran I statistic standard deviate = 24.366, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    1.438876e-01    -5.487594e-03     3.758259e-05 \n\n\nThe Global Moran's I test for residual spatial autocorrelation shows that it's p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.1438876 which is greater than 0, we can infer than the residuals resemble cluster distribution."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex4/Hands-on_Ex4.html#building-hedonic-pricing-models-using-gwmodel",
    "href": "Hands-on_Exercise/Hands-on_Ex4/Hands-on_Ex4.html#building-hedonic-pricing-models-using-gwmodel",
    "title": "Hands-on_Ex4:Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "9 Building Hedonic Pricing Models using GWmodel",
    "text": "9 Building Hedonic Pricing Models using GWmodel\nIn this section, you are going to learn how to modelling hedonic pricing using both the fixed and adaptive bandwidth schemes\n\n9.1 Building Fixed Bandwidth GWR Model\n\n9.1.1 Computing fixed bandwith\nIn the code chunk below bw.gwr() of GWModel package is used to determine the optimal fixed bandwidth to use in the model. Notice that the argument adaptive is set to FALSE indicates that we are interested to compute the fixed bandwidth.\nThere are two possible approaches can be uused to determine the stopping rule, they are: CV cross-validation approach and AIC corrected (AICc) approach. We define the stopping rule using approach argeement.\n\n#bw.fixed &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n#                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n#                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n#                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n#                     FAMILY_FRIENDLY + FREEHOLD, \n#                   data=condo_resale.sp, \n#                   approach=\"CV\", \n#                   kernel=\"gaussian\", \n#                   adaptive=FALSE, \n#                   longlat=FALSE)\n\nThe result shows that the recommended bandwidth is 971.3405 metres. (Quiz: Do you know why it is in metre?)\n** Cannot install GWmodel, the github repository cannot be found Error 404.\n\n\n9.1.2 GWModel method - fixed bandwith\nNow we can use the code chunk below to calibrate the gwr model using fixed bandwidth and gaussian kernel.\n\n#gwr.fixed &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n#                         PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n#                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n#                         PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n#                         FAMILY_FRIENDLY + FREEHOLD, \n#                       data=condo_resale.sp, \n#                       bw=bw.fixed, \n#                       kernel = 'gaussian', \n#                       longlat = FALSE)\n\nThe output is saved in a list of class \"gwrm\". The code below can be used to display the model output.\n\n#gwr.fixed\n\nThe report shows that the AICc of the gwr is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.1.\n\n\n\n9.2 Building Adaptive Bandwidth GWR Model\nIn this section, we will calibrate the gwr-based hedonic pricing model by using adaptive bandwidth approach.\n\n9.2.1 Computing the adaptive bandwidth\nSimilar to the earlier section, we will first use bw.gwr() to determine the recommended data point to use.\nThe code chunk used look very similar to the one used to compute the fixed bandwidth except the adaptive argument has changed to TRUE.\n\n#bw.adaptive &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + \n#                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + \n#                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n#                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + \n#                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n#                      data=condo_resale.sp, \n#                      approach=\"CV\", \n#                      kernel=\"gaussian\", \n#                      adaptive=TRUE, \n#                      longlat=FALSE)\n\nThe result shows that the 30 is the recommended data points to be used.\n\n\n9.2.2 Constructing the adaptive bandwidth gwr model\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and gaussian kernel as shown in the code chunk below.\n\n#gwr.adaptive &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n#                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n#                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n#                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n#                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n#                          data=condo_resale.sp, bw=bw.adaptive, \n#                          kernel = 'gaussian', \n#                          adaptive=TRUE, \n#                          longlat = FALSE)\n\nThe code below can be used to display the model output.\n\n#gwr.adaptive\n\nThe report shows that the AICc the adaptive distance gwr is 41982.22 which is even smaller than the AICc of the fixed distance gwr of 42263.61.\n\n\n\n9.3 Visualising GWR Output\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\n\nCondition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values 3. computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its \"data\" slot in an object called SDF of the output list.\n\n\n9.4 Converting SDF into sf data.frame\nTo visualise the fields in SDF, we need to first covert it into sf data.frame by using the code chunk below.\n\n#condo_resale.sf.adaptive &lt;- st_as_sf(gwr.adaptive$SDF) %&gt;%\n#  st_transform(crs=3414)\n\n\n#condo_resale.sf.adaptive.svy21 &lt;- st_transform(condo_resale.sf.adaptive, 3414)\n#condo_resale.sf.adaptive.svy21  \n\n\n#gwr.adaptive.output &lt;- as.data.frame(gwr.adaptive$SDF)\n#condo_resale.sf.adaptive &lt;- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))\n\nNext, glimpse() is used to display the content of condo_resale.sf.adaptive sf data frame.\n\n#glimpse(condo_resale.sf.adaptive)\n\n\n#summary(gwr.adaptive$SDF$yhat)\n\n\n\n9.5 Visualising local R2\nThe code chunks below is used to create an interactive point symbol map.\n\n#tmap_mode(\"view\")\n#tm_shape(mpsz_svy21)+\n#  tm_polygons(alpha = 0.1) +\n#tm_shape(condo_resale.sf.adaptive) +  \n#  tm_dots(col = \"Local_R2\",\n#          border.col = \"gray60\",\n#          border.lwd = 1) +\n#  tm_view(set.zoom.limits = c(11,14))\n\n\n#tmap_mode(\"plot\")\n\n\n\n9.6 Visualising coefficient estimates\nThe code chunks below is used to create an interactive point symbol map.\n\n#tmap_mode(\"view\")\n#AREA_SQM_SE &lt;- tm_shape(mpsz_svy21)+\n#  tm_polygons(alpha = 0.1) +\n#tm_shape(condo_resale.sf.adaptive) +  \n#  tm_dots(col = \"AREA_SQM_SE\",\n#          border.col = \"gray60\",\n#          border.lwd = 1) +\n#  tm_view(set.zoom.limits = c(11,14))\n\n#AREA_SQM_TV &lt;- tm_shape(mpsz_svy21)+\n#  tm_polygons(alpha = 0.1) +\n#tm_shape(condo_resale.sf.adaptive) +  \n#  tm_dots(col = \"AREA_SQM_TV\",\n#          border.col = \"gray60\",\n#          border.lwd = 1) +\n#  tm_view(set.zoom.limits = c(11,14))\n\n#tmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n#             asp=1, ncol=2,\n#             sync = TRUE)\n\n\n#tmap_mode(\"plot\")\n\n\n9.6.1 By URA Planning Region\n\n#tm_shape(mpsz_svy21[mpsz_svy21$REGION_N==\"CENTRAL REGION\", ])+\n#  tm_polygons()+\n#tm_shape(condo_resale.sf.adaptive) + \n#  tm_bubbles(col = \"Local_R2\",\n#           size = 0.15,\n#           border.col = \"gray60\",\n#           border.lwd = 1)"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex4/Hands-on_Ex4.html#reference",
    "href": "Hands-on_Exercise/Hands-on_Ex4/Hands-on_Ex4.html#reference",
    "title": "Hands-on_Ex4:Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "10 Reference",
    "text": "10 Reference\nGollini I, Lu B, Charlton M, Brunsdon C, Harris P (2015) \"GWmodel: an R Package for exploring Spatial Heterogeneity using Geographically Weighted Models\". Journal of Statistical Software, 63(17):1-50, http://www.jstatsoft.org/v63/i17/\nLu B, Harris P, Charlton M, Brunsdon C (2014) \"The GWmodel R Package: further topics for exploring Spatial Heterogeneity using GeographicallyWeighted Models\". Geo-spatial Information Science 17(2): 85-101, http://www.tandfonline.com/doi/abs/10.1080/1009502.2014.917453"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex3/data/geospatial/MPSZ-2019.html",
    "href": "Hands-on_Exercise/Hands-on_Ex3/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624 - Applied Geospatial Analytics",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex3/Hands-on_Ex3.html",
    "href": "Hands-on_Exercise/Hands-on_Ex3/Hands-on_Ex3.html",
    "title": "Hands-on_Ex15",
    "section": "",
    "text": "Spatial interaction represent the flow of people, material, or information between locations in geographical space. It encompasses everything from freight shipments, energy flows, and the global trade in rare antiquities, to flight schedules, rush hour woes, and pedestrian foot traffic.\nEach spatial interaction, as an analogy for a set of movements, is composed of a discrete origin/destination pair. Each pair can be represented as a cell in a matrix where rows are related to the locations (centroids) of origin, while columns are related to locations (centroids) of destination. Such a matrix is commonly known as an origin/destination matrix, or a spatial interaction matrix.\nIn this hands-on exercise, you will learn how to build an OD matrix by using Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall. By the end of this hands-on exercise, you will be able:\n\nto import and extract OD data for a selected time interval,\nto import and save geospatial data (i.e. bus stops and mpsz) into sf tibble data frame objects,\nto populate planning subzone code into bus stops sf tibble data frame,\nto construct desire lines geospatial data from the OD data, and\nto visualise passenger volume by origin and destination bus stops by using the desire lines data."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex3/Hands-on_Ex3.html#overview",
    "href": "Hands-on_Exercise/Hands-on_Ex3/Hands-on_Ex3.html#overview",
    "title": "Hands-on_Ex15",
    "section": "",
    "text": "Spatial interaction represent the flow of people, material, or information between locations in geographical space. It encompasses everything from freight shipments, energy flows, and the global trade in rare antiquities, to flight schedules, rush hour woes, and pedestrian foot traffic.\nEach spatial interaction, as an analogy for a set of movements, is composed of a discrete origin/destination pair. Each pair can be represented as a cell in a matrix where rows are related to the locations (centroids) of origin, while columns are related to locations (centroids) of destination. Such a matrix is commonly known as an origin/destination matrix, or a spatial interaction matrix.\nIn this hands-on exercise, you will learn how to build an OD matrix by using Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall. By the end of this hands-on exercise, you will be able:\n\nto import and extract OD data for a selected time interval,\nto import and save geospatial data (i.e. bus stops and mpsz) into sf tibble data frame objects,\nto populate planning subzone code into bus stops sf tibble data frame,\nto construct desire lines geospatial data from the OD data, and\nto visualise passenger volume by origin and destination bus stops by using the desire lines data."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex3/Hands-on_Ex3.html#getting-started",
    "href": "Hands-on_Exercise/Hands-on_Ex3/Hands-on_Ex3.html#getting-started",
    "title": "Hands-on_Ex15",
    "section": "2 Getting Started",
    "text": "2 Getting Started\nFor the purpose of this exercise, four r packages will be used. They are:\n\nsf for importing, integrating, processing and transforming geospatial data.\ntidyverse for importing, integrating, wrangling and visualising data.\ntmap for creating thematic maps.\n\n\npacman::p_load(tmap, sf, DT, stplanr,\n               performance,\n               ggpubr, tidyverse)"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex3/Hands-on_Ex3.html#preparing-the-flow-data",
    "href": "Hands-on_Exercise/Hands-on_Ex3/Hands-on_Ex3.html#preparing-the-flow-data",
    "title": "Hands-on_Ex15",
    "section": "3 Preparing the Flow Data",
    "text": "3 Preparing the Flow Data\n\n3.1 Importing the OD data\nFirstly, we will import the Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall by using read_csv() of readr package.\n\nodbus &lt;- read_csv(\"data/aspatial/origin_destination_bus_202310.csv\")\n\nLet use display the odbus tibble data table by using the code chunk below.\n\nglimpse(odbus)\n\nRows: 5,694,297\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-10\", \"2023-10\", \"2023-10\", \"2023-10\", \"2023-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKENDS/HOLIDAY\", \"WEEKDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;dbl&gt; 16, 16, 14, 14, 17, 17, 17, 7, 14, 14, 10, 20, 20,…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;chr&gt; \"04168\", \"04168\", \"80119\", \"80119\", \"44069\", \"2028…\n$ DESTINATION_PT_CODE &lt;chr&gt; \"10051\", \"10051\", \"90079\", \"90079\", \"17229\", \"2014…\n$ TOTAL_TRIPS         &lt;dbl&gt; 3, 5, 3, 5, 4, 1, 24, 2, 1, 7, 3, 2, 5, 1, 1, 1, 1…\n\n\nA quick check of odbus tibble data frame shows that the values in OROGIN_PT_CODE and DESTINATON_PT_CODE are in numeric data type. Hence, the code chunk below is used to convert these data values into character data type.\n\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE) \n\n\n\n3.2 Extracting the study data\nFor the purpose of this exercise, we will extract commuting flows on weekday and between 6 and 9 o’clock.\n\nodbus6_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE,\n           DESTINATION_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\nTable below shows the content of odbus6_9\n\ndatatable(odbus6_9)\n\n\n\n\n\n\nWe will save the output in rds format for future used.\n\nwrite_rds(odbus6_9, \"data/rds/odbus6_9.rds\")\n\nThe code chunk below will be used to import the save odbus6_9.rds into R environment.\n\nodbus6_9 &lt;- read_rds(\"data/rds/odbus6_9.rds\")"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex3/Hands-on_Ex3.html#working-with-geospatial-data",
    "href": "Hands-on_Exercise/Hands-on_Ex3/Hands-on_Ex3.html#working-with-geospatial-data",
    "title": "Hands-on_Ex15",
    "section": "4 Working with Geospatial Data",
    "text": "4 Working with Geospatial Data\nFor the purpose of this exercise, two geospatial data will be used. They are:\n\nBusStop: This data provides the location of bus stop as at last quarter of 2022.\nMPSZ-2019: This data provides the sub-zone boundary of URA Master Plan 2019.\n\nBoth data sets are in ESRI shapefile format.\n\n4.1 Importing geospatial data\nTwo geospatial data will be used in this exercise, they are:\n\nbusstop &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"BusStop\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `BusStop' from data source \n  `W:\\widyayutika\\ISSS624\\Hands-on_Exercise\\Hands-on_Ex3\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `W:\\widyayutika\\ISSS624\\Hands-on_Exercise\\Hands-on_Ex3\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\nmpsz\n\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                 SUBZONE_N SUBZONE_C       PLN_AREA_N PLN_AREA_C       REGION_N\n1              MARINA EAST    MESZ01      MARINA EAST         ME CENTRAL REGION\n2         INSTITUTION HILL    RVSZ05     RIVER VALLEY         RV CENTRAL REGION\n3           ROBERTSON QUAY    SRSZ01  SINGAPORE RIVER         SR CENTRAL REGION\n4  JURONG ISLAND AND BUKOM    WISZ01  WESTERN ISLANDS         WI    WEST REGION\n5             FORT CANNING    MUSZ02           MUSEUM         MU CENTRAL REGION\n6         MARINA EAST (MP)    MPSZ05    MARINE PARADE         MP CENTRAL REGION\n7                   SUDONG    WISZ03  WESTERN ISLANDS         WI    WEST REGION\n8                  SEMAKAU    WISZ02  WESTERN ISLANDS         WI    WEST REGION\n9           SOUTHERN GROUP    SISZ02 SOUTHERN ISLANDS         SI CENTRAL REGION\n10                 SENTOSA    SISZ01 SOUTHERN ISLANDS         SI CENTRAL REGION\n   REGION_C                       geometry\n1        CR MULTIPOLYGON (((33222.98 29...\n2        CR MULTIPOLYGON (((28481.45 30...\n3        CR MULTIPOLYGON (((28087.34 30...\n4        WR MULTIPOLYGON (((14557.7 304...\n5        CR MULTIPOLYGON (((29542.53 31...\n6        CR MULTIPOLYGON (((35279.55 30...\n7        WR MULTIPOLYGON (((15772.59 21...\n8        WR MULTIPOLYGON (((19843.41 21...\n9        CR MULTIPOLYGON (((30870.53 22...\n10       CR MULTIPOLYGON (((26879.04 26...\n\n\n\nwrite_rds(mpsz, \"data/rds/mpsz.rds\")  \n\n\nNote\n\nst_read() function of sf package is used to import the shapefile into R as sf data frame.\nst_transform() function of sf package is used to transform the projection to crs 3414."
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex3/Hands-on_Ex3.html#geospatial-data-wrangling",
    "href": "Hands-on_Exercise/Hands-on_Ex3/Hands-on_Ex3.html#geospatial-data-wrangling",
    "title": "Hands-on_Ex15",
    "section": "5 Geospatial data wrangling",
    "text": "5 Geospatial data wrangling\n\n5.1 Combining Busstop and mpsz\nCode chunk below populates the planning subzone code (i.e. SUBZONE_C) of mpsz sf data frame into busstop sf data frame.\n\nbusstop_mpsz &lt;- st_intersection(busstop, mpsz) %&gt;%\n  select(BUS_STOP_N, SUBZONE_C) %&gt;%\n  st_drop_geometry()\n\nNote\n\nst_intersection() is used to perform point and polygon overly and the output will be in point sf object.\nselect() of dplyr package is then use to retain only BUS_STOP_N and SUBZONE_C in the busstop_mpsz sf data frame.\nfive bus stops are excluded in the resultant data frame because they are outside of Singapore boundary.\n\n\ndatatable(busstop_mpsz)\n\n\n\n\n\n\nBefore moving to the next step, it is wise to save the output into rds format.\n\nwrite_rds(busstop_mpsz, \"data/rds/busstop_mpsz.rds\")  \n\nNext, we are going to append the planning subzone code from busstop_mpsz data frame onto odbus6_9 data frame.\n\nod_data &lt;- left_join(odbus6_9 , busstop_mpsz,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         ORIGIN_SZ = SUBZONE_C,\n         DESTIN_BS = DESTINATION_PT_CODE)\n\nBefore continue, it is a good practice for us to check for duplicating records.\n\nduplicate &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\nIf duplicated records are found, the code chunk below will be used to retain the unique records.\n\nod_data &lt;- unique(od_data)\n\nIt will be a good practice to confirm if the duplicating records issue has been addressed fully.\nNext, we will update od_data data frame with the planning subzone codes.\n\nod_data &lt;- left_join(od_data , busstop_mpsz,\n            by = c(\"DESTIN_BS\" = \"BUS_STOP_N\")) \n\n\nduplicate &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\n\nod_data &lt;- unique(od_data)\n\n\nod_data &lt;- od_data %&gt;%\n  rename(DESTIN_SZ = SUBZONE_C) %&gt;%\n  drop_na() %&gt;%\n  group_by(ORIGIN_SZ, DESTIN_SZ) %&gt;%\n  summarise(MORNING_PEAK = sum(TRIPS))\n\nIt is time to save the output into an rds file format.\n\nwrite_rds(od_data, \"data/rds/od_data.rds\")\n\n\nod_data &lt;- read_rds(\"data/rds/od_data.rds\")"
  },
  {
    "objectID": "Hands-on_Exercise/Hands-on_Ex3/Hands-on_Ex3.html#visualising-spatial-interaction",
    "href": "Hands-on_Exercise/Hands-on_Ex3/Hands-on_Ex3.html#visualising-spatial-interaction",
    "title": "Hands-on_Ex15",
    "section": "6 Visualising Spatial Interaction",
    "text": "6 Visualising Spatial Interaction\nIn this section, you will learn how to prepare a desire line by using stplanr package.\n\n6.1 Removing intra-zonal flows\nWe will not plot the intra-zonal flows. The code chunk below will be used to remove intra-zonal flows.\n\nod_data1 &lt;- od_data[od_data$ORIGIN_SZ!=od_data$DESTIN_SZ,]\n\n\n\n6.2 Creating desire lines\nIn this code chunk below, od2line() of stplanr package is used to create the desire lines.\n\nflowLine &lt;- od2line(flow = od_data1, \n                    zones = mpsz,\n                    zone_code = \"SUBZONE_C\")\n\n\n\n6.3 Visualising the desire lines\nTo visualise the resulting desire lines, the code chunk below is used.\n\ntmap_options(check.and.fix = TRUE)\ntm_shape(mpsz) +\n  tm_polygons() +\nflowLine %&gt;%  \ntm_shape() +\n  tm_lines(lwd = \"MORNING_PEAK\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.3)\n\n\n\n\nWarning\nBe patient, the rendering process takes more time because of the transparency argument (i.e. alpha)\nWhen the flow data are very messy and highly skewed like the one shown above, it is wiser to focus on selected flows, for example flow greater than or equal to 5000 as shown below.\n\ntmap_options(check.and.fix = TRUE)\ntm_shape(mpsz) +\n  tm_polygons() +\nflowLine %&gt;%  \n  filter(MORNING_PEAK &gt;= 200) %&gt;%\ntm_shape() +\n  tm_lines(lwd = \"MORNING_PEAK\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.3)"
  }
]